{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pT3rJzPszHgm"
   },
   "source": [
    "## Project 7 - Neural Networks\n",
    "Recognizing multi-digit numbers in photographs captured at street level is an important component of modern-day map making. A classic example of a corpus of such street level photographs is Googleâ€™s Street View imagery comprised of hundreds of millions of geo-located 360 degree panoramic images. The ability to automatically transcribe an address number from a geo-located patch of pixels and associate the transcribed number with a known street address helps pinpoint, with a high degree of accuracy, the location of the building it represents. \n",
    "\n",
    "More broadly, recognizing numbers in photographs is a problem of interest to the optical character recognition community. While OCR on constrained domains like document processing is well studied, arbitrary multi-character text recognition in photographs is still highly challenging. This difficulty arises due to the wide variability in the visual appearance of text in the wild on account of a large range of fonts, colors, styles, orientations, and character arrangements. The recognition problem is further complicated by environmental factors such as lighting, shadows, specularities, and occlusions as well as by image acquisition factors such as resolution, motion, and focus blurs.\n",
    "\n",
    "In this project we will use dataset with images centred around a single digit (many of the images do contain some distractors at the sides). Although we are taking a sample of the data which is simpler, it is more complex than MNIST because of the distractors. \n",
    "\n",
    "**The Street View House Numbers (SVHN) Dataset**\n",
    "\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data formatting but comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
    "\n",
    "[Link to the dataset](https://drive.google.com/file/d/1L2-WXzguhUsCArrFUc8EEkXcj33pahoS/view?usp=sharing)\n",
    "\n",
    "**Acknowledgement for the datasets**\n",
    "\n",
    "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011. PDF http://ufldl.stanford.edu/housenumbers as the URL for this site when necessary.\n",
    "\n",
    "**Objective of the project** is to learn how to implement a simple image classification pipeline based on a deep neural network.\n",
    "\n",
    "* Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages)\n",
    "* Data fetching and understand the train/val/test splits\n",
    "* Implement and apply a deep neural network classifier including (feed forward neural network, RELU, activations)\n",
    "* Understand and be able to implement (vectorized) backpropagation (cost stochastic gradient descent, cross entropy loss, cost functions)\n",
    "* Implement batch normalization for training the neural network\n",
    "* Print the classification accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lz9bffGcyhn8",
    "outputId": "88e7166a-d693-4798-ca57-4e9eca027f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mounting Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8PZN4ee1mJI"
   },
   "outputs": [],
   "source": [
    "# Setting the current working directory\n",
    "import os; os.chdir('drive/My Drive/Great Learning/Neural Network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DsvU1eB2dJ9"
   },
   "source": [
    "<a id='import'></a>\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "YdPPqfcR17r5",
    "outputId": "6de7e6c5-07ef-4596-e8f9-08b2861ce82f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, h5py\n",
    "import matplotlib.style as style; style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "# Metrics and preprocessing\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TF and Keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Checking if GPU is found\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uWLj6hFH3e36",
    "outputId": "329d6d2a-40ce-4b37-c3aa-4c2937112177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'07_Neural Network.ipynb'   SVHN_single_grey1.h5\n"
     ]
    }
   ],
   "source": [
    "!ls '/content/drive/My Drive/Great Learning/Neural Network'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4jslkxXdFB1"
   },
   "source": [
    "<a id='load'></a>\n",
    "### Load train, val and test datasets from h5py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "kdolI00z7KOf",
    "outputId": "7d3fd2b8-da8b-4c72-f55c-c0868eedb822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 32, 32) (42000,)\n",
      "Validation set (60000, 32, 32) (60000,)\n",
      "Test set (18000, 32, 32) (18000,)\n",
      "\n",
      "\n",
      "Unique labels in y_train: [0 1 2 3 4 5 6 7 8 9]\n",
      "Unique labels in y_val: [0 1 2 3 4 5 6 7 8 9]\n",
      "Unique labels in y_test: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Read the h5 file\n",
    "h5_SVH = h5py.File('SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "# Load the training, validation and test sets\n",
    "X_train = h5_SVH['X_train'][:]\n",
    "y_train_o = h5_SVH['y_train'][:]\n",
    "X_val = h5_SVH['X_val'][:]\n",
    "y_val_o = h5_SVH['y_val'][:]\n",
    "X_test = h5_SVH['X_test'][:]\n",
    "y_test_o = h5_SVH['y_test'][:]\n",
    "\n",
    "# Close this file\n",
    "\n",
    "h5_SVH.close()\n",
    "\n",
    "print('Training set', X_train.shape, y_train_o.shape)\n",
    "print('Validation set', X_val.shape, y_val_o.shape)\n",
    "print('Test set', X_test.shape, y_test_o.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('Unique labels in y_train:', np.unique(y_train_o))\n",
    "print('Unique labels in y_val:', np.unique(y_val_o))\n",
    "print('Unique labels in y_test:', np.unique(y_test_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwanQxdD9mBC"
   },
   "source": [
    "<a id='o1'></a>\n",
    "#### Observation 1 - Sets Shape\n",
    "* Length of training sets: 42k, validation sets: 60k, test sets: 18k\n",
    "* Size of the images: 32*32\n",
    "* Number of class: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8Zur6Qxs92_"
   },
   "source": [
    "<a id='visualize'></a>\n",
    "### Visualizing first 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "colab_type": "code",
    "id": "wuXKfSv58lbK",
    "outputId": "4d2bf75c-6b25-4c59-ecd7-0c06aade559e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7kAAAB1CAYAAACLZSaSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x324 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
     ]
    }
   ],
   "source": [
    "# Visualizing first 10 images in the dataset and their labels\n",
    "plt.figure(figsize = (15, 4.5))\n",
    "for i in range(10):  \n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i].reshape((32, 32)),cmap = plt.cm.binary)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace = -0.1, hspace = -0.1)\n",
    "plt.show()\n",
    "\n",
    "print('Label for each of the above image: %s' % (y_train_o[0 : 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "Td7LigoTynL2",
    "outputId": "dea62917-3429-40ac-c813-31b36f8a5f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking first image and label in training set\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYmjghETM/wIo2e2Bn1dKLgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    }
   ],
   "source": [
    "print('Checking first image and label in training set'); print('--'*40)\n",
    "plt.imshow(X_train[0], cmap = plt.cm.binary)    \n",
    "plt.show()\n",
    "print('Label:', y_train_o[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "Bcpa-MFxy0Zt",
    "outputId": "f4593ef0-8dfb-46d5-d06d-3d4c2c88907f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking first image and label in validation set\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYeTgQlQYObgQFebfAfmL0QQbDJBwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "print('Checking first image and label in validation set'); print('--'*40)\n",
    "plt.imshow(X_val[0], cmap = plt.cm.binary)    \n",
    "plt.show()\n",
    "print('Label:', y_val_o[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "r1OIeig-zHap",
    "outputId": "469be395-b214-4ccb-fdb3-5058f7a2f3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking first image and label in test set\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print('Checking first image and label in test set'); print('--'*40)\n",
    "plt.imshow(X_test[0], cmap = plt.cm.binary)    \n",
    "plt.show()\n",
    "print('Label:', y_test_o[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEQ-CNcVdr-c"
   },
   "source": [
    "<a id='flatten'></a>\n",
    "### Flatten and normalize the images for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "t4cfwZ9ldlO0",
    "outputId": "4c4d0e87-bb47-47d7-f016-7bf6347f7ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
      "--------------------------------------------------------------------------------\n",
      "Making sure that the values are float so that we can get decimal points after division\n",
      "--------------------------------------------------------------------------------\n",
      "Normalizing the RGB codes by dividing it to the max RGB value\n",
      "--------------------------------------------------------------------------------\n",
      "Converting y data into categorical (one-hot encoding)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_val = X_val.reshape((X_val.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\n",
    "y_train = to_categorical(y_train_o)\n",
    "y_val = to_categorical(y_val_o)\n",
    "y_test = to_categorical(y_test_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "evUXte-cnfRN",
    "outputId": "9f9be675-0b9e-4a14-dd97-1f4bd4d4efa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42000, 1024)\n",
      "X_val shape: (60000, 1024)\n",
      "X_test shape: (18000, 1024)\n",
      "\n",
      "\n",
      "y_train shape: (42000, 10)\n",
      "y_val shape: (60000, 10)\n",
      "y_test shape: (18000, 10)\n",
      "\n",
      "\n",
      "Number of images in X_train 42000\n",
      "Number of images in X_val 60000\n",
      "Number of images in X_test 18000\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('\\n')\n",
    "print('Number of images in X_train', X_train.shape[0])\n",
    "print('Number of images in X_val', X_val.shape[0])\n",
    "print('Number of images in X_test', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGKy-ZD1YAWv"
   },
   "source": [
    "<a id='Baby'></a>\n",
    "### Modelling - Baby sitting the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JX35qWq_AyLZ"
   },
   "source": [
    "#### Fully connected linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nyQvFvwMX_pI"
   },
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None        \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = np.dot(X, self.W) + self.b\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradW = np.dot(self.X.T, nextgrad)\n",
    "        self.gradB = np.sum(nextgrad, axis=0)\n",
    "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQegqIQXA1Wi"
   },
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0HxN2PnaaaM"
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.output = np.maximum(X, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        self.gradInput = nextgrad.copy()\n",
    "        self.gradInput[self.output <=0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CwuYJdhA7sz"
   },
   "source": [
    "#### Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AxCBE9fagiP"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6X9_AZeA-61"
   },
   "source": [
    "#### Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbjl_6qYaq80"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
    "        loss = np.sum(cross_entropy) / self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()        \n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m), y] -= 1\n",
    "        grad /= self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-dLPKqbBCuT"
   },
   "source": [
    "#### NN class that enables the forward prop and backward propagation of the entire network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOUQbEfIawH0"
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, lossfunc = CrossEntropy(), mode = 'train'):\n",
    "        self.params = []\n",
    "        self.layers = []\n",
    "        self.loss_func = lossfunc\n",
    "        self.grads = []\n",
    "        self.mode = mode\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, nextgrad):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            nextgrad, grad = layer.backward(nextgrad)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(out,y)\n",
    "        nextgrad = self.loss_func.backward(out,y)\n",
    "        grads = self.backward(nextgrad)\n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "    \n",
    "    def predict_scores(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return p\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHLu_qtoBGYg"
   },
   "source": [
    "#### Update function SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpEjX7qqbN9q"
   },
   "outputs": [],
   "source": [
    "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = (mu * v[i]) - (learning_rate * g[i])\n",
    "            p[i] += v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQnT5wBFBJUH"
   },
   "source": [
    "#### Get minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYxPXK7Fbfgk"
   },
   "outputs": [],
   "source": [
    "def minibatch(X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    for i in range(0, n , minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, :]\n",
    "        y_batch = y[i:i + minibatch_size, ]\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "        \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScfG6m51BQfz"
   },
   "source": [
    "#### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6F3W-wfbpqC"
   },
   "outputs": [],
   "source": [
    "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu = 0.9, X_val = None, y_val = None, Lambda = 0, verb = True):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "            \n",
    "        # iterate over mini batches\n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "        \n",
    "        # accuracy of model at end of epoch after all mini batch updates\n",
    "        m_train = X_train.shape[0]\n",
    "        m_val = X_val.shape[0]\n",
    "        y_train_pred = []\n",
    "        y_val_pred = []\n",
    "        y_train1 = []\n",
    "        y_vall = []\n",
    "        for ii in range(0, m_train, minibatch_size):\n",
    "            X_tr = X_train[ii:ii + minibatch_size, : ]\n",
    "            y_tr = y_train[ii:ii + minibatch_size,]\n",
    "            y_train1 = np.append(y_train1, y_tr)\n",
    "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
    "\n",
    "        for ii in range(0, m_val, minibatch_size):\n",
    "            X_va = X_val[ii:ii + minibatch_size, : ]\n",
    "            y_va = y_val[ii:ii + minibatch_size,]\n",
    "            y_vall = np.append(y_vall, y_va)\n",
    "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
    "            \n",
    "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
    "        \n",
    "        ## weights\n",
    "        w = np.array(net.params[0][0])\n",
    "        \n",
    "        ## adding regularization to cost\n",
    "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n",
    "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "        \n",
    "        val_loss_epoch.append(mean_val_loss)\n",
    "        if verb:\n",
    "            if i%50==0:\n",
    "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n",
    "    return net, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFKQ9JIPBU2L"
   },
   "source": [
    "#### Checking the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAAOfhBebz8d"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iagKp2kvBaZD"
   },
   "source": [
    "#### Invoking all that we have created until now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vA1-5ojib7XH"
   },
   "outputs": [],
   "source": [
    "# Invoking the model\n",
    "## input size\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "def train_and_test_loop(iterations, lr, Lambda, verb = True):\n",
    "    ## hyperparameters\n",
    "    iterations = iterations\n",
    "    learning_rate = lr\n",
    "    hidden_nodes1 = 10\n",
    "    output_nodes = 10\n",
    "\n",
    "    ## define neural net\n",
    "    nn = NN()\n",
    "    nn.add_layer(Linear(input_dim, hidden_nodes1))\n",
    "\n",
    "    nn, val_acc = train(nn, X_train, y_train_o, minibatch_size = 200, epoch = iterations, learning_rate = learning_rate,\\\n",
    "                      X_val = X_test, y_val = y_test_o, Lambda = Lambda, verb = verb)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JhBJRfvBfiC"
   },
   "source": [
    "#### Double Check that the loss is reasonable : Disable the regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "fjbjgrHl6LBv",
    "outputId": "8dd2b91f-4761-480b-c68e-bdfe4c4a54a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 2.3117883972138844 | Training Accuracy = 0.09335714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09438888888888888"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 0\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wFbKNxLpBl7i"
   },
   "source": [
    "#### Now, lets crank up the Lambda(Regularization) and check what it does to our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "_OtyvtW39bmm",
    "outputId": "ce84f1ba-9d21-4a36-c881-579f3fa3ba4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 2.3082000425724667 | Training Accuracy = 0.08478571428571428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08188888888888889"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 1e3\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Zq7TGKfBped"
   },
   "source": [
    "#### Now, lets overfit to a small subset of our dataset, in this case 20 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "882INFrA9wSF",
    "outputId": "8f0d13f9-0dc9-40eb-a335-30f87bb8b746"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 1024), (20,))"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_subset = X_train[0:20]\n",
    "y_train_subset = y_train_o[0:20]\n",
    "\n",
    "X_train = X_train_subset\n",
    "y_train_o = y_train_subset\n",
    "\n",
    "X_train.shape, y_train_o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmS0Eicu-T00"
   },
   "source": [
    "#### Make sure that you can overfit very small portion of the training data\n",
    "So, set a small learning rate and turn regularization off\n",
    "In the code below:\n",
    "* Take the first 20 examples\n",
    "* turn off regularization(reg=0.0)\n",
    "* use simple vanilla 'sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "adxWZ2NO-OrY",
    "outputId": "1a35adc0-91a8-4603-f953-b6430d9030e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 8 Âµs, total: 8 Âµs\n",
      "Wall time: 8.11 Âµs\n",
      "Epoch 0/5000: Loss = 2.343525441008851 | Training Accuracy = 0.0\n",
      "Epoch 50/5000: Loss = 1.9368840865311359 | Training Accuracy = 0.3\n",
      "Epoch 100/5000: Loss = 1.8502138907049706 | Training Accuracy = 0.3\n",
      "Epoch 150/5000: Loss = 1.795704759770493 | Training Accuracy = 0.35\n",
      "Epoch 200/5000: Loss = 1.7505141963680764 | Training Accuracy = 0.4\n",
      "Epoch 250/5000: Loss = 1.7098591715226046 | Training Accuracy = 0.45\n",
      "Epoch 300/5000: Loss = 1.6720807405140872 | Training Accuracy = 0.45\n",
      "Epoch 350/5000: Loss = 1.6364256819871694 | Training Accuracy = 0.5\n",
      "Epoch 400/5000: Loss = 1.602489808977548 | Training Accuracy = 0.55\n",
      "Epoch 450/5000: Loss = 1.5700278004175436 | Training Accuracy = 0.55\n",
      "Epoch 500/5000: Loss = 1.5388749934942745 | Training Accuracy = 0.6\n",
      "Epoch 550/5000: Loss = 1.5089113416081257 | Training Accuracy = 0.6\n",
      "Epoch 600/5000: Loss = 1.480043412413456 | Training Accuracy = 0.6\n",
      "Epoch 650/5000: Loss = 1.4521947854019437 | Training Accuracy = 0.65\n",
      "Epoch 700/5000: Loss = 1.425300621663516 | Training Accuracy = 0.65\n",
      "Epoch 750/5000: Loss = 1.3993044229509757 | Training Accuracy = 0.65\n",
      "Epoch 800/5000: Loss = 1.3741559975684867 | Training Accuracy = 0.65\n",
      "Epoch 850/5000: Loss = 1.3498101223720105 | Training Accuracy = 0.65\n",
      "Epoch 900/5000: Loss = 1.3262256235641494 | Training Accuracy = 0.65\n",
      "Epoch 950/5000: Loss = 1.303364719491542 | Training Accuracy = 0.65\n",
      "Epoch 1000/5000: Loss = 1.2811925334058236 | Training Accuracy = 0.65\n",
      "Epoch 1050/5000: Loss = 1.2596767202488166 | Training Accuracy = 0.65\n",
      "Epoch 1100/5000: Loss = 1.2387871723562258 | Training Accuracy = 0.65\n",
      "Epoch 1150/5000: Loss = 1.218495781389982 | Training Accuracy = 0.65\n",
      "Epoch 1200/5000: Loss = 1.1987762414312768 | Training Accuracy = 0.7\n",
      "Epoch 1250/5000: Loss = 1.1796038829744615 | Training Accuracy = 0.7\n",
      "Epoch 1300/5000: Loss = 1.1609555306717911 | Training Accuracy = 0.7\n",
      "Epoch 1350/5000: Loss = 1.1428093797371377 | Training Accuracy = 0.7\n",
      "Epoch 1400/5000: Loss = 1.1251448873080339 | Training Accuracy = 0.7\n",
      "Epoch 1450/5000: Loss = 1.1079426760244366 | Training Accuracy = 0.8\n",
      "Epoch 1500/5000: Loss = 1.0911844477557584 | Training Accuracy = 0.85\n",
      "Epoch 1550/5000: Loss = 1.0748529058881875 | Training Accuracy = 0.85\n",
      "Epoch 1600/5000: Loss = 1.058931684932634 | Training Accuracy = 0.85\n",
      "Epoch 1650/5000: Loss = 1.0434052864697851 | Training Accuracy = 0.85\n",
      "Epoch 1700/5000: Loss = 1.0282590206397022 | Training Accuracy = 0.85\n",
      "Epoch 1750/5000: Loss = 1.013478952527564 | Training Accuracy = 0.85\n",
      "Epoch 1800/5000: Loss = 0.9990518529073633 | Training Accuracy = 0.85\n",
      "Epoch 1850/5000: Loss = 0.9849651528906417 | Training Accuracy = 0.85\n",
      "Epoch 1900/5000: Loss = 0.9712069020941513 | Training Accuracy = 0.85\n",
      "Epoch 1950/5000: Loss = 0.9577657299933456 | Training Accuracy = 0.85\n",
      "Epoch 2000/5000: Loss = 0.9446308101711862 | Training Accuracy = 0.85\n",
      "Epoch 2050/5000: Loss = 0.9317918272064819 | Training Accuracy = 0.85\n",
      "Epoch 2100/5000: Loss = 0.9192389459746264 | Training Accuracy = 0.85\n",
      "Epoch 2150/5000: Loss = 0.9069627831576392 | Training Accuracy = 0.85\n",
      "Epoch 2200/5000: Loss = 0.8949543807808057 | Training Accuracy = 0.85\n",
      "Epoch 2250/5000: Loss = 0.8832051816107626 | Training Accuracy = 0.9\n",
      "Epoch 2300/5000: Loss = 0.8717070062651777 | Training Accuracy = 0.95\n",
      "Epoch 2350/5000: Loss = 0.8604520318976363 | Training Accuracy = 0.95\n",
      "Epoch 2400/5000: Loss = 0.849432772333326 | Training Accuracy = 0.95\n",
      "Epoch 2450/5000: Loss = 0.8386420595418531 | Training Accuracy = 0.95\n",
      "Epoch 2500/5000: Loss = 0.828073026343203 | Training Accuracy = 0.95\n",
      "Epoch 2550/5000: Loss = 0.8177190902516587 | Training Accuracy = 0.95\n",
      "Epoch 2600/5000: Loss = 0.8075739383704768 | Training Accuracy = 0.95\n",
      "Epoch 2650/5000: Loss = 0.7976315132574238 | Training Accuracy = 0.95\n",
      "Epoch 2700/5000: Loss = 0.7878859996879619 | Training Accuracy = 0.95\n",
      "Epoch 2750/5000: Loss = 0.778331812248966 | Training Accuracy = 0.95\n",
      "Epoch 2800/5000: Loss = 0.7689635837014647 | Training Accuracy = 1.0\n",
      "Epoch 2850/5000: Loss = 0.7597761540560017 | Training Accuracy = 1.0\n",
      "Epoch 2900/5000: Loss = 0.7507645603089181 | Training Accuracy = 1.0\n",
      "Epoch 2950/5000: Loss = 0.7419240267921232 | Training Accuracy = 1.0\n",
      "Epoch 3000/5000: Loss = 0.7332499560928596 | Training Accuracy = 1.0\n",
      "Epoch 3050/5000: Loss = 0.7247379205035223 | Training Accuracy = 1.0\n",
      "Epoch 3100/5000: Loss = 0.716383653964874 | Training Accuracy = 1.0\n",
      "Epoch 3150/5000: Loss = 0.7081830444689625 | Training Accuracy = 1.0\n",
      "Epoch 3200/5000: Loss = 0.700132126890769 | Training Accuracy = 1.0\n",
      "Epoch 3250/5000: Loss = 0.6922270762200713 | Training Accuracy = 1.0\n",
      "Epoch 3300/5000: Loss = 0.6844642011672761 | Training Accuracy = 1.0\n",
      "Epoch 3350/5000: Loss = 0.6768399381190008 | Training Accuracy = 1.0\n",
      "Epoch 3400/5000: Loss = 0.6693508454210667 | Training Accuracy = 1.0\n",
      "Epoch 3450/5000: Loss = 0.6619935979682519 | Training Accuracy = 1.0\n",
      "Epoch 3500/5000: Loss = 0.6547649820817112 | Training Accuracy = 1.0\n",
      "Epoch 3550/5000: Loss = 0.6476618906563633 | Training Accuracy = 1.0\n",
      "Epoch 3600/5000: Loss = 0.6406813185618502 | Training Accuracy = 1.0\n",
      "Epoch 3650/5000: Loss = 0.6338203582818351 | Training Accuracy = 1.0\n",
      "Epoch 3700/5000: Loss = 0.62707619577748 | Training Accuracy = 1.0\n",
      "Epoch 3750/5000: Loss = 0.6204461065619331 | Training Accuracy = 1.0\n",
      "Epoch 3800/5000: Loss = 0.613927451973544 | Training Accuracy = 1.0\n",
      "Epoch 3850/5000: Loss = 0.6075176756363615 | Training Accuracy = 1.0\n",
      "Epoch 3900/5000: Loss = 0.6012143000972181 | Training Accuracy = 1.0\n",
      "Epoch 3950/5000: Loss = 0.5950149236294051 | Training Accuracy = 1.0\n",
      "Epoch 4000/5000: Loss = 0.5889172171935882 | Training Accuracy = 1.0\n",
      "Epoch 4050/5000: Loss = 0.5829189215472004 | Training Accuracy = 1.0\n",
      "Epoch 4100/5000: Loss = 0.5770178444941011 | Training Accuracy = 1.0\n",
      "Epoch 4150/5000: Loss = 0.5712118582668013 | Training Accuracy = 1.0\n",
      "Epoch 4200/5000: Loss = 0.5654988970340131 | Training Accuracy = 1.0\n",
      "Epoch 4250/5000: Loss = 0.5598769545267331 | Training Accuracy = 1.0\n",
      "Epoch 4300/5000: Loss = 0.5543440817764675 | Training Accuracy = 1.0\n",
      "Epoch 4350/5000: Loss = 0.5488983849595841 | Training Accuracy = 1.0\n",
      "Epoch 4400/5000: Loss = 0.5435380233421363 | Training Accuracy = 1.0\n",
      "Epoch 4450/5000: Loss = 0.538261207319821 | Training Accuracy = 1.0\n",
      "Epoch 4500/5000: Loss = 0.5330661965480605 | Training Accuracy = 1.0\n",
      "Epoch 4550/5000: Loss = 0.5279512981574574 | Training Accuracy = 1.0\n",
      "Epoch 4600/5000: Loss = 0.5229148650501726 | Training Accuracy = 1.0\n",
      "Epoch 4650/5000: Loss = 0.5179552942730058 | Training Accuracy = 1.0\n",
      "Epoch 4700/5000: Loss = 0.513071025463205 | Training Accuracy = 1.0\n",
      "Epoch 4750/5000: Loss = 0.5082605393632545 | Training Accuracy = 1.0\n",
      "Epoch 4800/5000: Loss = 0.5035223564010968 | Training Accuracy = 1.0\n",
      "Epoch 4850/5000: Loss = 0.49885503533244346 | Training Accuracy = 1.0\n",
      "Epoch 4900/5000: Loss = 0.4942571719420129 | Training Accuracy = 1.0\n",
      "Epoch 4950/5000: Loss = 0.4897273978007071 | Training Accuracy = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1381111111111111"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "lr = 0.001\n",
    "Lambda = 0\n",
    "train_and_test_loop(5000, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BleQhuF6ByTQ"
   },
   "source": [
    "#### Loading the original dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "C1iZh7go--Tx",
    "outputId": "688eb4bb-3310-47e0-a01c-39b97ca301f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
      "--------------------------------------------------------------------------------\n",
      "Making sure that the values are float so that we can get decimal points after division\n",
      "--------------------------------------------------------------------------------\n",
      "Normalizing the RGB codes by dividing it to the max RGB value\n",
      "--------------------------------------------------------------------------------\n",
      "Converting y data into categorical (one-hot encoding)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "h5_SVH = h5py.File('SVHN_single_grey1.h5', 'r')\n",
    "# Load the training, validation and test sets\n",
    "X_train = h5_SVH['X_train'][:]\n",
    "y_train_o = h5_SVH['y_train'][:]\n",
    "X_val = h5_SVH['X_val'][:]\n",
    "y_val_o = h5_SVH['y_val'][:]\n",
    "X_test = h5_SVH['X_test'][:]\n",
    "y_test_o = h5_SVH['y_test'][:]\n",
    "\n",
    "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_val = X_val.reshape((X_val.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\n",
    "y_train = to_categorical(y_train_o)\n",
    "y_val = to_categorical(y_val_o)\n",
    "y_test = to_categorical(y_test_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qZ1E-lE_vD9"
   },
   "source": [
    "#### Start with small regularization and find learning rate that makes the loss go down.\n",
    "* we start with Lambda(small regularization) = 1e-7\n",
    "* we start with a small learning rate = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "V7q3hsLT_oDn",
    "outputId": "842b874b-99ea-4eee-efc3-369b5b83f4aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 2.317728157047642 | Training Accuracy = 0.104\n",
      "Epoch 50/500: Loss = 2.3127425227478797 | Training Accuracy = 0.10471428571428572\n",
      "Epoch 100/500: Loss = 2.3094828415428403 | Training Accuracy = 0.1055\n",
      "Epoch 150/500: Loss = 2.307345836509397 | Training Accuracy = 0.10542857142857143\n",
      "Epoch 200/500: Loss = 2.305942805671862 | Training Accuracy = 0.10428571428571429\n",
      "Epoch 250/500: Loss = 2.3050204802732313 | Training Accuracy = 0.10395238095238095\n",
      "Epoch 300/500: Loss = 2.3044129184583078 | Training Accuracy = 0.10254761904761905\n",
      "Epoch 350/500: Loss = 2.3040111864235984 | Training Accuracy = 0.101\n",
      "Epoch 400/500: Loss = 2.303743794017468 | Training Accuracy = 0.10019047619047619\n",
      "Epoch 450/500: Loss = 2.3035638954300524 | Training Accuracy = 0.09911904761904762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09555555555555556"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-7\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtPHH89wB8YE"
   },
   "source": [
    "#### Lets try to train now with a value of learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "7hvRY3b__90J",
    "outputId": "47584191-1536-49b4-ccff-db2e2a597ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 2.3045206865294667 | Training Accuracy = 0.11473809523809524\n",
      "Epoch 50/500: Loss = 2.259865884155515 | Training Accuracy = 0.20614285714285716\n",
      "Epoch 100/500: Loss = 2.251416078728257 | Training Accuracy = 0.21633333333333332\n",
      "Epoch 150/500: Loss = 2.2471058220607465 | Training Accuracy = 0.22138095238095237\n",
      "Epoch 200/500: Loss = 2.2442164094973287 | Training Accuracy = 0.22454761904761905\n",
      "Epoch 250/500: Loss = 2.2420426056295346 | Training Accuracy = 0.22678571428571428\n",
      "Epoch 300/500: Loss = 2.240300552374621 | Training Accuracy = 0.22766666666666666\n",
      "Epoch 350/500: Loss = 2.2388468080995563 | Training Accuracy = 0.22914285714285715\n",
      "Epoch 400/500: Loss = 2.237598607772921 | Training Accuracy = 0.2300952380952381\n",
      "Epoch 450/500: Loss = 2.2365039099696107 | Training Accuracy = 0.23076190476190475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21433333333333332"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gU8B62CzAdTA"
   },
   "source": [
    "#### Hyperparameter Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qp_2H1IwCUIY"
   },
   "source": [
    "#### Running a finer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "GZN36etSCUsA",
    "outputId": "9b5791f7-94d4-4134-b18c-f2bb284e340c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/10: Best_val_acc: 0.18288888888888888, lr: 0.005248163657617819, Lambda: 64.48340786832672\n",
      "\n",
      "Try 2/10: Best_val_acc: 0.2053888888888889, lr: 0.006679712019881258, Lambda: 0.3646857771377305\n",
      "\n",
      "Try 3/10: Best_val_acc: 0.18827777777777777, lr: 0.0046008725625766725, Lambda: 0.03253300599381195\n",
      "\n",
      "Try 4/10: Best_val_acc: 0.19616666666666666, lr: 0.004120077105146745, Lambda: 13.396751846027179\n",
      "\n",
      "Try 5/10: Best_val_acc: 0.18988888888888888, lr: 0.0015322728061813945, Lambda: 0.05780821685783054\n",
      "\n",
      "Try 6/10: Best_val_acc: 0.2031111111111111, lr: 0.003223010681924104, Lambda: 19.57233858291238\n",
      "\n",
      "Try 7/10: Best_val_acc: 0.18738888888888888, lr: 0.007976192792141505, Lambda: 0.0004221599617830911\n",
      "\n",
      "Try 8/10: Best_val_acc: 0.19411111111111112, lr: 0.009793789314692974, Lambda: 1.7025515288536694\n",
      "\n",
      "Try 9/10: Best_val_acc: 0.19827777777777778, lr: 0.0023285365617220083, Lambda: 5.1261156557902146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1, 10):\n",
    "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5, 2))\n",
    "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 10, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzm9IwhkvrIm"
   },
   "source": [
    "##### Observation 2 - Baby sitting the neural network for SVHN\n",
    "* Best accuracy achieved using this method after hyperparameter optimization: 21%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwYNz4zM-gob"
   },
   "source": [
    "<a id='BasicNN'></a>\n",
    "### Modelling - Neural Network API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrFnLj4hP787"
   },
   "source": [
    "#### NN model, sigmoid activations, SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "clFSpvEc-fkg",
    "outputId": "4ce26658-e9be-40c6-d6c6-331144bd2345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with sigmoid activations\n",
      "--------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "print('NN model with sigmoid activations'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model1 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions sigmoid\n",
    "model1.add(Dense(128, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model1.add(Dense(64))\n",
    "# Adding activation function\n",
    "model1.add(Activation('sigmoid'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model1.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model1.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "vYDJ37MPlLiL",
    "outputId": "f3c7951d-68c1-4a5a-a18a-b1e0b246c517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 140,106\n",
      "Trainable params: 140,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RkURmgxTQYhx",
    "outputId": "5a9d0c3d-f955-4493-d0e4-d42618d9401b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 3s 64us/sample - loss: 2.3172 - acc: 0.1011 - val_loss: 2.3029 - val_acc: 0.1029\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3030 - acc: 0.1024 - val_loss: 2.3030 - val_acc: 0.1024\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3031 - acc: 0.0996 - val_loss: 2.3028 - val_acc: 0.0991\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3030 - acc: 0.0988 - val_loss: 2.3028 - val_acc: 0.1019\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3029 - acc: 0.0998 - val_loss: 2.3027 - val_acc: 0.1009\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3028 - acc: 0.0989 - val_loss: 2.3029 - val_acc: 0.0998\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3028 - acc: 0.1022 - val_loss: 2.3026 - val_acc: 0.1011\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.3027 - acc: 0.1015 - val_loss: 2.3027 - val_acc: 0.0998\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.3027 - acc: 0.1011 - val_loss: 2.3028 - val_acc: 0.1001\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3027 - acc: 0.1016 - val_loss: 2.3026 - val_acc: 0.1012\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3027 - acc: 0.1014 - val_loss: 2.3024 - val_acc: 0.1020\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3026 - acc: 0.0993 - val_loss: 2.3024 - val_acc: 0.0998\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3026 - acc: 0.1008 - val_loss: 2.3023 - val_acc: 0.1032\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3025 - acc: 0.1026 - val_loss: 2.3023 - val_acc: 0.1045\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3024 - acc: 0.1019 - val_loss: 2.3022 - val_acc: 0.1067\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3024 - acc: 0.1028 - val_loss: 2.3022 - val_acc: 0.0962\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3024 - acc: 0.1019 - val_loss: 2.3022 - val_acc: 0.1077\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3023 - acc: 0.1036 - val_loss: 2.3021 - val_acc: 0.1125\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3022 - acc: 0.1033 - val_loss: 2.3022 - val_acc: 0.1000\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3023 - acc: 0.1028 - val_loss: 2.3021 - val_acc: 0.1013\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3022 - acc: 0.1040 - val_loss: 2.3021 - val_acc: 0.1061\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3021 - acc: 0.1048 - val_loss: 2.3020 - val_acc: 0.1111\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3021 - acc: 0.1043 - val_loss: 2.3020 - val_acc: 0.1078\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.3020 - acc: 0.1039 - val_loss: 2.3021 - val_acc: 0.1015\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3021 - acc: 0.1037 - val_loss: 2.3019 - val_acc: 0.1015\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3020 - acc: 0.1052 - val_loss: 2.3018 - val_acc: 0.1037\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.3020 - acc: 0.1053 - val_loss: 2.3018 - val_acc: 0.1041\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3019 - acc: 0.1047 - val_loss: 2.3018 - val_acc: 0.1081\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3019 - acc: 0.1064 - val_loss: 2.3018 - val_acc: 0.1072\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3019 - acc: 0.1028 - val_loss: 2.3018 - val_acc: 0.1039\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.3018 - acc: 0.1055 - val_loss: 2.3017 - val_acc: 0.1143\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3017 - acc: 0.1055 - val_loss: 2.3018 - val_acc: 0.1142\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3016 - acc: 0.1061 - val_loss: 2.3018 - val_acc: 0.1019\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.3018 - acc: 0.1057 - val_loss: 2.3015 - val_acc: 0.1066\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3017 - acc: 0.1079 - val_loss: 2.3015 - val_acc: 0.1040\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3017 - acc: 0.1078 - val_loss: 2.3015 - val_acc: 0.1039\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3014 - acc: 0.1084 - val_loss: 2.3018 - val_acc: 0.1082\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.3015 - acc: 0.1052 - val_loss: 2.3017 - val_acc: 0.0992\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3014 - acc: 0.1075 - val_loss: 2.3017 - val_acc: 0.1029\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3015 - acc: 0.1057 - val_loss: 2.3013 - val_acc: 0.1061\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3014 - acc: 0.1077 - val_loss: 2.3015 - val_acc: 0.1013\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3014 - acc: 0.1063 - val_loss: 2.3013 - val_acc: 0.1096\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3014 - acc: 0.1086 - val_loss: 2.3013 - val_acc: 0.1166\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3014 - acc: 0.1085 - val_loss: 2.3012 - val_acc: 0.1049\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3013 - acc: 0.1101 - val_loss: 2.3013 - val_acc: 0.1107\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3013 - acc: 0.1059 - val_loss: 2.3012 - val_acc: 0.1158\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.3012 - acc: 0.1110 - val_loss: 2.3012 - val_acc: 0.1061\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3012 - acc: 0.1094 - val_loss: 2.3011 - val_acc: 0.1081\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3012 - acc: 0.1083 - val_loss: 2.3013 - val_acc: 0.1005\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3011 - acc: 0.1099 - val_loss: 2.3011 - val_acc: 0.1132\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3010 - acc: 0.1121 - val_loss: 2.3013 - val_acc: 0.1023\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3011 - acc: 0.1110 - val_loss: 2.3010 - val_acc: 0.1064\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3011 - acc: 0.1106 - val_loss: 2.3009 - val_acc: 0.1101\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3010 - acc: 0.1100 - val_loss: 2.3009 - val_acc: 0.1094\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3009 - acc: 0.1097 - val_loss: 2.3009 - val_acc: 0.1133\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3009 - acc: 0.1113 - val_loss: 2.3009 - val_acc: 0.1195\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3009 - acc: 0.1130 - val_loss: 2.3008 - val_acc: 0.1099\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3009 - acc: 0.1145 - val_loss: 2.3008 - val_acc: 0.1093\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3009 - acc: 0.1117 - val_loss: 2.3008 - val_acc: 0.1138\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3008 - acc: 0.1125 - val_loss: 2.3006 - val_acc: 0.1204\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3008 - acc: 0.1122 - val_loss: 2.3007 - val_acc: 0.1053\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3007 - acc: 0.1095 - val_loss: 2.3008 - val_acc: 0.1187\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3007 - acc: 0.1140 - val_loss: 2.3007 - val_acc: 0.1177\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.3007 - acc: 0.1117 - val_loss: 2.3005 - val_acc: 0.1193\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3006 - acc: 0.1120 - val_loss: 2.3005 - val_acc: 0.1189\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3006 - acc: 0.1135 - val_loss: 2.3005 - val_acc: 0.1089\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3006 - acc: 0.1143 - val_loss: 2.3005 - val_acc: 0.1201\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3004 - acc: 0.1117 - val_loss: 2.3005 - val_acc: 0.1099\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3005 - acc: 0.1138 - val_loss: 2.3004 - val_acc: 0.1221\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3004 - acc: 0.1152 - val_loss: 2.3004 - val_acc: 0.1177\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3004 - acc: 0.1198 - val_loss: 2.3005 - val_acc: 0.1110\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3004 - acc: 0.1152 - val_loss: 2.3003 - val_acc: 0.1209\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3004 - acc: 0.1161 - val_loss: 2.3002 - val_acc: 0.1174\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3003 - acc: 0.1162 - val_loss: 2.3002 - val_acc: 0.1155\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3002 - acc: 0.1153 - val_loss: 2.3002 - val_acc: 0.1095\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3002 - acc: 0.1156 - val_loss: 2.3001 - val_acc: 0.1209\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3002 - acc: 0.1168 - val_loss: 2.3001 - val_acc: 0.1187\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3001 - acc: 0.1217 - val_loss: 2.3003 - val_acc: 0.1151\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3001 - acc: 0.1150 - val_loss: 2.3000 - val_acc: 0.1183\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3001 - acc: 0.1196 - val_loss: 2.3001 - val_acc: 0.1110\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3001 - acc: 0.1145 - val_loss: 2.3000 - val_acc: 0.1260\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.3000 - acc: 0.1196 - val_loss: 2.3001 - val_acc: 0.1094\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.3000 - acc: 0.1144 - val_loss: 2.2999 - val_acc: 0.1208\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2999 - acc: 0.1197 - val_loss: 2.2998 - val_acc: 0.1211\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2999 - acc: 0.1178 - val_loss: 2.2998 - val_acc: 0.1247\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2998 - acc: 0.1186 - val_loss: 2.2997 - val_acc: 0.1234\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2998 - acc: 0.1185 - val_loss: 2.2998 - val_acc: 0.1148\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2998 - acc: 0.1168 - val_loss: 2.2997 - val_acc: 0.1222\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2998 - acc: 0.1189 - val_loss: 2.2997 - val_acc: 0.1289\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2997 - acc: 0.1195 - val_loss: 2.2998 - val_acc: 0.1131\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2997 - acc: 0.1196 - val_loss: 2.2997 - val_acc: 0.1146\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2996 - acc: 0.1196 - val_loss: 2.2995 - val_acc: 0.1250\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2996 - acc: 0.1202 - val_loss: 2.2995 - val_acc: 0.1207\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2995 - acc: 0.1209 - val_loss: 2.2995 - val_acc: 0.1238\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2995 - acc: 0.1215 - val_loss: 2.2995 - val_acc: 0.1158\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2994 - acc: 0.1203 - val_loss: 2.2996 - val_acc: 0.1181\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2994 - acc: 0.1227 - val_loss: 2.2995 - val_acc: 0.1090\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2994 - acc: 0.1210 - val_loss: 2.2994 - val_acc: 0.1194\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2994 - acc: 0.1224 - val_loss: 2.2993 - val_acc: 0.1255\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2993 - acc: 0.1243 - val_loss: 2.2994 - val_acc: 0.1126\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "airVUr8-FQJ8",
    "outputId": "6ca2bcf8-d177-43c5-894f-c610cb411f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with sigmoid activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 2.2994 - acc: 0.1126\n",
      "Validation accuracy: 11.26\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with sigmoid activations'); print('--'*40)\n",
    "results1 = model1.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2rIo2V1Q10H"
   },
   "source": [
    "#### NN model, sigmoid activations, SGD optimizer, changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YTbX96-vGjFj",
    "outputId": "275c3e7d-35df-4a15-c2f1-e26a47eba163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with sigmoid activations - changing learning rate\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2991 - acc: 0.1169 - val_loss: 2.2993 - val_acc: 0.1169\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1221 - val_loss: 2.2992 - val_acc: 0.1207\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2990 - acc: 0.1222 - val_loss: 2.2992 - val_acc: 0.1231\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1248 - val_loss: 2.2992 - val_acc: 0.1244\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1266 - val_loss: 2.2992 - val_acc: 0.1257\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1285 - val_loss: 2.2992 - val_acc: 0.1260\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1264 - val_loss: 2.2991 - val_acc: 0.1277\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1262 - val_loss: 2.2991 - val_acc: 0.1290\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1290 - val_loss: 2.2991 - val_acc: 0.1291\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1296 - val_loss: 2.2991 - val_acc: 0.1291\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2990 - acc: 0.1286 - val_loss: 2.2991 - val_acc: 0.1295\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1320 - val_loss: 2.2991 - val_acc: 0.1291\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2989 - acc: 0.1301 - val_loss: 2.2991 - val_acc: 0.1289\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1279 - val_loss: 2.2991 - val_acc: 0.1287\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1305 - val_loss: 2.2991 - val_acc: 0.1286\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2989 - acc: 0.1282 - val_loss: 2.2991 - val_acc: 0.1289\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1305 - val_loss: 2.2991 - val_acc: 0.1286\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1276 - val_loss: 2.2991 - val_acc: 0.1293\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1284 - val_loss: 2.2991 - val_acc: 0.1300\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2989 - acc: 0.1305 - val_loss: 2.2991 - val_acc: 0.1300\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1314 - val_loss: 2.2991 - val_acc: 0.1294\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1301 - val_loss: 2.2991 - val_acc: 0.1296\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2989 - acc: 0.1304 - val_loss: 2.2991 - val_acc: 0.1293\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2989 - acc: 0.1315 - val_loss: 2.2991 - val_acc: 0.1279\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2989 - acc: 0.1285 - val_loss: 2.2991 - val_acc: 0.1289\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2989 - acc: 0.1281 - val_loss: 2.2991 - val_acc: 0.1292\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2989 - acc: 0.1303 - val_loss: 2.2991 - val_acc: 0.1293\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2989 - acc: 0.1315 - val_loss: 2.2991 - val_acc: 0.1281\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2989 - acc: 0.1288 - val_loss: 2.2991 - val_acc: 0.1285\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2989 - acc: 0.1285 - val_loss: 2.2990 - val_acc: 0.1287\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2989 - acc: 0.1293 - val_loss: 2.2990 - val_acc: 0.1290\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1287 - val_loss: 2.2990 - val_acc: 0.1294\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1316 - val_loss: 2.2990 - val_acc: 0.1288\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1297 - val_loss: 2.2990 - val_acc: 0.1289\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2989 - acc: 0.1300 - val_loss: 2.2990 - val_acc: 0.1294\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2989 - acc: 0.1313 - val_loss: 2.2990 - val_acc: 0.1283\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2988 - acc: 0.1293 - val_loss: 2.2990 - val_acc: 0.1284\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2988 - acc: 0.1300 - val_loss: 2.2990 - val_acc: 0.1288\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1285 - val_loss: 2.2990 - val_acc: 0.1293\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1292 - val_loss: 2.2990 - val_acc: 0.1298\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 2.2988 - acc: 0.1310 - val_loss: 2.2990 - val_acc: 0.1293\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1317 - val_loss: 2.2990 - val_acc: 0.1290\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1301 - val_loss: 2.2990 - val_acc: 0.1291\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1301 - val_loss: 2.2990 - val_acc: 0.1285\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1310 - val_loss: 2.2990 - val_acc: 0.1288\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1300 - val_loss: 2.2990 - val_acc: 0.1286\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1310 - val_loss: 2.2990 - val_acc: 0.1283\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1312 - val_loss: 2.2990 - val_acc: 0.1279\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1260 - val_loss: 2.2990 - val_acc: 0.1295\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1295 - val_loss: 2.2990 - val_acc: 0.1300\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1297 - val_loss: 2.2990 - val_acc: 0.1302\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2988 - acc: 0.1319 - val_loss: 2.2990 - val_acc: 0.1297\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2988 - acc: 0.1313 - val_loss: 2.2990 - val_acc: 0.1287\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1287 - val_loss: 2.2989 - val_acc: 0.1296\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1314 - val_loss: 2.2989 - val_acc: 0.1295\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1299 - val_loss: 2.2989 - val_acc: 0.1292\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2988 - acc: 0.1321 - val_loss: 2.2989 - val_acc: 0.1289\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1298 - val_loss: 2.2989 - val_acc: 0.1293\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2988 - acc: 0.1293 - val_loss: 2.2989 - val_acc: 0.1301\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2988 - acc: 0.1285 - val_loss: 2.2989 - val_acc: 0.1304\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1317 - val_loss: 2.2989 - val_acc: 0.1305\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2987 - acc: 0.1312 - val_loss: 2.2989 - val_acc: 0.1306\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1307 - val_loss: 2.2989 - val_acc: 0.1302\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1331 - val_loss: 2.2989 - val_acc: 0.1294\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1318 - val_loss: 2.2989 - val_acc: 0.1296\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1304 - val_loss: 2.2989 - val_acc: 0.1296\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1306 - val_loss: 2.2989 - val_acc: 0.1300\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1323 - val_loss: 2.2989 - val_acc: 0.1293\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2987 - acc: 0.1300 - val_loss: 2.2989 - val_acc: 0.1298\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2987 - acc: 0.1308 - val_loss: 2.2989 - val_acc: 0.1302\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2987 - acc: 0.1307 - val_loss: 2.2989 - val_acc: 0.1304\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2987 - acc: 0.1316 - val_loss: 2.2989 - val_acc: 0.1309\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2987 - acc: 0.1319 - val_loss: 2.2989 - val_acc: 0.1301\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2987 - acc: 0.1331 - val_loss: 2.2989 - val_acc: 0.1287\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1304 - val_loss: 2.2989 - val_acc: 0.1290\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2987 - acc: 0.1294 - val_loss: 2.2989 - val_acc: 0.1298\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2987 - acc: 0.1307 - val_loss: 2.2989 - val_acc: 0.1300\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1292 - val_loss: 2.2989 - val_acc: 0.1303\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1312 - val_loss: 2.2988 - val_acc: 0.1303\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2987 - acc: 0.1324 - val_loss: 2.2988 - val_acc: 0.1301\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1315 - val_loss: 2.2988 - val_acc: 0.1308\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2987 - acc: 0.1318 - val_loss: 2.2988 - val_acc: 0.1305\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2987 - acc: 0.1308 - val_loss: 2.2988 - val_acc: 0.1310\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2987 - acc: 0.1311 - val_loss: 2.2988 - val_acc: 0.1316\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1324 - val_loss: 2.2988 - val_acc: 0.1314\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1318 - val_loss: 2.2988 - val_acc: 0.1308\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1334 - val_loss: 2.2988 - val_acc: 0.1299\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1312 - val_loss: 2.2988 - val_acc: 0.1305\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1306 - val_loss: 2.2988 - val_acc: 0.1307\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1332 - val_loss: 2.2988 - val_acc: 0.1297\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1320 - val_loss: 2.2988 - val_acc: 0.1299\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 2.2986 - acc: 0.1315 - val_loss: 2.2988 - val_acc: 0.1297\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2986 - acc: 0.1313 - val_loss: 2.2988 - val_acc: 0.1295\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1301 - val_loss: 2.2988 - val_acc: 0.1297\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2986 - acc: 0.1307 - val_loss: 2.2988 - val_acc: 0.1302\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1309 - val_loss: 2.2988 - val_acc: 0.1303\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2986 - acc: 0.1324 - val_loss: 2.2988 - val_acc: 0.1299\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1320 - val_loss: 2.2988 - val_acc: 0.1295\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2986 - acc: 0.1299 - val_loss: 2.2988 - val_acc: 0.1310\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2986 - acc: 0.1308 - val_loss: 2.2988 - val_acc: 0.1307\n"
     ]
    }
   ],
   "source": [
    "print('NN model with sigmoid activations - changing learning rate'); print('--'*40)\n",
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.001)\n",
    "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "scbm_YOeJkap",
    "outputId": "a96146ce-4b68-43da-f4b7-7005e0e5e1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with sigmoid activations - changing learning rate\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 2.2988 - acc: 0.1307\n",
      "Validation accuracy: 13.07\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with sigmoid activations - changing learning rate'); print('--'*40)\n",
    "results1 = model1.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SUgKHYEmt1X"
   },
   "source": [
    "<a id='o3'></a>\n",
    "##### Observation 3 - NN model with sigmoid activations\n",
    "* Validation score is very low, changing learning rate further reduces it.\n",
    "* Optimizing the network in order to better learn the patterns in the dataset.\n",
    "* Best model out of the above is the one with lower learning rate using SGD optimizer and sigmoid activations.\n",
    "* Next, let's use relu activations and see if the score improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3qkfHovRFig"
   },
   "source": [
    "#### NN model, relu activations, SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "BRsNW7ss_iL9",
    "outputId": "0ba86d89-5f13-4e0e-c9bc-2a96bcbee867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 1 Âµs, total: 3 Âµs\n",
      "Wall time: 6.68 Âµs\n",
      "NN model with relu activations and sgd optimizers\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and sgd optimizers'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model2 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu\n",
    "model2.add(Dense(128, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model2.add(Dense(64))\n",
    "# Adding activation function\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model2.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "x4uIPsDZqi8n",
    "outputId": "b633d453-b0bf-4b57-e252-8e313eb8cf12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 140,106\n",
      "Trainable params: 140,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OJzn5I1ORL8q",
    "outputId": "674afc1b-75b8-4cfa-f82e-c27339777082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.3001 - acc: 0.1166 - val_loss: 2.2903 - val_acc: 0.1328\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2854 - acc: 0.1465 - val_loss: 2.2786 - val_acc: 0.1673\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2722 - acc: 0.1781 - val_loss: 2.2643 - val_acc: 0.1904\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 2.2561 - acc: 0.2140 - val_loss: 2.2467 - val_acc: 0.2199\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2369 - acc: 0.2507 - val_loss: 2.2263 - val_acc: 0.2579\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.2128 - acc: 0.2883 - val_loss: 2.1973 - val_acc: 0.3150\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.1828 - acc: 0.3158 - val_loss: 2.1645 - val_acc: 0.3450\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.1463 - acc: 0.3465 - val_loss: 2.1254 - val_acc: 0.3584\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.1025 - acc: 0.3752 - val_loss: 2.0772 - val_acc: 0.3822\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 2.0490 - acc: 0.4008 - val_loss: 2.0188 - val_acc: 0.4164\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.9873 - acc: 0.4255 - val_loss: 1.9517 - val_acc: 0.4352\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.9183 - acc: 0.4470 - val_loss: 1.8801 - val_acc: 0.4531\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.8455 - acc: 0.4674 - val_loss: 1.8076 - val_acc: 0.4808\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.7732 - acc: 0.4886 - val_loss: 1.7343 - val_acc: 0.4963\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.7041 - acc: 0.5085 - val_loss: 1.6676 - val_acc: 0.5157\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.6407 - acc: 0.5236 - val_loss: 1.6103 - val_acc: 0.5370\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.5833 - acc: 0.5417 - val_loss: 1.5525 - val_acc: 0.5501\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.5326 - acc: 0.5534 - val_loss: 1.5026 - val_acc: 0.5629\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.4851 - acc: 0.5659 - val_loss: 1.4584 - val_acc: 0.5728\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.4431 - acc: 0.5756 - val_loss: 1.4193 - val_acc: 0.5908\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.4061 - acc: 0.5859 - val_loss: 1.3822 - val_acc: 0.5966\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.3737 - acc: 0.5931 - val_loss: 1.3552 - val_acc: 0.5914\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.3442 - acc: 0.6000 - val_loss: 1.3209 - val_acc: 0.6088\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.3146 - acc: 0.6076 - val_loss: 1.2990 - val_acc: 0.6103\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.2933 - acc: 0.6137 - val_loss: 1.2714 - val_acc: 0.6263\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.2681 - acc: 0.6206 - val_loss: 1.2635 - val_acc: 0.6183\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.2485 - acc: 0.6246 - val_loss: 1.2253 - val_acc: 0.6389\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.2259 - acc: 0.6333 - val_loss: 1.2039 - val_acc: 0.6449\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.2094 - acc: 0.6355 - val_loss: 1.1899 - val_acc: 0.6465\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.1919 - acc: 0.6437 - val_loss: 1.1801 - val_acc: 0.6503\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.1786 - acc: 0.6477 - val_loss: 1.1587 - val_acc: 0.6580\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1616 - acc: 0.6523 - val_loss: 1.1568 - val_acc: 0.6514\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1512 - acc: 0.6531 - val_loss: 1.1345 - val_acc: 0.6618\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1352 - acc: 0.6613 - val_loss: 1.1188 - val_acc: 0.6700\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.1242 - acc: 0.6632 - val_loss: 1.1040 - val_acc: 0.6727\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1088 - acc: 0.6689 - val_loss: 1.0946 - val_acc: 0.6725\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0974 - acc: 0.6719 - val_loss: 1.0941 - val_acc: 0.6751\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0869 - acc: 0.6758 - val_loss: 1.0704 - val_acc: 0.6830\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0775 - acc: 0.6783 - val_loss: 1.0789 - val_acc: 0.6772\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0685 - acc: 0.6812 - val_loss: 1.0560 - val_acc: 0.6874\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0565 - acc: 0.6835 - val_loss: 1.0430 - val_acc: 0.6918\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0457 - acc: 0.6873 - val_loss: 1.0934 - val_acc: 0.6640\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.0348 - acc: 0.6916 - val_loss: 1.0173 - val_acc: 0.6985\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.0230 - acc: 0.6930 - val_loss: 1.0077 - val_acc: 0.7038\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 1.0125 - acc: 0.6962 - val_loss: 0.9966 - val_acc: 0.7057\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0083 - acc: 0.6993 - val_loss: 1.0228 - val_acc: 0.6914\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9951 - acc: 0.7034 - val_loss: 1.0381 - val_acc: 0.6836\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9866 - acc: 0.7040 - val_loss: 0.9950 - val_acc: 0.7046\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.9829 - acc: 0.7075 - val_loss: 0.9727 - val_acc: 0.7103\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9720 - acc: 0.7098 - val_loss: 0.9611 - val_acc: 0.7149\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9621 - acc: 0.7136 - val_loss: 0.9521 - val_acc: 0.7190\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9557 - acc: 0.7144 - val_loss: 0.9603 - val_acc: 0.7140\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9464 - acc: 0.7175 - val_loss: 0.9550 - val_acc: 0.7157\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.9408 - acc: 0.7178 - val_loss: 0.9284 - val_acc: 0.7271\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9319 - acc: 0.7215 - val_loss: 0.9394 - val_acc: 0.7183\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.9281 - acc: 0.7238 - val_loss: 0.9207 - val_acc: 0.7272\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9127 - acc: 0.7280 - val_loss: 0.9036 - val_acc: 0.7338\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.9087 - acc: 0.7296 - val_loss: 0.9100 - val_acc: 0.7281\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9039 - acc: 0.7315 - val_loss: 0.9017 - val_acc: 0.7313\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8953 - acc: 0.7330 - val_loss: 0.8901 - val_acc: 0.7363\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8903 - acc: 0.7342 - val_loss: 0.9211 - val_acc: 0.7243\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8842 - acc: 0.7363 - val_loss: 0.8956 - val_acc: 0.7299\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8779 - acc: 0.7378 - val_loss: 0.8624 - val_acc: 0.7475\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8720 - acc: 0.7404 - val_loss: 0.8584 - val_acc: 0.7469\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.8632 - acc: 0.7435 - val_loss: 0.8876 - val_acc: 0.7364\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8595 - acc: 0.7448 - val_loss: 0.8548 - val_acc: 0.7465\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8490 - acc: 0.7484 - val_loss: 0.8642 - val_acc: 0.7442\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8486 - acc: 0.7457 - val_loss: 0.8481 - val_acc: 0.7492\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8393 - acc: 0.7504 - val_loss: 0.8327 - val_acc: 0.7544\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8353 - acc: 0.7501 - val_loss: 0.8289 - val_acc: 0.7560\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8261 - acc: 0.7539 - val_loss: 0.8208 - val_acc: 0.7594\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8244 - acc: 0.7542 - val_loss: 0.8829 - val_acc: 0.7296\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8219 - acc: 0.7559 - val_loss: 0.8300 - val_acc: 0.7531\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8139 - acc: 0.7574 - val_loss: 0.8140 - val_acc: 0.7588\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8078 - acc: 0.7588 - val_loss: 0.8330 - val_acc: 0.7498\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8063 - acc: 0.7593 - val_loss: 0.7989 - val_acc: 0.7649\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8026 - acc: 0.7609 - val_loss: 0.8428 - val_acc: 0.7449\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7969 - acc: 0.7625 - val_loss: 0.8424 - val_acc: 0.7461\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7871 - acc: 0.7662 - val_loss: 0.8011 - val_acc: 0.7637\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7874 - acc: 0.7667 - val_loss: 0.7827 - val_acc: 0.7681\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7836 - acc: 0.7662 - val_loss: 0.7725 - val_acc: 0.7734\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7790 - acc: 0.7682 - val_loss: 0.7892 - val_acc: 0.7655\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7736 - acc: 0.7713 - val_loss: 0.7670 - val_acc: 0.7764\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.7643 - acc: 0.7731 - val_loss: 0.7617 - val_acc: 0.7786\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7653 - acc: 0.7727 - val_loss: 0.7642 - val_acc: 0.7736\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7625 - acc: 0.7740 - val_loss: 0.7830 - val_acc: 0.7649\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7564 - acc: 0.7759 - val_loss: 0.7579 - val_acc: 0.7788\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7540 - acc: 0.7751 - val_loss: 0.7516 - val_acc: 0.7801\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7418 - acc: 0.7804 - val_loss: 0.7488 - val_acc: 0.7813\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7452 - acc: 0.7795 - val_loss: 0.7650 - val_acc: 0.7742\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7419 - acc: 0.7801 - val_loss: 0.7780 - val_acc: 0.7645\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7399 - acc: 0.7804 - val_loss: 0.7546 - val_acc: 0.7770\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7336 - acc: 0.7807 - val_loss: 0.7553 - val_acc: 0.7751\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7345 - acc: 0.7808 - val_loss: 0.7326 - val_acc: 0.7833\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7269 - acc: 0.7834 - val_loss: 0.7250 - val_acc: 0.7897\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7175 - acc: 0.7872 - val_loss: 0.7265 - val_acc: 0.7863\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.7217 - acc: 0.7840 - val_loss: 0.7476 - val_acc: 0.7760\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7156 - acc: 0.7873 - val_loss: 0.7110 - val_acc: 0.7939\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7118 - acc: 0.7906 - val_loss: 0.7762 - val_acc: 0.7646\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.7070 - acc: 0.7895 - val_loss: 0.7015 - val_acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "RGwKAGtRDEm8",
    "outputId": "071ffd46-eaf6-4cd6-ad9f-d03e610e4ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.7015 - acc: 0.7965\n",
      "Validation accuracy: 79.65\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4p0ZX0GTRRyG"
   },
   "source": [
    "#### NN model, relu activations, SGD optimizer, changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DWIBrdDdGnDY",
    "outputId": "9bf968c3-4375-4a81-daf3-aa8f6f228898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 1e+03 ns, total: 3 Âµs\n",
      "Wall time: 15.5 Âµs\n",
      "NN model with relu activations and sgd optimizers - changing learning rate\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6704 - acc: 0.8051 - val_loss: 0.6922 - val_acc: 0.8003\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6680 - acc: 0.8054 - val_loss: 0.6917 - val_acc: 0.7996\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6679 - acc: 0.8055 - val_loss: 0.6901 - val_acc: 0.8005\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6671 - acc: 0.8058 - val_loss: 0.6899 - val_acc: 0.7997\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6665 - acc: 0.8065 - val_loss: 0.6900 - val_acc: 0.8002\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6665 - acc: 0.8061 - val_loss: 0.6891 - val_acc: 0.8007\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6659 - acc: 0.8061 - val_loss: 0.6901 - val_acc: 0.8005\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6655 - acc: 0.8067 - val_loss: 0.6885 - val_acc: 0.8010\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6651 - acc: 0.8063 - val_loss: 0.6880 - val_acc: 0.8008\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6648 - acc: 0.8069 - val_loss: 0.6878 - val_acc: 0.8016\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6643 - acc: 0.8066 - val_loss: 0.6878 - val_acc: 0.8010\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6637 - acc: 0.8069 - val_loss: 0.6871 - val_acc: 0.8013\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6638 - acc: 0.8066 - val_loss: 0.6866 - val_acc: 0.8017\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6632 - acc: 0.8067 - val_loss: 0.6865 - val_acc: 0.8019\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6627 - acc: 0.8073 - val_loss: 0.6869 - val_acc: 0.8012\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6625 - acc: 0.8076 - val_loss: 0.6858 - val_acc: 0.8022\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6623 - acc: 0.8075 - val_loss: 0.6851 - val_acc: 0.8023\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6619 - acc: 0.8079 - val_loss: 0.6852 - val_acc: 0.8011\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6611 - acc: 0.8081 - val_loss: 0.6861 - val_acc: 0.8014\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6610 - acc: 0.8076 - val_loss: 0.6847 - val_acc: 0.8024\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6606 - acc: 0.8094 - val_loss: 0.6845 - val_acc: 0.8022\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6603 - acc: 0.8082 - val_loss: 0.6841 - val_acc: 0.8021\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6602 - acc: 0.8080 - val_loss: 0.6833 - val_acc: 0.8022\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6594 - acc: 0.8084 - val_loss: 0.6835 - val_acc: 0.8027\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6592 - acc: 0.8084 - val_loss: 0.6834 - val_acc: 0.8022\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6587 - acc: 0.8086 - val_loss: 0.6830 - val_acc: 0.8022\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6585 - acc: 0.8086 - val_loss: 0.6827 - val_acc: 0.8020\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6582 - acc: 0.8083 - val_loss: 0.6815 - val_acc: 0.8029\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6578 - acc: 0.8085 - val_loss: 0.6820 - val_acc: 0.8035\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6573 - acc: 0.8087 - val_loss: 0.6811 - val_acc: 0.8037\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6568 - acc: 0.8087 - val_loss: 0.6810 - val_acc: 0.8034\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6568 - acc: 0.8094 - val_loss: 0.6814 - val_acc: 0.8037\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6561 - acc: 0.8095 - val_loss: 0.6820 - val_acc: 0.8025\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6561 - acc: 0.8091 - val_loss: 0.6800 - val_acc: 0.8039\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6556 - acc: 0.8095 - val_loss: 0.6815 - val_acc: 0.8023\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6553 - acc: 0.8102 - val_loss: 0.6793 - val_acc: 0.8039\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6552 - acc: 0.8091 - val_loss: 0.6786 - val_acc: 0.8048\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6545 - acc: 0.8091 - val_loss: 0.6787 - val_acc: 0.8039\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6544 - acc: 0.8102 - val_loss: 0.6791 - val_acc: 0.8040\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6538 - acc: 0.8104 - val_loss: 0.6795 - val_acc: 0.8035\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6536 - acc: 0.8103 - val_loss: 0.6788 - val_acc: 0.8041\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6531 - acc: 0.8098 - val_loss: 0.6793 - val_acc: 0.8042\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6528 - acc: 0.8108 - val_loss: 0.6783 - val_acc: 0.8041\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6525 - acc: 0.8099 - val_loss: 0.6770 - val_acc: 0.8048\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6522 - acc: 0.8120 - val_loss: 0.6772 - val_acc: 0.8040\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6518 - acc: 0.8104 - val_loss: 0.6773 - val_acc: 0.8048\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6514 - acc: 0.8103 - val_loss: 0.6762 - val_acc: 0.8050\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6512 - acc: 0.8111 - val_loss: 0.6762 - val_acc: 0.8047\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6509 - acc: 0.8113 - val_loss: 0.6750 - val_acc: 0.8047\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6503 - acc: 0.8108 - val_loss: 0.6763 - val_acc: 0.8041\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6502 - acc: 0.8111 - val_loss: 0.6752 - val_acc: 0.8049\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6497 - acc: 0.8110 - val_loss: 0.6747 - val_acc: 0.8061\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6495 - acc: 0.8118 - val_loss: 0.6739 - val_acc: 0.8058\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6493 - acc: 0.8111 - val_loss: 0.6742 - val_acc: 0.8049\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6491 - acc: 0.8117 - val_loss: 0.6735 - val_acc: 0.8054\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6483 - acc: 0.8115 - val_loss: 0.6735 - val_acc: 0.8059\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6482 - acc: 0.8117 - val_loss: 0.6736 - val_acc: 0.8061\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6479 - acc: 0.8123 - val_loss: 0.6735 - val_acc: 0.8053\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6475 - acc: 0.8125 - val_loss: 0.6731 - val_acc: 0.8060\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6472 - acc: 0.8123 - val_loss: 0.6724 - val_acc: 0.8061\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6469 - acc: 0.8119 - val_loss: 0.6721 - val_acc: 0.8064\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6465 - acc: 0.8119 - val_loss: 0.6723 - val_acc: 0.8060\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6463 - acc: 0.8126 - val_loss: 0.6713 - val_acc: 0.8056\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6460 - acc: 0.8130 - val_loss: 0.6707 - val_acc: 0.8064\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6454 - acc: 0.8123 - val_loss: 0.6711 - val_acc: 0.8061\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6453 - acc: 0.8136 - val_loss: 0.6703 - val_acc: 0.8069\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6449 - acc: 0.8130 - val_loss: 0.6699 - val_acc: 0.8062\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6446 - acc: 0.8127 - val_loss: 0.6698 - val_acc: 0.8075\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6442 - acc: 0.8132 - val_loss: 0.6693 - val_acc: 0.8071\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6437 - acc: 0.8129 - val_loss: 0.6696 - val_acc: 0.8058\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6437 - acc: 0.8131 - val_loss: 0.6694 - val_acc: 0.8057\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6433 - acc: 0.8131 - val_loss: 0.6692 - val_acc: 0.8067\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6430 - acc: 0.8137 - val_loss: 0.6687 - val_acc: 0.8073\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6428 - acc: 0.8131 - val_loss: 0.6690 - val_acc: 0.8073\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6425 - acc: 0.8139 - val_loss: 0.6676 - val_acc: 0.8073\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6419 - acc: 0.8140 - val_loss: 0.6672 - val_acc: 0.8077\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6418 - acc: 0.8141 - val_loss: 0.6672 - val_acc: 0.8074\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6415 - acc: 0.8141 - val_loss: 0.6676 - val_acc: 0.8061\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6411 - acc: 0.8145 - val_loss: 0.6669 - val_acc: 0.8072\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6409 - acc: 0.8139 - val_loss: 0.6685 - val_acc: 0.8061\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 25us/sample - loss: 0.6408 - acc: 0.8138 - val_loss: 0.6668 - val_acc: 0.8075\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6404 - acc: 0.8141 - val_loss: 0.6655 - val_acc: 0.8078\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6398 - acc: 0.8137 - val_loss: 0.6651 - val_acc: 0.8077\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6394 - acc: 0.8151 - val_loss: 0.6666 - val_acc: 0.8076\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6395 - acc: 0.8140 - val_loss: 0.6655 - val_acc: 0.8083\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6392 - acc: 0.8147 - val_loss: 0.6653 - val_acc: 0.8075\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6387 - acc: 0.8147 - val_loss: 0.6643 - val_acc: 0.8076\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6384 - acc: 0.8151 - val_loss: 0.6646 - val_acc: 0.8078\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6380 - acc: 0.8145 - val_loss: 0.6636 - val_acc: 0.8078\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6375 - acc: 0.8148 - val_loss: 0.6638 - val_acc: 0.8083\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6376 - acc: 0.8157 - val_loss: 0.6637 - val_acc: 0.8088\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6370 - acc: 0.8152 - val_loss: 0.6632 - val_acc: 0.8087\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6367 - acc: 0.8152 - val_loss: 0.6636 - val_acc: 0.8081\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6367 - acc: 0.8140 - val_loss: 0.6623 - val_acc: 0.8089\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6362 - acc: 0.8158 - val_loss: 0.6631 - val_acc: 0.8092\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6358 - acc: 0.8150 - val_loss: 0.6630 - val_acc: 0.8080\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6358 - acc: 0.8151 - val_loss: 0.6615 - val_acc: 0.8091\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6353 - acc: 0.8160 - val_loss: 0.6622 - val_acc: 0.8086\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6350 - acc: 0.8152 - val_loss: 0.6614 - val_acc: 0.8087\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6348 - acc: 0.8156 - val_loss: 0.6611 - val_acc: 0.8088\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and sgd optimizers - changing learning rate'); print('--'*40)\n",
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.001)\n",
    "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "9O9KhqH5HMWW",
    "outputId": "03347a3f-e86f-45b9-dce0-8d47001f6b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.6611 - acc: 0.8088\n",
      "Validation accuracy: 80.88\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CKJFAr9JRfJJ"
   },
   "source": [
    "#### NN model, relu activations, adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9Q8PtyA-Jw74",
    "outputId": "15020f47-a35d-45bf-9090-cd6556442388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs\n",
      "Wall time: 5.01 Âµs\n",
      "NN model with relu activations and adam optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 34us/sample - loss: 3.3943 - acc: 0.1031 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 2.3013 - acc: 0.1004 - val_loss: 2.3027 - val_acc: 0.1014\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 2.2895 - acc: 0.1191 - val_loss: 2.2743 - val_acc: 0.1331\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 2.2683 - acc: 0.1293 - val_loss: 2.2518 - val_acc: 0.1432\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.1576 - acc: 0.1853 - val_loss: 2.0355 - val_acc: 0.2498\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.8345 - acc: 0.3417 - val_loss: 1.6677 - val_acc: 0.4254\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.5568 - acc: 0.4742 - val_loss: 1.4540 - val_acc: 0.5070\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.3969 - acc: 0.5453 - val_loss: 1.4334 - val_acc: 0.5188\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.3505 - acc: 0.5663 - val_loss: 1.3200 - val_acc: 0.5760\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2855 - acc: 0.5901 - val_loss: 1.2031 - val_acc: 0.6226\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2371 - acc: 0.6069 - val_loss: 1.2411 - val_acc: 0.6005\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2108 - acc: 0.6166 - val_loss: 1.2598 - val_acc: 0.5979\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2105 - acc: 0.6179 - val_loss: 1.2068 - val_acc: 0.6124\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1587 - acc: 0.6356 - val_loss: 1.1786 - val_acc: 0.6265\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1466 - acc: 0.6399 - val_loss: 1.0966 - val_acc: 0.6581\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.1529 - acc: 0.6390 - val_loss: 1.2340 - val_acc: 0.6092\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1638 - acc: 0.6342 - val_loss: 1.1474 - val_acc: 0.6446\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1361 - acc: 0.6437 - val_loss: 1.1375 - val_acc: 0.6424\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1319 - acc: 0.6441 - val_loss: 1.1487 - val_acc: 0.6376\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1126 - acc: 0.6519 - val_loss: 1.0731 - val_acc: 0.6636\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1100 - acc: 0.6529 - val_loss: 1.1204 - val_acc: 0.6526\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1103 - acc: 0.6528 - val_loss: 1.1775 - val_acc: 0.6292\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1069 - acc: 0.6525 - val_loss: 1.0985 - val_acc: 0.6520\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0999 - acc: 0.6546 - val_loss: 1.0967 - val_acc: 0.6591\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0973 - acc: 0.6555 - val_loss: 1.0405 - val_acc: 0.6792\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1014 - acc: 0.6541 - val_loss: 1.1677 - val_acc: 0.6265\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0893 - acc: 0.6584 - val_loss: 1.0540 - val_acc: 0.6711\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0889 - acc: 0.6568 - val_loss: 1.0725 - val_acc: 0.6622\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0825 - acc: 0.6615 - val_loss: 1.1438 - val_acc: 0.6394\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0843 - acc: 0.6596 - val_loss: 1.0277 - val_acc: 0.6816\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0669 - acc: 0.6661 - val_loss: 1.0409 - val_acc: 0.6762\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.0803 - acc: 0.6599 - val_loss: 1.0849 - val_acc: 0.6575\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0737 - acc: 0.6627 - val_loss: 1.1215 - val_acc: 0.6391\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0673 - acc: 0.6649 - val_loss: 1.0624 - val_acc: 0.6672\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0648 - acc: 0.6656 - val_loss: 1.0486 - val_acc: 0.6743\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0647 - acc: 0.6647 - val_loss: 1.0660 - val_acc: 0.6658\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0650 - acc: 0.6636 - val_loss: 1.0980 - val_acc: 0.6513\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0624 - acc: 0.6673 - val_loss: 1.0543 - val_acc: 0.6686\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0690 - acc: 0.6653 - val_loss: 1.0335 - val_acc: 0.6793\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0487 - acc: 0.6729 - val_loss: 1.0449 - val_acc: 0.6762\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0556 - acc: 0.6700 - val_loss: 1.0483 - val_acc: 0.6722\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0526 - acc: 0.6711 - val_loss: 1.0227 - val_acc: 0.6811\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0472 - acc: 0.6733 - val_loss: 1.0363 - val_acc: 0.6783\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0532 - acc: 0.6717 - val_loss: 1.0223 - val_acc: 0.6808\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0416 - acc: 0.6732 - val_loss: 1.0771 - val_acc: 0.6583\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0372 - acc: 0.6736 - val_loss: 1.0675 - val_acc: 0.6671\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0606 - acc: 0.6655 - val_loss: 1.0282 - val_acc: 0.6787\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0661 - acc: 0.6638 - val_loss: 1.0242 - val_acc: 0.6798\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0452 - acc: 0.6718 - val_loss: 1.0708 - val_acc: 0.6636\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0386 - acc: 0.6764 - val_loss: 1.0317 - val_acc: 0.6770\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0393 - acc: 0.6755 - val_loss: 1.0551 - val_acc: 0.6654\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0469 - acc: 0.6717 - val_loss: 1.0547 - val_acc: 0.6689\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0485 - acc: 0.6729 - val_loss: 1.0864 - val_acc: 0.6599\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0401 - acc: 0.6743 - val_loss: 0.9906 - val_acc: 0.6897\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0368 - acc: 0.6735 - val_loss: 1.0132 - val_acc: 0.6853\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0445 - acc: 0.6722 - val_loss: 1.0171 - val_acc: 0.6813\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 1.0387 - acc: 0.6763 - val_loss: 1.0498 - val_acc: 0.6689\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0460 - acc: 0.6731 - val_loss: 1.0686 - val_acc: 0.6675\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0375 - acc: 0.6751 - val_loss: 1.0465 - val_acc: 0.6723\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0389 - acc: 0.6742 - val_loss: 1.0595 - val_acc: 0.6651\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0250 - acc: 0.6791 - val_loss: 1.0227 - val_acc: 0.6823\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0342 - acc: 0.6758 - val_loss: 1.0227 - val_acc: 0.6824\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0375 - acc: 0.6764 - val_loss: 1.0060 - val_acc: 0.6898\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0230 - acc: 0.6793 - val_loss: 1.0115 - val_acc: 0.6835\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0224 - acc: 0.6798 - val_loss: 1.0603 - val_acc: 0.6657\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0352 - acc: 0.6760 - val_loss: 1.0281 - val_acc: 0.6746\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0335 - acc: 0.6775 - val_loss: 0.9844 - val_acc: 0.6960\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0224 - acc: 0.6811 - val_loss: 1.0320 - val_acc: 0.6740\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 1.0351 - acc: 0.6750 - val_loss: 1.0805 - val_acc: 0.6631\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0223 - acc: 0.6813 - val_loss: 1.0350 - val_acc: 0.6804\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0255 - acc: 0.6781 - val_loss: 1.0166 - val_acc: 0.6827\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0279 - acc: 0.6781 - val_loss: 1.0422 - val_acc: 0.6733\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0341 - acc: 0.6777 - val_loss: 0.9790 - val_acc: 0.6986\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0214 - acc: 0.6818 - val_loss: 1.0892 - val_acc: 0.6592\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0283 - acc: 0.6802 - val_loss: 1.0154 - val_acc: 0.6855\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0244 - acc: 0.6779 - val_loss: 1.0062 - val_acc: 0.6894\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0194 - acc: 0.6811 - val_loss: 1.0128 - val_acc: 0.6851\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0110 - acc: 0.6831 - val_loss: 1.0117 - val_acc: 0.6898\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0215 - acc: 0.6807 - val_loss: 1.0087 - val_acc: 0.6883\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0231 - acc: 0.6806 - val_loss: 1.0040 - val_acc: 0.6889\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0298 - acc: 0.6776 - val_loss: 1.0346 - val_acc: 0.6800\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0202 - acc: 0.6811 - val_loss: 1.0856 - val_acc: 0.6620\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0209 - acc: 0.6805 - val_loss: 0.9843 - val_acc: 0.6952\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0093 - acc: 0.6862 - val_loss: 1.0686 - val_acc: 0.6638\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0101 - acc: 0.6857 - val_loss: 0.9739 - val_acc: 0.6989\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0075 - acc: 0.6861 - val_loss: 0.9899 - val_acc: 0.6946\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0080 - acc: 0.6878 - val_loss: 1.0014 - val_acc: 0.6913\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0107 - acc: 0.6852 - val_loss: 1.0051 - val_acc: 0.6876\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0133 - acc: 0.6821 - val_loss: 1.0165 - val_acc: 0.6839\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0023 - acc: 0.6889 - val_loss: 0.9863 - val_acc: 0.6931\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0083 - acc: 0.6878 - val_loss: 0.9998 - val_acc: 0.6890\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0092 - acc: 0.6862 - val_loss: 0.9811 - val_acc: 0.6973\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0127 - acc: 0.6836 - val_loss: 1.0062 - val_acc: 0.6899\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9985 - acc: 0.6901 - val_loss: 0.9626 - val_acc: 0.7035\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9983 - acc: 0.6881 - val_loss: 0.9701 - val_acc: 0.7008\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9938 - acc: 0.6915 - val_loss: 0.9868 - val_acc: 0.6932\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0051 - acc: 0.6875 - val_loss: 1.0199 - val_acc: 0.6880\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0072 - acc: 0.6864 - val_loss: 0.9687 - val_acc: 0.7041\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0078 - acc: 0.6880 - val_loss: 0.9660 - val_acc: 0.7028\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9940 - acc: 0.6921 - val_loss: 1.0037 - val_acc: 0.6911\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.01)\n",
    "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "pAICaMQJKFE5",
    "outputId": "17b397c4-bf5e-44fc-ae73-4959635540eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 1.0037 - acc: 0.6911\n",
      "Validation accuracy: 69.11\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JT_j1efHRpGa"
   },
   "source": [
    "#### NN model, relu activations, adam optimizer, changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "j6CfiDwmRscX",
    "outputId": "66419ce6-9a33-4623-f0a3-e91822b6004a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 1e+03 ns, total: 3 Âµs\n",
      "Wall time: 4.77 Âµs\n",
      "NN model with relu activations and adam optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.9207 - acc: 0.7166 - val_loss: 0.9223 - val_acc: 0.7184\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9098 - acc: 0.7203 - val_loss: 0.9175 - val_acc: 0.7193\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9100 - acc: 0.7193 - val_loss: 0.9187 - val_acc: 0.7174\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.9088 - acc: 0.7202 - val_loss: 0.9171 - val_acc: 0.7187\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9080 - acc: 0.7191 - val_loss: 0.9185 - val_acc: 0.7184\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9064 - acc: 0.7220 - val_loss: 0.9181 - val_acc: 0.7187\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.9074 - acc: 0.7217 - val_loss: 0.9131 - val_acc: 0.7214\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9038 - acc: 0.7233 - val_loss: 0.9160 - val_acc: 0.7208\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9038 - acc: 0.7221 - val_loss: 0.9150 - val_acc: 0.7202\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9043 - acc: 0.7222 - val_loss: 0.9147 - val_acc: 0.7218\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9017 - acc: 0.7234 - val_loss: 0.9187 - val_acc: 0.7187\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9026 - acc: 0.7225 - val_loss: 0.9187 - val_acc: 0.7187\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9057 - acc: 0.7221 - val_loss: 0.9153 - val_acc: 0.7211\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9016 - acc: 0.7229 - val_loss: 0.9116 - val_acc: 0.7207\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9031 - acc: 0.7222 - val_loss: 0.9213 - val_acc: 0.7177\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9001 - acc: 0.7229 - val_loss: 0.9053 - val_acc: 0.7233\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9013 - acc: 0.7225 - val_loss: 0.9189 - val_acc: 0.7166\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8989 - acc: 0.7245 - val_loss: 0.9059 - val_acc: 0.7229\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.9003 - acc: 0.7237 - val_loss: 0.9150 - val_acc: 0.7199\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9003 - acc: 0.7231 - val_loss: 0.9091 - val_acc: 0.7216\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8980 - acc: 0.7254 - val_loss: 0.9098 - val_acc: 0.7218\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8977 - acc: 0.7244 - val_loss: 0.9094 - val_acc: 0.7217\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8987 - acc: 0.7237 - val_loss: 0.9145 - val_acc: 0.7200\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8967 - acc: 0.7244 - val_loss: 0.9107 - val_acc: 0.7208\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8976 - acc: 0.7246 - val_loss: 0.9145 - val_acc: 0.7199\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8976 - acc: 0.7237 - val_loss: 0.9047 - val_acc: 0.7238\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8966 - acc: 0.7230 - val_loss: 0.9074 - val_acc: 0.7229\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8947 - acc: 0.7263 - val_loss: 0.9040 - val_acc: 0.7226\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8959 - acc: 0.7252 - val_loss: 0.9215 - val_acc: 0.7157\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8937 - acc: 0.7264 - val_loss: 0.9097 - val_acc: 0.7207\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8959 - acc: 0.7242 - val_loss: 0.9120 - val_acc: 0.7211\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8944 - acc: 0.7248 - val_loss: 0.9027 - val_acc: 0.7243\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8942 - acc: 0.7254 - val_loss: 0.9032 - val_acc: 0.7250\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8928 - acc: 0.7254 - val_loss: 0.9032 - val_acc: 0.7240\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8926 - acc: 0.7265 - val_loss: 0.9091 - val_acc: 0.7202\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8949 - acc: 0.7246 - val_loss: 0.9041 - val_acc: 0.7226\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8940 - acc: 0.7237 - val_loss: 0.9028 - val_acc: 0.7241\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8925 - acc: 0.7257 - val_loss: 0.9063 - val_acc: 0.7229\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8917 - acc: 0.7259 - val_loss: 0.9024 - val_acc: 0.7245\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8943 - acc: 0.7253 - val_loss: 0.9051 - val_acc: 0.7239\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8918 - acc: 0.7263 - val_loss: 0.9123 - val_acc: 0.7199\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8918 - acc: 0.7262 - val_loss: 0.9091 - val_acc: 0.7220\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8912 - acc: 0.7266 - val_loss: 0.9088 - val_acc: 0.7209\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8900 - acc: 0.7281 - val_loss: 0.9084 - val_acc: 0.7224\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8904 - acc: 0.7269 - val_loss: 0.9066 - val_acc: 0.7211\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8903 - acc: 0.7260 - val_loss: 0.9017 - val_acc: 0.7251\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8908 - acc: 0.7267 - val_loss: 0.9037 - val_acc: 0.7225\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8892 - acc: 0.7248 - val_loss: 0.9043 - val_acc: 0.7222\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8892 - acc: 0.7263 - val_loss: 0.8993 - val_acc: 0.7247\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8902 - acc: 0.7260 - val_loss: 0.9123 - val_acc: 0.7200\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8890 - acc: 0.7261 - val_loss: 0.9017 - val_acc: 0.7247\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8873 - acc: 0.7271 - val_loss: 0.8991 - val_acc: 0.7254\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8879 - acc: 0.7272 - val_loss: 0.8996 - val_acc: 0.7247\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8880 - acc: 0.7273 - val_loss: 0.9110 - val_acc: 0.7222\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8882 - acc: 0.7275 - val_loss: 0.9008 - val_acc: 0.7245\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8879 - acc: 0.7273 - val_loss: 0.9052 - val_acc: 0.7240\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8860 - acc: 0.7280 - val_loss: 0.9027 - val_acc: 0.7250\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8857 - acc: 0.7295 - val_loss: 0.8991 - val_acc: 0.7258\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8899 - acc: 0.7264 - val_loss: 0.9094 - val_acc: 0.7238\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8868 - acc: 0.7277 - val_loss: 0.9053 - val_acc: 0.7228\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8864 - acc: 0.7288 - val_loss: 0.8992 - val_acc: 0.7254\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8879 - acc: 0.7276 - val_loss: 0.8937 - val_acc: 0.7282\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8860 - acc: 0.7279 - val_loss: 0.9124 - val_acc: 0.7200\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8867 - acc: 0.7286 - val_loss: 0.9027 - val_acc: 0.7235\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8852 - acc: 0.7286 - val_loss: 0.8949 - val_acc: 0.7265\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8861 - acc: 0.7287 - val_loss: 0.8954 - val_acc: 0.7266\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8840 - acc: 0.7283 - val_loss: 0.9055 - val_acc: 0.7222\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8838 - acc: 0.7285 - val_loss: 0.8979 - val_acc: 0.7258\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8849 - acc: 0.7277 - val_loss: 0.8946 - val_acc: 0.7275\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8830 - acc: 0.7291 - val_loss: 0.8960 - val_acc: 0.7247\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8848 - acc: 0.7285 - val_loss: 0.8995 - val_acc: 0.7262\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8834 - acc: 0.7289 - val_loss: 0.9017 - val_acc: 0.7239\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8826 - acc: 0.7281 - val_loss: 0.8945 - val_acc: 0.7273\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8825 - acc: 0.7292 - val_loss: 0.8963 - val_acc: 0.7259\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8826 - acc: 0.7280 - val_loss: 0.8977 - val_acc: 0.7272\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8830 - acc: 0.7291 - val_loss: 0.8935 - val_acc: 0.7268\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8825 - acc: 0.7294 - val_loss: 0.8943 - val_acc: 0.7264\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8835 - acc: 0.7282 - val_loss: 0.8985 - val_acc: 0.7260\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8825 - acc: 0.7271 - val_loss: 0.8932 - val_acc: 0.7282\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8817 - acc: 0.7296 - val_loss: 0.8885 - val_acc: 0.7285\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8824 - acc: 0.7278 - val_loss: 0.8929 - val_acc: 0.7276\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8817 - acc: 0.7291 - val_loss: 0.8922 - val_acc: 0.7279\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8800 - acc: 0.7310 - val_loss: 0.9002 - val_acc: 0.7251\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8794 - acc: 0.7299 - val_loss: 0.8957 - val_acc: 0.7252\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8807 - acc: 0.7294 - val_loss: 0.8998 - val_acc: 0.7242\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8795 - acc: 0.7294 - val_loss: 0.8992 - val_acc: 0.7249\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8797 - acc: 0.7305 - val_loss: 0.8921 - val_acc: 0.7282\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8812 - acc: 0.7295 - val_loss: 0.8967 - val_acc: 0.7238\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8798 - acc: 0.7304 - val_loss: 0.8901 - val_acc: 0.7291\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8821 - acc: 0.7296 - val_loss: 0.8958 - val_acc: 0.7253\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8812 - acc: 0.7286 - val_loss: 0.8913 - val_acc: 0.7284\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8796 - acc: 0.7296 - val_loss: 0.8986 - val_acc: 0.7248\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8789 - acc: 0.7305 - val_loss: 0.8969 - val_acc: 0.7255\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8808 - acc: 0.7296 - val_loss: 0.8978 - val_acc: 0.7260\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8788 - acc: 0.7307 - val_loss: 0.8898 - val_acc: 0.7299\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8781 - acc: 0.7309 - val_loss: 0.8893 - val_acc: 0.7282\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8786 - acc: 0.7301 - val_loss: 0.8924 - val_acc: 0.7273\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8804 - acc: 0.7296 - val_loss: 0.8923 - val_acc: 0.7278\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8776 - acc: 0.7310 - val_loss: 0.8898 - val_acc: 0.7301\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8787 - acc: 0.7312 - val_loss: 0.9003 - val_acc: 0.7231\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "sERp232wRvAN",
    "outputId": "4e9f4dd0-22bf-4f2a-fc71-d76816807e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.9003 - acc: 0.7231\n",
      "Validation accuracy: 72.31\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations'); print('--'*40)\n",
    "results2 = model2.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKt6irM6ownA"
   },
   "source": [
    "<a id='o4'></a>\n",
    "##### Observation 4 - NN model with relu activations\n",
    "* Improves the scores considerably.\n",
    "* Best accuracy achieved till now is using relu activations, SGD optimizer, changing learning rate to 0.001.\n",
    "* Next, let's try and change the number of activators and see if the score improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "17b7kG5oR7J_"
   },
   "source": [
    "#### NN model, relu activations, changing number of activators, SGD optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "58Iskt9pKB3L",
    "outputId": "13040982-5c85-4d03-daf3-8729d8c3687a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with relu activations and changing number of activators\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model3 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu\n",
    "model3.add(Dense(256, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model3.add(Dense(128))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - Adding second hidden layer\n",
    "model3.add(Dense(64))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model3.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model3.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "code",
    "id": "2q5A5rfawr08",
    "outputId": "a42e0e5e-d814-4ae9-b1a2-facd345a31e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 304,202\n",
      "Trainable params: 304,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WbApDf7hTG87",
    "outputId": "141ce0c1-6f66-4e22-f40a-6954dd4636f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 2.2970 - acc: 0.1352 - val_loss: 2.2856 - val_acc: 0.1681\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2796 - acc: 0.1758 - val_loss: 2.2716 - val_acc: 0.2027\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2635 - acc: 0.2046 - val_loss: 2.2535 - val_acc: 0.2214\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.2420 - acc: 0.2349 - val_loss: 2.2278 - val_acc: 0.2569\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2120 - acc: 0.2681 - val_loss: 2.1917 - val_acc: 0.2880\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.1718 - acc: 0.2916 - val_loss: 2.1462 - val_acc: 0.3122\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.1214 - acc: 0.3206 - val_loss: 2.0898 - val_acc: 0.3460\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 2.0581 - acc: 0.3461 - val_loss: 2.0236 - val_acc: 0.3535\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.9848 - acc: 0.3783 - val_loss: 1.9432 - val_acc: 0.3811\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.9031 - acc: 0.4055 - val_loss: 1.8582 - val_acc: 0.4124\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.8185 - acc: 0.4314 - val_loss: 1.7743 - val_acc: 0.4404\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.7381 - acc: 0.4524 - val_loss: 1.6980 - val_acc: 0.4704\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.6675 - acc: 0.4743 - val_loss: 1.6228 - val_acc: 0.4852\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.5969 - acc: 0.4995 - val_loss: 1.5545 - val_acc: 0.5227\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.5448 - acc: 0.5187 - val_loss: 1.5015 - val_acc: 0.5362\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.4915 - acc: 0.5380 - val_loss: 1.4665 - val_acc: 0.5372\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.4440 - acc: 0.5557 - val_loss: 1.4270 - val_acc: 0.5549\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.3993 - acc: 0.5724 - val_loss: 1.3648 - val_acc: 0.5817\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.3595 - acc: 0.5881 - val_loss: 1.3138 - val_acc: 0.6082\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.3237 - acc: 0.5979 - val_loss: 1.2766 - val_acc: 0.6204\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2891 - acc: 0.6102 - val_loss: 1.2549 - val_acc: 0.6227\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 1.2631 - acc: 0.6179 - val_loss: 1.2217 - val_acc: 0.6326\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2333 - acc: 0.6262 - val_loss: 1.1897 - val_acc: 0.6449\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.2085 - acc: 0.6334 - val_loss: 1.2089 - val_acc: 0.6307\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.1796 - acc: 0.6431 - val_loss: 1.1399 - val_acc: 0.6573\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1567 - acc: 0.6515 - val_loss: 1.1690 - val_acc: 0.6470\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1386 - acc: 0.6560 - val_loss: 1.1205 - val_acc: 0.6615\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.1176 - acc: 0.6613 - val_loss: 1.1342 - val_acc: 0.6515\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 1.0952 - acc: 0.6679 - val_loss: 1.0826 - val_acc: 0.6724\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0830 - acc: 0.6719 - val_loss: 1.0669 - val_acc: 0.6750\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0597 - acc: 0.6795 - val_loss: 1.0353 - val_acc: 0.6884\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0421 - acc: 0.6851 - val_loss: 1.0258 - val_acc: 0.6925\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0331 - acc: 0.6867 - val_loss: 1.0104 - val_acc: 0.6980\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0165 - acc: 0.6934 - val_loss: 1.0069 - val_acc: 0.6970\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 1.0008 - acc: 0.6960 - val_loss: 0.9970 - val_acc: 0.6992\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9945 - acc: 0.6982 - val_loss: 0.9940 - val_acc: 0.6965\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9793 - acc: 0.7029 - val_loss: 0.9624 - val_acc: 0.7110\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.9663 - acc: 0.7077 - val_loss: 1.0115 - val_acc: 0.6923\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9564 - acc: 0.7122 - val_loss: 0.9731 - val_acc: 0.7039\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.9448 - acc: 0.7154 - val_loss: 0.9362 - val_acc: 0.7169\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9365 - acc: 0.7180 - val_loss: 0.9292 - val_acc: 0.7188\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9225 - acc: 0.7215 - val_loss: 0.8999 - val_acc: 0.7302\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9145 - acc: 0.7238 - val_loss: 0.9025 - val_acc: 0.7299\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9029 - acc: 0.7281 - val_loss: 1.0152 - val_acc: 0.6853\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.9000 - acc: 0.7281 - val_loss: 0.8805 - val_acc: 0.7349\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8898 - acc: 0.7289 - val_loss: 0.9236 - val_acc: 0.7178\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8760 - acc: 0.7355 - val_loss: 0.8661 - val_acc: 0.7407\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8694 - acc: 0.7385 - val_loss: 0.8628 - val_acc: 0.7422\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8612 - acc: 0.7381 - val_loss: 0.8997 - val_acc: 0.7266\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8591 - acc: 0.7398 - val_loss: 0.8416 - val_acc: 0.7478\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8462 - acc: 0.7453 - val_loss: 0.9037 - val_acc: 0.7258\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8391 - acc: 0.7459 - val_loss: 0.8628 - val_acc: 0.7382\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8393 - acc: 0.7460 - val_loss: 0.8461 - val_acc: 0.7462\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8288 - acc: 0.7493 - val_loss: 0.8156 - val_acc: 0.7549\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8158 - acc: 0.7539 - val_loss: 0.8001 - val_acc: 0.7626\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.8154 - acc: 0.7545 - val_loss: 0.8082 - val_acc: 0.7612\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.8053 - acc: 0.7569 - val_loss: 0.8299 - val_acc: 0.7492\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7994 - acc: 0.7599 - val_loss: 0.8095 - val_acc: 0.7570\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7924 - acc: 0.7619 - val_loss: 0.8090 - val_acc: 0.7562\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7901 - acc: 0.7609 - val_loss: 0.7917 - val_acc: 0.7615\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7827 - acc: 0.7656 - val_loss: 0.7713 - val_acc: 0.7694\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7769 - acc: 0.7650 - val_loss: 0.7830 - val_acc: 0.7661\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7675 - acc: 0.7671 - val_loss: 0.7965 - val_acc: 0.7606\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7644 - acc: 0.7679 - val_loss: 0.7599 - val_acc: 0.7731\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7584 - acc: 0.7713 - val_loss: 0.7570 - val_acc: 0.7728\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7528 - acc: 0.7725 - val_loss: 0.7684 - val_acc: 0.7698\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7447 - acc: 0.7757 - val_loss: 0.7679 - val_acc: 0.7717\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7423 - acc: 0.7766 - val_loss: 0.7558 - val_acc: 0.7750\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7405 - acc: 0.7773 - val_loss: 0.7588 - val_acc: 0.7726\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7341 - acc: 0.7778 - val_loss: 0.7447 - val_acc: 0.7783\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7243 - acc: 0.7812 - val_loss: 0.7490 - val_acc: 0.7745\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7239 - acc: 0.7818 - val_loss: 0.7168 - val_acc: 0.7861\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7154 - acc: 0.7850 - val_loss: 0.7268 - val_acc: 0.7806\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7103 - acc: 0.7860 - val_loss: 0.7208 - val_acc: 0.7813\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7072 - acc: 0.7857 - val_loss: 0.7144 - val_acc: 0.7858\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7010 - acc: 0.7887 - val_loss: 0.7003 - val_acc: 0.7927\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.7012 - acc: 0.7879 - val_loss: 0.7082 - val_acc: 0.7896\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6929 - acc: 0.7915 - val_loss: 0.6906 - val_acc: 0.7964\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6939 - acc: 0.7909 - val_loss: 0.6842 - val_acc: 0.7974\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6882 - acc: 0.7921 - val_loss: 0.7020 - val_acc: 0.7897\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6787 - acc: 0.7941 - val_loss: 0.7151 - val_acc: 0.7848\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6810 - acc: 0.7929 - val_loss: 0.6744 - val_acc: 0.7991\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6717 - acc: 0.7963 - val_loss: 0.6813 - val_acc: 0.7977\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6671 - acc: 0.7987 - val_loss: 0.7453 - val_acc: 0.7745\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6686 - acc: 0.8000 - val_loss: 0.6739 - val_acc: 0.7992\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6614 - acc: 0.8003 - val_loss: 0.6793 - val_acc: 0.7975\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6552 - acc: 0.8014 - val_loss: 0.6969 - val_acc: 0.7904\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6524 - acc: 0.8022 - val_loss: 0.6672 - val_acc: 0.8000\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6473 - acc: 0.8039 - val_loss: 0.6661 - val_acc: 0.7994\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6463 - acc: 0.8042 - val_loss: 0.6901 - val_acc: 0.7949\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6423 - acc: 0.8070 - val_loss: 0.6594 - val_acc: 0.8036\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6373 - acc: 0.8077 - val_loss: 0.7119 - val_acc: 0.7857\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6325 - acc: 0.8087 - val_loss: 0.7002 - val_acc: 0.7896\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6286 - acc: 0.8102 - val_loss: 0.6647 - val_acc: 0.8014\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6266 - acc: 0.8102 - val_loss: 0.6754 - val_acc: 0.7953\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6194 - acc: 0.8131 - val_loss: 0.6490 - val_acc: 0.8067\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6192 - acc: 0.8125 - val_loss: 0.6230 - val_acc: 0.8147\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6151 - acc: 0.8145 - val_loss: 0.6406 - val_acc: 0.8116\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6118 - acc: 0.8143 - val_loss: 0.6255 - val_acc: 0.8143\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 0.6101 - acc: 0.8161 - val_loss: 0.6550 - val_acc: 0.8033\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model3.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "4nU-fCy5KbwM",
    "outputId": "efbd3096-803c-420b-a9cb-2403397384f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations and changing the number of activators\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.6550 - acc: 0.8033\n",
      "Validation accuracy: 80.33\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
    "results3 = model3.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCjOofsRTN1K"
   },
   "source": [
    "#### NN model, relu activations, changing number of activators, Adam optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DhnK4y6VTQAC",
    "outputId": "248d2cc8-22c1-482b-f193-44a2c5025f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.9431 - acc: 0.7111 - val_loss: 0.7650 - val_acc: 0.7681\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7697 - acc: 0.7615 - val_loss: 0.8645 - val_acc: 0.7370\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7513 - acc: 0.7701 - val_loss: 0.7942 - val_acc: 0.7514\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7354 - acc: 0.7748 - val_loss: 0.7716 - val_acc: 0.7655\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7217 - acc: 0.7776 - val_loss: 0.7024 - val_acc: 0.7878\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6969 - acc: 0.7889 - val_loss: 0.7223 - val_acc: 0.7825\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6787 - acc: 0.7916 - val_loss: 0.6787 - val_acc: 0.7961\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.6733 - acc: 0.7931 - val_loss: 0.6695 - val_acc: 0.7995\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.6702 - acc: 0.7970 - val_loss: 0.6557 - val_acc: 0.7993\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6459 - acc: 0.8030 - val_loss: 0.6377 - val_acc: 0.8059\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.6307 - acc: 0.8063 - val_loss: 0.6681 - val_acc: 0.7943\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6220 - acc: 0.8095 - val_loss: 0.6519 - val_acc: 0.7985\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6159 - acc: 0.8104 - val_loss: 0.5682 - val_acc: 0.8313\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5888 - acc: 0.8195 - val_loss: 0.6379 - val_acc: 0.8075\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5854 - acc: 0.8195 - val_loss: 0.6212 - val_acc: 0.8140\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5868 - acc: 0.8183 - val_loss: 0.6059 - val_acc: 0.8149\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5652 - acc: 0.8265 - val_loss: 0.6075 - val_acc: 0.8173\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5573 - acc: 0.8286 - val_loss: 0.5845 - val_acc: 0.8208\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5520 - acc: 0.8299 - val_loss: 0.6187 - val_acc: 0.8110\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5610 - acc: 0.8274 - val_loss: 0.5504 - val_acc: 0.8365\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5195 - acc: 0.8405 - val_loss: 0.5797 - val_acc: 0.8260\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5315 - acc: 0.8363 - val_loss: 0.5837 - val_acc: 0.8220\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5184 - acc: 0.8403 - val_loss: 0.5482 - val_acc: 0.8365\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5084 - acc: 0.8423 - val_loss: 0.5310 - val_acc: 0.8398\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4920 - acc: 0.8479 - val_loss: 0.5043 - val_acc: 0.8500\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.4861 - acc: 0.8503 - val_loss: 0.5457 - val_acc: 0.8358\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4943 - acc: 0.8464 - val_loss: 0.5342 - val_acc: 0.8367\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4859 - acc: 0.8496 - val_loss: 0.4831 - val_acc: 0.8547\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4807 - acc: 0.8505 - val_loss: 0.5247 - val_acc: 0.8406\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4695 - acc: 0.8536 - val_loss: 0.5498 - val_acc: 0.8342\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4653 - acc: 0.8545 - val_loss: 0.5010 - val_acc: 0.8485\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4577 - acc: 0.8581 - val_loss: 0.4785 - val_acc: 0.8563\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4566 - acc: 0.8579 - val_loss: 0.5020 - val_acc: 0.8483\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4525 - acc: 0.8579 - val_loss: 0.5862 - val_acc: 0.8170\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4355 - acc: 0.8633 - val_loss: 0.4661 - val_acc: 0.8595\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4291 - acc: 0.8661 - val_loss: 0.5259 - val_acc: 0.8383\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4338 - acc: 0.8625 - val_loss: 0.6068 - val_acc: 0.8148\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4325 - acc: 0.8645 - val_loss: 0.4514 - val_acc: 0.8656\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4294 - acc: 0.8657 - val_loss: 0.4641 - val_acc: 0.8617\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4239 - acc: 0.8666 - val_loss: 0.5175 - val_acc: 0.8439\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4266 - acc: 0.8659 - val_loss: 0.5023 - val_acc: 0.8489\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4206 - acc: 0.8668 - val_loss: 0.4673 - val_acc: 0.8594\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.4083 - acc: 0.8699 - val_loss: 0.4995 - val_acc: 0.8479\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3917 - acc: 0.8773 - val_loss: 0.4362 - val_acc: 0.8698\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3893 - acc: 0.8769 - val_loss: 0.5263 - val_acc: 0.8397\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3958 - acc: 0.8752 - val_loss: 0.4380 - val_acc: 0.8702\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3959 - acc: 0.8743 - val_loss: 0.4739 - val_acc: 0.8581\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3956 - acc: 0.8740 - val_loss: 0.4374 - val_acc: 0.8686\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3933 - acc: 0.8749 - val_loss: 0.4651 - val_acc: 0.8607\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3671 - acc: 0.8832 - val_loss: 0.4376 - val_acc: 0.8698\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3662 - acc: 0.8822 - val_loss: 0.4165 - val_acc: 0.8781\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3612 - acc: 0.8860 - val_loss: 0.4172 - val_acc: 0.8770\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3767 - acc: 0.8793 - val_loss: 0.4456 - val_acc: 0.8668\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3575 - acc: 0.8863 - val_loss: 0.4409 - val_acc: 0.8695\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3584 - acc: 0.8840 - val_loss: 0.4215 - val_acc: 0.8756\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3558 - acc: 0.8864 - val_loss: 0.4387 - val_acc: 0.8690\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3602 - acc: 0.8850 - val_loss: 0.4316 - val_acc: 0.8732\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3350 - acc: 0.8931 - val_loss: 0.4214 - val_acc: 0.8757\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3381 - acc: 0.8934 - val_loss: 0.4294 - val_acc: 0.8722\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3374 - acc: 0.8938 - val_loss: 0.4462 - val_acc: 0.8694\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3499 - acc: 0.8881 - val_loss: 0.4223 - val_acc: 0.8754\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3301 - acc: 0.8923 - val_loss: 0.4261 - val_acc: 0.8756\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3302 - acc: 0.8942 - val_loss: 0.4492 - val_acc: 0.8649\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3206 - acc: 0.8978 - val_loss: 0.3936 - val_acc: 0.8864\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3162 - acc: 0.8980 - val_loss: 0.3932 - val_acc: 0.8864\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3092 - acc: 0.9009 - val_loss: 0.4013 - val_acc: 0.8825\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3158 - acc: 0.8982 - val_loss: 0.4132 - val_acc: 0.8783\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3189 - acc: 0.8970 - val_loss: 0.3641 - val_acc: 0.8957\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3035 - acc: 0.9028 - val_loss: 0.4218 - val_acc: 0.8760\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3131 - acc: 0.8994 - val_loss: 0.4032 - val_acc: 0.8841\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3009 - acc: 0.9026 - val_loss: 0.4012 - val_acc: 0.8855\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2948 - acc: 0.9077 - val_loss: 0.3779 - val_acc: 0.8928\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2957 - acc: 0.9047 - val_loss: 0.4244 - val_acc: 0.8788\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.2927 - acc: 0.9074 - val_loss: 0.4000 - val_acc: 0.8860\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2952 - acc: 0.9060 - val_loss: 0.4045 - val_acc: 0.8850\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.2872 - acc: 0.9078 - val_loss: 0.4378 - val_acc: 0.8738\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.3008 - acc: 0.9033 - val_loss: 0.3690 - val_acc: 0.8970\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2724 - acc: 0.9125 - val_loss: 0.3860 - val_acc: 0.8903\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.2974 - acc: 0.9023 - val_loss: 0.4051 - val_acc: 0.8840\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2782 - acc: 0.9085 - val_loss: 0.3634 - val_acc: 0.9003\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2770 - acc: 0.9094 - val_loss: 0.3970 - val_acc: 0.8867\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2813 - acc: 0.9098 - val_loss: 0.3967 - val_acc: 0.8843\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2933 - acc: 0.9037 - val_loss: 0.4170 - val_acc: 0.8803\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2803 - acc: 0.9088 - val_loss: 0.3711 - val_acc: 0.8977\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2688 - acc: 0.9134 - val_loss: 0.4142 - val_acc: 0.8851\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2594 - acc: 0.9154 - val_loss: 0.3891 - val_acc: 0.8889\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2721 - acc: 0.9106 - val_loss: 0.4202 - val_acc: 0.8810\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2624 - acc: 0.9143 - val_loss: 0.4145 - val_acc: 0.8846\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.2672 - acc: 0.9133 - val_loss: 0.4157 - val_acc: 0.8844\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2453 - acc: 0.9196 - val_loss: 0.3752 - val_acc: 0.8970\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2564 - acc: 0.9160 - val_loss: 0.3450 - val_acc: 0.9084\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2401 - acc: 0.9220 - val_loss: 0.4096 - val_acc: 0.8861\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2460 - acc: 0.9199 - val_loss: 0.3834 - val_acc: 0.8932\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2424 - acc: 0.9212 - val_loss: 0.3874 - val_acc: 0.8963\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.2699 - acc: 0.9125 - val_loss: 0.3681 - val_acc: 0.9016\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2392 - acc: 0.9211 - val_loss: 0.4090 - val_acc: 0.8891\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2432 - acc: 0.9213 - val_loss: 0.3536 - val_acc: 0.9046\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.2430 - acc: 0.9205 - val_loss: 0.3895 - val_acc: 0.8961\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2386 - acc: 0.9211 - val_loss: 0.3641 - val_acc: 0.9031\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2458 - acc: 0.9195 - val_loss: 0.3539 - val_acc: 0.9066\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "sHt5CH17TYgP",
    "outputId": "a1e0b820-8142-414c-c95f-4db5e98130f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate NN model with relu activations and changing the number of activators\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.3539 - acc: 0.9066\n",
      "Validation accuracy: 90.66\n"
     ]
    }
   ],
   "source": [
    "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
    "results3 = model3.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMH4JH0Aw1Xz"
   },
   "source": [
    "<a id='o5'></a>\n",
    "##### Observation 5 - NN model with relu activations and changing activators\n",
    "* Adding relu activations and changing activators results in improvement of score.\n",
    "* Best accuracy achieved till now is using relu activations, changing number of activators and Adam optimizers with a learning rate of 0.001\n",
    "* Next, let's try adding weight initilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kermiDSyKAPQ"
   },
   "source": [
    "<a id='Weight'></a>\n",
    "### With Weight Initializers\n",
    "Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdMa80vmVAfU"
   },
   "source": [
    "#### NN model, relu activations, SGD optimizers with weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jLn1T1HTE5ms",
    "outputId": "ffdb5aa0-c9ef-4033-8453-4f4e3ab775fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with weight initializers\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with weight initializers'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model4 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
    "model4.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model4.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - adding second hidden layer\n",
    "model4.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 3 - adding third hidden layer\n",
    "model4.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model4.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model4.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "colab_type": "code",
    "id": "j4Ll9EKlU0Ep",
    "outputId": "918e5dc5-c837-4e54-9ad9-22ee8ec24f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 305,962\n",
      "Trainable params: 305,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "W8Np2U7LU2vN",
    "outputId": "e6bcfe8a-09e0-4d1f-e43b-3543a24cc155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 2.3023 - acc: 0.1175 - val_loss: 2.2772 - val_acc: 0.1445\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2583 - acc: 0.1590 - val_loss: 2.2352 - val_acc: 0.1811\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.2025 - acc: 0.1988 - val_loss: 2.1630 - val_acc: 0.2169\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.1205 - acc: 0.2455 - val_loss: 2.0677 - val_acc: 0.2791\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 2.0151 - acc: 0.3002 - val_loss: 1.9792 - val_acc: 0.2936\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.9102 - acc: 0.3502 - val_loss: 1.8788 - val_acc: 0.3721\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 1.8119 - acc: 0.3987 - val_loss: 1.7286 - val_acc: 0.4290\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.7188 - acc: 0.4394 - val_loss: 1.6887 - val_acc: 0.4459\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.6454 - acc: 0.4664 - val_loss: 1.5630 - val_acc: 0.4977\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 1.5575 - acc: 0.5016 - val_loss: 1.4854 - val_acc: 0.5321\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.4950 - acc: 0.5230 - val_loss: 1.4138 - val_acc: 0.5634\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.4227 - acc: 0.5528 - val_loss: 1.3826 - val_acc: 0.5729\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.3695 - acc: 0.5711 - val_loss: 1.3418 - val_acc: 0.5759\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.3157 - acc: 0.5902 - val_loss: 1.2598 - val_acc: 0.6136\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.2658 - acc: 0.6087 - val_loss: 1.2570 - val_acc: 0.6051\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.2305 - acc: 0.6206 - val_loss: 1.1832 - val_acc: 0.6360\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.1781 - acc: 0.6407 - val_loss: 1.1872 - val_acc: 0.6325\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.1564 - acc: 0.6430 - val_loss: 1.1064 - val_acc: 0.6654\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.1314 - acc: 0.6545 - val_loss: 1.1058 - val_acc: 0.6610\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.1027 - acc: 0.6636 - val_loss: 1.0935 - val_acc: 0.6723\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.0788 - acc: 0.6717 - val_loss: 1.0264 - val_acc: 0.6935\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.0554 - acc: 0.6769 - val_loss: 1.0594 - val_acc: 0.6763\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.0394 - acc: 0.6810 - val_loss: 1.0024 - val_acc: 0.6969\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0207 - acc: 0.6883 - val_loss: 1.0085 - val_acc: 0.6931\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 1.0036 - acc: 0.6933 - val_loss: 0.9629 - val_acc: 0.7112\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9817 - acc: 0.7005 - val_loss: 0.9621 - val_acc: 0.7084\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9767 - acc: 0.7021 - val_loss: 0.9664 - val_acc: 0.7089\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9630 - acc: 0.7063 - val_loss: 0.9301 - val_acc: 0.7188\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9437 - acc: 0.7120 - val_loss: 0.9326 - val_acc: 0.7169\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9364 - acc: 0.7131 - val_loss: 0.9065 - val_acc: 0.7268\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.9258 - acc: 0.7177 - val_loss: 0.9086 - val_acc: 0.7228\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.9093 - acc: 0.7233 - val_loss: 0.9859 - val_acc: 0.6932\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8930 - acc: 0.7258 - val_loss: 0.8975 - val_acc: 0.7272\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8898 - acc: 0.7287 - val_loss: 0.9300 - val_acc: 0.7106\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8749 - acc: 0.7321 - val_loss: 0.8580 - val_acc: 0.7386\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8671 - acc: 0.7364 - val_loss: 0.8540 - val_acc: 0.7415\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.8604 - acc: 0.7385 - val_loss: 0.8776 - val_acc: 0.7315\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8520 - acc: 0.7388 - val_loss: 0.8911 - val_acc: 0.7285\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8411 - acc: 0.7417 - val_loss: 0.8206 - val_acc: 0.7509\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8298 - acc: 0.7486 - val_loss: 0.8165 - val_acc: 0.7536\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8214 - acc: 0.7479 - val_loss: 0.8244 - val_acc: 0.7494\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8155 - acc: 0.7504 - val_loss: 0.8274 - val_acc: 0.7457\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.8055 - acc: 0.7525 - val_loss: 0.7904 - val_acc: 0.7614\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7994 - acc: 0.7544 - val_loss: 0.8214 - val_acc: 0.7481\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7903 - acc: 0.7585 - val_loss: 0.7907 - val_acc: 0.7623\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7772 - acc: 0.7641 - val_loss: 0.7760 - val_acc: 0.7646\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7708 - acc: 0.7637 - val_loss: 0.7748 - val_acc: 0.7641\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7647 - acc: 0.7668 - val_loss: 0.7755 - val_acc: 0.7647\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7619 - acc: 0.7675 - val_loss: 0.8349 - val_acc: 0.7467\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7527 - acc: 0.7716 - val_loss: 0.8013 - val_acc: 0.7557\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7460 - acc: 0.7721 - val_loss: 0.8218 - val_acc: 0.7488\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7374 - acc: 0.7745 - val_loss: 0.7460 - val_acc: 0.7739\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.7298 - acc: 0.7764 - val_loss: 0.7361 - val_acc: 0.7758\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7241 - acc: 0.7789 - val_loss: 0.7359 - val_acc: 0.7771\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7231 - acc: 0.7796 - val_loss: 0.7706 - val_acc: 0.7632\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.7154 - acc: 0.7819 - val_loss: 0.6990 - val_acc: 0.7884\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.7089 - acc: 0.7844 - val_loss: 0.7225 - val_acc: 0.7826\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6965 - acc: 0.7884 - val_loss: 0.7235 - val_acc: 0.7802\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6945 - acc: 0.7878 - val_loss: 0.6968 - val_acc: 0.7902\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.6934 - acc: 0.7889 - val_loss: 0.6906 - val_acc: 0.7930\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6812 - acc: 0.7935 - val_loss: 0.7031 - val_acc: 0.7886\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6734 - acc: 0.7948 - val_loss: 0.6841 - val_acc: 0.7936\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6704 - acc: 0.7957 - val_loss: 0.7160 - val_acc: 0.7796\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6639 - acc: 0.7963 - val_loss: 0.7002 - val_acc: 0.7873\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6635 - acc: 0.7970 - val_loss: 0.6885 - val_acc: 0.7896\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6577 - acc: 0.7989 - val_loss: 0.6442 - val_acc: 0.8066\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6460 - acc: 0.8025 - val_loss: 0.6553 - val_acc: 0.8033\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6456 - acc: 0.8022 - val_loss: 0.6597 - val_acc: 0.8029\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6428 - acc: 0.8029 - val_loss: 0.6701 - val_acc: 0.7975\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6438 - acc: 0.8018 - val_loss: 0.6699 - val_acc: 0.7985\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6290 - acc: 0.8080 - val_loss: 0.6421 - val_acc: 0.8052\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.6306 - acc: 0.8080 - val_loss: 0.7013 - val_acc: 0.7821\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6264 - acc: 0.8086 - val_loss: 0.6395 - val_acc: 0.8061\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.6162 - acc: 0.8125 - val_loss: 0.6528 - val_acc: 0.8025\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6137 - acc: 0.8125 - val_loss: 0.6327 - val_acc: 0.8090\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6107 - acc: 0.8144 - val_loss: 0.6555 - val_acc: 0.8019\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6071 - acc: 0.8128 - val_loss: 0.6139 - val_acc: 0.8156\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6012 - acc: 0.8164 - val_loss: 0.6277 - val_acc: 0.8092\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.6028 - acc: 0.8175 - val_loss: 0.6266 - val_acc: 0.8112\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5946 - acc: 0.8199 - val_loss: 0.6035 - val_acc: 0.8183\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5892 - acc: 0.8203 - val_loss: 0.6225 - val_acc: 0.8119\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5836 - acc: 0.8201 - val_loss: 0.6568 - val_acc: 0.8032\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5772 - acc: 0.8246 - val_loss: 0.6034 - val_acc: 0.8190\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5778 - acc: 0.8245 - val_loss: 0.6260 - val_acc: 0.8090\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5757 - acc: 0.8242 - val_loss: 0.5882 - val_acc: 0.8230\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5666 - acc: 0.8275 - val_loss: 0.6215 - val_acc: 0.8104\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5657 - acc: 0.8270 - val_loss: 0.5943 - val_acc: 0.8211\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5611 - acc: 0.8284 - val_loss: 0.5841 - val_acc: 0.8231\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5546 - acc: 0.8317 - val_loss: 0.5673 - val_acc: 0.8303\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5512 - acc: 0.8315 - val_loss: 0.5769 - val_acc: 0.8254\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5524 - acc: 0.8309 - val_loss: 0.5750 - val_acc: 0.8269\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5496 - acc: 0.8310 - val_loss: 0.5701 - val_acc: 0.8285\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5473 - acc: 0.8340 - val_loss: 0.5738 - val_acc: 0.8263\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5401 - acc: 0.8360 - val_loss: 0.5505 - val_acc: 0.8365\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5413 - acc: 0.8350 - val_loss: 0.6173 - val_acc: 0.8122\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 27us/sample - loss: 0.5368 - acc: 0.8359 - val_loss: 0.5707 - val_acc: 0.8304\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5295 - acc: 0.8385 - val_loss: 0.5667 - val_acc: 0.8296\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5249 - acc: 0.8381 - val_loss: 0.5843 - val_acc: 0.8244\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5273 - acc: 0.8393 - val_loss: 0.5687 - val_acc: 0.8294\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.5213 - acc: 0.8399 - val_loss: 0.5648 - val_acc: 0.8295\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model4.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "xPbRGkQ7NKyW",
    "outputId": "c3f60776-2e5c-4e2f-ea1c-ad478024d792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with weight initializers\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.5648 - acc: 0.8295\n",
      "Validation accuracy: 82.95\n"
     ]
    }
   ],
   "source": [
    "print('NN with weight initializers'); print('--'*40)\n",
    "results4 = model4.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSWZvLlSVgq5"
   },
   "source": [
    "#### NN model, relu activations, Adam optimizers with weight initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ysyBxxvgVjWk",
    "outputId": "450c0d00-c085-45d7-ed52-96360d18e32a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8991 - acc: 0.7259 - val_loss: 0.7777 - val_acc: 0.7567\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.7556 - acc: 0.7651 - val_loss: 0.7145 - val_acc: 0.7837\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.7233 - acc: 0.7769 - val_loss: 0.7429 - val_acc: 0.7717\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.7134 - acc: 0.7808 - val_loss: 0.7203 - val_acc: 0.7836\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.7006 - acc: 0.7834 - val_loss: 0.7579 - val_acc: 0.7643\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6854 - acc: 0.7876 - val_loss: 0.6947 - val_acc: 0.7862\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.6375 - acc: 0.8048 - val_loss: 0.6920 - val_acc: 0.7847\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6482 - acc: 0.7993 - val_loss: 0.7172 - val_acc: 0.7798\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6429 - acc: 0.8015 - val_loss: 0.6664 - val_acc: 0.7970\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6220 - acc: 0.8086 - val_loss: 0.5867 - val_acc: 0.8212\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.6078 - acc: 0.8123 - val_loss: 0.6032 - val_acc: 0.8152\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5877 - acc: 0.8190 - val_loss: 0.5871 - val_acc: 0.8198\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6041 - acc: 0.8137 - val_loss: 0.6405 - val_acc: 0.8071\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5615 - acc: 0.8272 - val_loss: 0.6205 - val_acc: 0.8112\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5613 - acc: 0.8273 - val_loss: 0.5388 - val_acc: 0.8364\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5540 - acc: 0.8296 - val_loss: 0.5700 - val_acc: 0.8283\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5484 - acc: 0.8299 - val_loss: 0.6048 - val_acc: 0.8130\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.5484 - acc: 0.8280 - val_loss: 0.5539 - val_acc: 0.8290\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 33us/sample - loss: 0.5517 - acc: 0.8289 - val_loss: 0.5933 - val_acc: 0.8200\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5333 - acc: 0.8333 - val_loss: 0.5172 - val_acc: 0.8419\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5145 - acc: 0.8397 - val_loss: 0.5776 - val_acc: 0.8227\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5038 - acc: 0.8428 - val_loss: 0.5885 - val_acc: 0.8203\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.5021 - acc: 0.8435 - val_loss: 0.5563 - val_acc: 0.8313\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4995 - acc: 0.8436 - val_loss: 0.5187 - val_acc: 0.8443\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4902 - acc: 0.8461 - val_loss: 0.5196 - val_acc: 0.8422\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4710 - acc: 0.8533 - val_loss: 0.4911 - val_acc: 0.8530\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4936 - acc: 0.8451 - val_loss: 0.5424 - val_acc: 0.8342\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4797 - acc: 0.8485 - val_loss: 0.5177 - val_acc: 0.8429\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4690 - acc: 0.8533 - val_loss: 0.4953 - val_acc: 0.8504\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4609 - acc: 0.8557 - val_loss: 0.5042 - val_acc: 0.8469\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4545 - acc: 0.8564 - val_loss: 0.4685 - val_acc: 0.8587\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4528 - acc: 0.8584 - val_loss: 0.5315 - val_acc: 0.8411\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.4509 - acc: 0.8584 - val_loss: 0.5534 - val_acc: 0.8298\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4374 - acc: 0.8635 - val_loss: 0.5036 - val_acc: 0.8473\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4344 - acc: 0.8632 - val_loss: 0.4816 - val_acc: 0.8529\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4361 - acc: 0.8620 - val_loss: 0.6043 - val_acc: 0.8112\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4331 - acc: 0.8619 - val_loss: 0.5036 - val_acc: 0.8474\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4255 - acc: 0.8643 - val_loss: 0.4520 - val_acc: 0.8638\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4218 - acc: 0.8663 - val_loss: 0.4762 - val_acc: 0.8538\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4104 - acc: 0.8693 - val_loss: 0.4917 - val_acc: 0.8511\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4133 - acc: 0.8700 - val_loss: 0.4658 - val_acc: 0.8579\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4138 - acc: 0.8682 - val_loss: 0.4311 - val_acc: 0.8703\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4022 - acc: 0.8722 - val_loss: 0.5085 - val_acc: 0.8441\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3980 - acc: 0.8724 - val_loss: 0.4468 - val_acc: 0.8643\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3898 - acc: 0.8754 - val_loss: 0.4190 - val_acc: 0.8749\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3825 - acc: 0.8784 - val_loss: 0.4479 - val_acc: 0.8654\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3758 - acc: 0.8804 - val_loss: 0.4472 - val_acc: 0.8663\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3744 - acc: 0.8803 - val_loss: 0.4578 - val_acc: 0.8621\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3732 - acc: 0.8777 - val_loss: 0.4057 - val_acc: 0.8789\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3702 - acc: 0.8817 - val_loss: 0.4784 - val_acc: 0.8530\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3802 - acc: 0.8784 - val_loss: 0.4168 - val_acc: 0.8766\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.3640 - acc: 0.8855 - val_loss: 0.4354 - val_acc: 0.8693\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3655 - acc: 0.8817 - val_loss: 0.4408 - val_acc: 0.8706\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3629 - acc: 0.8839 - val_loss: 0.4955 - val_acc: 0.8507\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 33us/sample - loss: 0.3575 - acc: 0.8848 - val_loss: 0.4405 - val_acc: 0.8676\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3460 - acc: 0.8889 - val_loss: 0.4270 - val_acc: 0.8721\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3463 - acc: 0.8882 - val_loss: 0.4216 - val_acc: 0.8741\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3444 - acc: 0.8886 - val_loss: 0.4267 - val_acc: 0.8738\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3352 - acc: 0.8926 - val_loss: 0.4318 - val_acc: 0.8720\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3576 - acc: 0.8835 - val_loss: 0.4341 - val_acc: 0.8686\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3416 - acc: 0.8905 - val_loss: 0.4321 - val_acc: 0.8719\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3321 - acc: 0.8934 - val_loss: 0.4097 - val_acc: 0.8776\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3265 - acc: 0.8952 - val_loss: 0.3944 - val_acc: 0.8839\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3281 - acc: 0.8930 - val_loss: 0.4291 - val_acc: 0.8740\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3175 - acc: 0.8973 - val_loss: 0.4325 - val_acc: 0.8718\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.3356 - acc: 0.8910 - val_loss: 0.3925 - val_acc: 0.8855\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3264 - acc: 0.8937 - val_loss: 0.4110 - val_acc: 0.8832\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3193 - acc: 0.8961 - val_loss: 0.4201 - val_acc: 0.8763\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3213 - acc: 0.8955 - val_loss: 0.4312 - val_acc: 0.8732\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3135 - acc: 0.8977 - val_loss: 0.4648 - val_acc: 0.8613\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3129 - acc: 0.8984 - val_loss: 0.3997 - val_acc: 0.8838\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3039 - acc: 0.9007 - val_loss: 0.4164 - val_acc: 0.8781\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3088 - acc: 0.9005 - val_loss: 0.4063 - val_acc: 0.8815\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3041 - acc: 0.9022 - val_loss: 0.3704 - val_acc: 0.8949\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2971 - acc: 0.9039 - val_loss: 0.4283 - val_acc: 0.8779\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3031 - acc: 0.9018 - val_loss: 0.4305 - val_acc: 0.8752\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2963 - acc: 0.9041 - val_loss: 0.4077 - val_acc: 0.8816\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.2873 - acc: 0.9086 - val_loss: 0.4098 - val_acc: 0.8811\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2926 - acc: 0.9044 - val_loss: 0.3991 - val_acc: 0.8887\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.2840 - acc: 0.9085 - val_loss: 0.3795 - val_acc: 0.8923\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.2877 - acc: 0.9056 - val_loss: 0.4328 - val_acc: 0.8731\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2810 - acc: 0.9073 - val_loss: 0.3815 - val_acc: 0.8924\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.2881 - acc: 0.9066 - val_loss: 0.4516 - val_acc: 0.8707\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2902 - acc: 0.9050 - val_loss: 0.3960 - val_acc: 0.8873\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2647 - acc: 0.9148 - val_loss: 0.3972 - val_acc: 0.8877\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2760 - acc: 0.9100 - val_loss: 0.3952 - val_acc: 0.8874\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2739 - acc: 0.9095 - val_loss: 0.4250 - val_acc: 0.8813\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2745 - acc: 0.9099 - val_loss: 0.4271 - val_acc: 0.8791\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2576 - acc: 0.9163 - val_loss: 0.4173 - val_acc: 0.8834\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2657 - acc: 0.9130 - val_loss: 0.3729 - val_acc: 0.8973\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.2493 - acc: 0.9177 - val_loss: 0.3755 - val_acc: 0.8954\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2611 - acc: 0.9147 - val_loss: 0.3596 - val_acc: 0.9012\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2543 - acc: 0.9182 - val_loss: 0.3893 - val_acc: 0.8908\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2478 - acc: 0.9176 - val_loss: 0.4036 - val_acc: 0.8882\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2613 - acc: 0.9129 - val_loss: 0.4178 - val_acc: 0.8851\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2535 - acc: 0.9170 - val_loss: 0.3966 - val_acc: 0.8928\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2520 - acc: 0.9163 - val_loss: 0.3765 - val_acc: 0.8971\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2548 - acc: 0.9166 - val_loss: 0.4093 - val_acc: 0.8857\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.2535 - acc: 0.9170 - val_loss: 0.4349 - val_acc: 0.8797\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.2450 - acc: 0.9200 - val_loss: 0.4037 - val_acc: 0.8873\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model4.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "95vBMvCvVrg0",
    "outputId": "328825e1-3e71-4e25-8e71-3e58733cef83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with weight initializers\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4037 - acc: 0.8873\n",
      "Validation accuracy: 88.73\n"
     ]
    }
   ],
   "source": [
    "print('NN with weight initializers'); print('--'*40)\n",
    "results4 = model4.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjy8FCIQz_s4"
   },
   "source": [
    "<a id='o6'></a>\n",
    "##### Observation 6 - Weight initializers\n",
    "* Adding weight initialiers didn't result in improvement of score.\n",
    "* relu activations, changing number of activators, Adam optimizers gives the best score out of the ones tried as of now.\n",
    "* Next, let's try batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlLYOhj_TNcf"
   },
   "source": [
    "<a id='Batch'></a>\n",
    "### Batch Normalization\n",
    "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective. Normalize each mini-batch before nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8QVHk2dWbgK"
   },
   "source": [
    "#### NN model, relu activations, SGD optimizers with weight initializers and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "BCATllTeUghF",
    "outputId": "61ca8e1f-aac8-4a3d-f81c-5a9656f7de71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with batch normalization\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with batch normalization'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model5 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
    "model5.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model5.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - adding second hidden layer\n",
    "model5.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 3 - adding third hidden layer\n",
    "model5.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model5.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model5.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model5.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model5.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "colab_type": "code",
    "id": "3Yz93UDo557M",
    "outputId": "b1f09216-291e-4c56-b0b2-466f3ab28aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 307,882\n",
      "Trainable params: 306,922\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lvXUI89PWUcr",
    "outputId": "767307aa-ba57-45b3-c51b-91583fa5592c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 2.3150 - acc: 0.1844 - val_loss: 2.2163 - val_acc: 0.1867\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 1.8823 - acc: 0.3717 - val_loss: 1.8685 - val_acc: 0.3833\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 1.6338 - acc: 0.4850 - val_loss: 1.6083 - val_acc: 0.4873\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 1.4471 - acc: 0.5612 - val_loss: 1.4532 - val_acc: 0.5435\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 1.3017 - acc: 0.6110 - val_loss: 1.2819 - val_acc: 0.6060\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 1.1888 - acc: 0.6459 - val_loss: 1.2110 - val_acc: 0.6357\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 1.0931 - acc: 0.6720 - val_loss: 1.1470 - val_acc: 0.6414\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 1.0188 - acc: 0.6929 - val_loss: 1.0641 - val_acc: 0.6710\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.9576 - acc: 0.7099 - val_loss: 1.0080 - val_acc: 0.6921\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.9052 - acc: 0.7238 - val_loss: 0.9584 - val_acc: 0.7056\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.8609 - acc: 0.7344 - val_loss: 0.9839 - val_acc: 0.6924\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.8205 - acc: 0.7494 - val_loss: 0.9355 - val_acc: 0.7078\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.7844 - acc: 0.7591 - val_loss: 0.8979 - val_acc: 0.7123\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.7555 - acc: 0.7667 - val_loss: 0.9690 - val_acc: 0.6953\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.7297 - acc: 0.7736 - val_loss: 0.8284 - val_acc: 0.7423\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.7034 - acc: 0.7813 - val_loss: 0.9007 - val_acc: 0.7136\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.6805 - acc: 0.7895 - val_loss: 0.9803 - val_acc: 0.6925\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.6578 - acc: 0.7969 - val_loss: 0.7991 - val_acc: 0.7457\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.6425 - acc: 0.8005 - val_loss: 0.8517 - val_acc: 0.7338\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.6255 - acc: 0.8049 - val_loss: 0.8117 - val_acc: 0.7400\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.6085 - acc: 0.8114 - val_loss: 0.8065 - val_acc: 0.7500\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.5910 - acc: 0.8164 - val_loss: 0.7252 - val_acc: 0.7730\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.5801 - acc: 0.8195 - val_loss: 0.8067 - val_acc: 0.7415\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.5632 - acc: 0.8253 - val_loss: 0.8137 - val_acc: 0.7422\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.5505 - acc: 0.8272 - val_loss: 0.6932 - val_acc: 0.7826\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.5378 - acc: 0.8322 - val_loss: 0.9460 - val_acc: 0.7073\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.5254 - acc: 0.8360 - val_loss: 0.6978 - val_acc: 0.7786\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.5168 - acc: 0.8379 - val_loss: 0.8921 - val_acc: 0.7368\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.5062 - acc: 0.8424 - val_loss: 0.7901 - val_acc: 0.7526\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.4920 - acc: 0.8462 - val_loss: 0.6657 - val_acc: 0.7909\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.4873 - acc: 0.8475 - val_loss: 0.9750 - val_acc: 0.7089\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.4793 - acc: 0.8509 - val_loss: 0.7757 - val_acc: 0.7625\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.4699 - acc: 0.8526 - val_loss: 0.7678 - val_acc: 0.7598\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.4629 - acc: 0.8545 - val_loss: 0.7930 - val_acc: 0.7597\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.4515 - acc: 0.8596 - val_loss: 0.6978 - val_acc: 0.7844\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.4392 - acc: 0.8636 - val_loss: 0.6710 - val_acc: 0.7926\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.4332 - acc: 0.8650 - val_loss: 0.7249 - val_acc: 0.7742\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.4249 - acc: 0.8675 - val_loss: 0.6876 - val_acc: 0.7842\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.4179 - acc: 0.8695 - val_loss: 0.6177 - val_acc: 0.8071\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.4102 - acc: 0.8732 - val_loss: 1.1939 - val_acc: 0.6915\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.4061 - acc: 0.8729 - val_loss: 0.7878 - val_acc: 0.7606\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.4005 - acc: 0.8746 - val_loss: 0.6239 - val_acc: 0.8039\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.3936 - acc: 0.8773 - val_loss: 0.6273 - val_acc: 0.8059\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.3872 - acc: 0.8802 - val_loss: 0.5841 - val_acc: 0.8211\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.3789 - acc: 0.8835 - val_loss: 0.5863 - val_acc: 0.8203\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.3709 - acc: 0.8839 - val_loss: 0.6137 - val_acc: 0.8091\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.3669 - acc: 0.8873 - val_loss: 0.6152 - val_acc: 0.8077\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.3640 - acc: 0.8870 - val_loss: 0.6704 - val_acc: 0.7876\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.3575 - acc: 0.8888 - val_loss: 0.6125 - val_acc: 0.8124\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3488 - acc: 0.8913 - val_loss: 0.7482 - val_acc: 0.7721\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.3417 - acc: 0.8925 - val_loss: 0.6436 - val_acc: 0.8008\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.3388 - acc: 0.8943 - val_loss: 0.6630 - val_acc: 0.8002\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3363 - acc: 0.8953 - val_loss: 1.0303 - val_acc: 0.7077\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3302 - acc: 0.8977 - val_loss: 0.6981 - val_acc: 0.7801\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.3246 - acc: 0.8993 - val_loss: 0.7736 - val_acc: 0.7778\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.3225 - acc: 0.9000 - val_loss: 0.9034 - val_acc: 0.7348\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.3214 - acc: 0.9002 - val_loss: 0.9498 - val_acc: 0.7229\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3085 - acc: 0.9051 - val_loss: 0.5623 - val_acc: 0.8292\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.3056 - acc: 0.9064 - val_loss: 0.9083 - val_acc: 0.7462\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3017 - acc: 0.9069 - val_loss: 0.6753 - val_acc: 0.7982\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2960 - acc: 0.9097 - val_loss: 0.8200 - val_acc: 0.7688\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2921 - acc: 0.9085 - val_loss: 0.7007 - val_acc: 0.7875\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.2870 - acc: 0.9111 - val_loss: 0.5730 - val_acc: 0.8280\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2868 - acc: 0.9124 - val_loss: 0.6654 - val_acc: 0.8033\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2802 - acc: 0.9140 - val_loss: 0.9222 - val_acc: 0.7542\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2723 - acc: 0.9172 - val_loss: 0.7212 - val_acc: 0.7878\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2724 - acc: 0.9156 - val_loss: 0.5668 - val_acc: 0.8287\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.2662 - acc: 0.9178 - val_loss: 0.6644 - val_acc: 0.8062\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2692 - acc: 0.9163 - val_loss: 0.5836 - val_acc: 0.8216\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2586 - acc: 0.9201 - val_loss: 0.6806 - val_acc: 0.7996\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2588 - acc: 0.9211 - val_loss: 0.6559 - val_acc: 0.8069\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2556 - acc: 0.9227 - val_loss: 0.6227 - val_acc: 0.8114\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2572 - acc: 0.9206 - val_loss: 0.5525 - val_acc: 0.8367\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2480 - acc: 0.9237 - val_loss: 1.9892 - val_acc: 0.6235\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2453 - acc: 0.9242 - val_loss: 0.9709 - val_acc: 0.7505\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2417 - acc: 0.9260 - val_loss: 0.5700 - val_acc: 0.8296\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2377 - acc: 0.9266 - val_loss: 0.5419 - val_acc: 0.8381\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.2350 - acc: 0.9288 - val_loss: 0.7219 - val_acc: 0.7956\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2356 - acc: 0.9276 - val_loss: 0.6147 - val_acc: 0.8211\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2337 - acc: 0.9287 - val_loss: 0.6259 - val_acc: 0.8096\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.2274 - acc: 0.9311 - val_loss: 0.5695 - val_acc: 0.8381\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2224 - acc: 0.9329 - val_loss: 0.5421 - val_acc: 0.8424\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2269 - acc: 0.9294 - val_loss: 0.8632 - val_acc: 0.7679\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2149 - acc: 0.9343 - val_loss: 0.6775 - val_acc: 0.8163\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2183 - acc: 0.9321 - val_loss: 0.6102 - val_acc: 0.8263\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.2136 - acc: 0.9340 - val_loss: 0.6119 - val_acc: 0.8273\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2130 - acc: 0.9346 - val_loss: 0.9060 - val_acc: 0.7550\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2108 - acc: 0.9350 - val_loss: 0.7187 - val_acc: 0.8004\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2061 - acc: 0.9364 - val_loss: 0.6345 - val_acc: 0.8180\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2057 - acc: 0.9381 - val_loss: 0.4849 - val_acc: 0.8626\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.1968 - acc: 0.9413 - val_loss: 0.6472 - val_acc: 0.8304\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1917 - acc: 0.9424 - val_loss: 0.6683 - val_acc: 0.8133\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.1934 - acc: 0.9407 - val_loss: 0.7798 - val_acc: 0.7920\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1913 - acc: 0.9417 - val_loss: 0.7867 - val_acc: 0.7778\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1907 - acc: 0.9420 - val_loss: 0.6375 - val_acc: 0.8199\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.1893 - acc: 0.9419 - val_loss: 0.6015 - val_acc: 0.8279\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1853 - acc: 0.9440 - val_loss: 0.5841 - val_acc: 0.8378\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.1819 - acc: 0.9442 - val_loss: 0.7618 - val_acc: 0.8002\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 2s 46us/sample - loss: 0.1804 - acc: 0.9435 - val_loss: 0.7315 - val_acc: 0.8016\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1793 - acc: 0.9457 - val_loss: 0.7381 - val_acc: 0.8078\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model5.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "8jMKdIGlU0eG",
    "outputId": "6680e992-5559-4379-de7c-2b80fa37a638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with batch normalization\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.7381 - acc: 0.8078\n",
      "Validation accuracy: 80.78\n"
     ]
    }
   ],
   "source": [
    "print('NN with batch normalization'); print('--'*40)\n",
    "results5 = model5.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvExZp5EXHFf"
   },
   "source": [
    "#### NN model, relu activations, Adam optimizers with weight initializers and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rws18ePgW5HZ",
    "outputId": "1761ca54-9f96-43db-9705-dd67994d8d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.7504 - acc: 0.7711 - val_loss: 3.2994 - val_acc: 0.3926\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.5840 - acc: 0.8120 - val_loss: 1.2989 - val_acc: 0.5782\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.5254 - acc: 0.8286 - val_loss: 1.5073 - val_acc: 0.5588\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.4785 - acc: 0.8432 - val_loss: 1.4805 - val_acc: 0.5900\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.4628 - acc: 0.8490 - val_loss: 1.5200 - val_acc: 0.5520\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.4416 - acc: 0.8556 - val_loss: 1.1569 - val_acc: 0.6578\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.4216 - acc: 0.8632 - val_loss: 1.1616 - val_acc: 0.6521\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.3979 - acc: 0.8718 - val_loss: 1.0359 - val_acc: 0.6737\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.3967 - acc: 0.8716 - val_loss: 1.4157 - val_acc: 0.6081\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3868 - acc: 0.8749 - val_loss: 1.1907 - val_acc: 0.6233\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.3657 - acc: 0.8803 - val_loss: 1.0921 - val_acc: 0.6475\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.3518 - acc: 0.8872 - val_loss: 1.0731 - val_acc: 0.6741\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3442 - acc: 0.8874 - val_loss: 1.1014 - val_acc: 0.6781\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.3349 - acc: 0.8899 - val_loss: 1.2347 - val_acc: 0.6433\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.3198 - acc: 0.8963 - val_loss: 1.0597 - val_acc: 0.6864\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.3151 - acc: 0.8965 - val_loss: 1.3385 - val_acc: 0.6374\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.3006 - acc: 0.9024 - val_loss: 1.2399 - val_acc: 0.6477\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.2937 - acc: 0.9026 - val_loss: 1.7937 - val_acc: 0.6011\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2807 - acc: 0.9075 - val_loss: 0.8963 - val_acc: 0.7358\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2783 - acc: 0.9098 - val_loss: 0.9110 - val_acc: 0.7418\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2730 - acc: 0.9120 - val_loss: 0.9502 - val_acc: 0.7330\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.2706 - acc: 0.9109 - val_loss: 0.9985 - val_acc: 0.7336\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2596 - acc: 0.9153 - val_loss: 1.4751 - val_acc: 0.6206\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2459 - acc: 0.9192 - val_loss: 1.0131 - val_acc: 0.7315\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2410 - acc: 0.9215 - val_loss: 1.3443 - val_acc: 0.6587\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2471 - acc: 0.9187 - val_loss: 1.0037 - val_acc: 0.7185\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2468 - acc: 0.9188 - val_loss: 0.8069 - val_acc: 0.7687\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2256 - acc: 0.9253 - val_loss: 0.7956 - val_acc: 0.7738\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.2232 - acc: 0.9268 - val_loss: 0.9224 - val_acc: 0.7399\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2189 - acc: 0.9274 - val_loss: 0.8176 - val_acc: 0.7669\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.2169 - acc: 0.9281 - val_loss: 0.8483 - val_acc: 0.7552\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2185 - acc: 0.9276 - val_loss: 0.9837 - val_acc: 0.7370\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.2079 - acc: 0.9323 - val_loss: 0.8542 - val_acc: 0.7669\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.2026 - acc: 0.9330 - val_loss: 0.9887 - val_acc: 0.7467\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1968 - acc: 0.9350 - val_loss: 1.0074 - val_acc: 0.7355\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1947 - acc: 0.9348 - val_loss: 0.8603 - val_acc: 0.7645\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.1904 - acc: 0.9379 - val_loss: 0.7189 - val_acc: 0.7949\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1789 - acc: 0.9404 - val_loss: 0.9522 - val_acc: 0.7534\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1776 - acc: 0.9414 - val_loss: 0.7963 - val_acc: 0.8006\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1767 - acc: 0.9409 - val_loss: 0.9288 - val_acc: 0.7571\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1766 - acc: 0.9411 - val_loss: 1.1659 - val_acc: 0.7217\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1626 - acc: 0.9458 - val_loss: 0.9868 - val_acc: 0.7480\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.1718 - acc: 0.9429 - val_loss: 0.7413 - val_acc: 0.7992\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.1626 - acc: 0.9460 - val_loss: 1.3095 - val_acc: 0.6971\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1561 - acc: 0.9485 - val_loss: 0.8414 - val_acc: 0.7886\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1580 - acc: 0.9470 - val_loss: 0.7729 - val_acc: 0.8021\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1604 - acc: 0.9472 - val_loss: 1.3858 - val_acc: 0.7020\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1538 - acc: 0.9488 - val_loss: 1.4758 - val_acc: 0.6984\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1425 - acc: 0.9529 - val_loss: 1.3721 - val_acc: 0.7090\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1504 - acc: 0.9494 - val_loss: 1.0456 - val_acc: 0.7430\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.1506 - acc: 0.9496 - val_loss: 1.0405 - val_acc: 0.7593\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1414 - acc: 0.9524 - val_loss: 0.8466 - val_acc: 0.7840\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1373 - acc: 0.9538 - val_loss: 0.9790 - val_acc: 0.7692\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1469 - acc: 0.9508 - val_loss: 0.6021 - val_acc: 0.8406\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1327 - acc: 0.9560 - val_loss: 1.0881 - val_acc: 0.7641\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1317 - acc: 0.9556 - val_loss: 1.0091 - val_acc: 0.7638\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1291 - acc: 0.9558 - val_loss: 1.0545 - val_acc: 0.7565\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1291 - acc: 0.9569 - val_loss: 0.6812 - val_acc: 0.8264\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1252 - acc: 0.9581 - val_loss: 0.8298 - val_acc: 0.8017\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.1217 - acc: 0.9590 - val_loss: 1.1110 - val_acc: 0.7636\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.1219 - acc: 0.9591 - val_loss: 0.8744 - val_acc: 0.7934\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.1249 - acc: 0.9574 - val_loss: 1.2907 - val_acc: 0.7352\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 2s 47us/sample - loss: 0.1188 - acc: 0.9600 - val_loss: 0.9428 - val_acc: 0.7811\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1189 - acc: 0.9592 - val_loss: 1.0607 - val_acc: 0.7717\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1173 - acc: 0.9614 - val_loss: 0.8250 - val_acc: 0.8036\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1102 - acc: 0.9630 - val_loss: 0.8915 - val_acc: 0.7794\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1150 - acc: 0.9620 - val_loss: 0.8700 - val_acc: 0.7963\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1051 - acc: 0.9645 - val_loss: 0.8872 - val_acc: 0.7971\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1120 - acc: 0.9622 - val_loss: 1.2268 - val_acc: 0.7523\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1117 - acc: 0.9622 - val_loss: 0.8054 - val_acc: 0.8112\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.1154 - acc: 0.9614 - val_loss: 0.8851 - val_acc: 0.7975\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.1071 - acc: 0.9640 - val_loss: 1.2763 - val_acc: 0.7354\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.1113 - acc: 0.9617 - val_loss: 1.0526 - val_acc: 0.7878\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0973 - acc: 0.9672 - val_loss: 0.9450 - val_acc: 0.7733\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1012 - acc: 0.9657 - val_loss: 1.3132 - val_acc: 0.7354\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.1073 - acc: 0.9642 - val_loss: 1.2652 - val_acc: 0.7547\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0974 - acc: 0.9671 - val_loss: 0.6400 - val_acc: 0.8382\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0929 - acc: 0.9692 - val_loss: 1.3082 - val_acc: 0.7578\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.0951 - acc: 0.9686 - val_loss: 0.6770 - val_acc: 0.8472\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.0961 - acc: 0.9680 - val_loss: 0.8655 - val_acc: 0.8092\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0991 - acc: 0.9665 - val_loss: 0.8122 - val_acc: 0.8212\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.0931 - acc: 0.9688 - val_loss: 0.7419 - val_acc: 0.8227\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.0910 - acc: 0.9695 - val_loss: 1.1053 - val_acc: 0.7612\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.1015 - acc: 0.9658 - val_loss: 0.8460 - val_acc: 0.8165\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.0871 - acc: 0.9707 - val_loss: 0.8258 - val_acc: 0.8185\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.0931 - acc: 0.9679 - val_loss: 0.7424 - val_acc: 0.8282\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0896 - acc: 0.9692 - val_loss: 1.8734 - val_acc: 0.7012\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 2s 48us/sample - loss: 0.0949 - acc: 0.9678 - val_loss: 0.9185 - val_acc: 0.7866\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.0945 - acc: 0.9676 - val_loss: 1.1041 - val_acc: 0.7891\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0850 - acc: 0.9713 - val_loss: 1.0702 - val_acc: 0.7694\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0888 - acc: 0.9695 - val_loss: 0.8052 - val_acc: 0.8224\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.0860 - acc: 0.9713 - val_loss: 0.7990 - val_acc: 0.8161\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0729 - acc: 0.9757 - val_loss: 1.1168 - val_acc: 0.7737\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0783 - acc: 0.9731 - val_loss: 0.7263 - val_acc: 0.8390\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.0854 - acc: 0.9707 - val_loss: 0.6956 - val_acc: 0.8449\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.0763 - acc: 0.9735 - val_loss: 1.0002 - val_acc: 0.8003\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 2s 50us/sample - loss: 0.0863 - acc: 0.9700 - val_loss: 0.9083 - val_acc: 0.8045\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 2s 51us/sample - loss: 0.0833 - acc: 0.9722 - val_loss: 1.1478 - val_acc: 0.7791\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 2s 49us/sample - loss: 0.0819 - acc: 0.9722 - val_loss: 0.9367 - val_acc: 0.8083\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 2s 52us/sample - loss: 0.0795 - acc: 0.9729 - val_loss: 1.1002 - val_acc: 0.7901\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model5.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "x612tS1vXAfv",
    "outputId": "f11fc93f-16ff-47dd-865b-972c3e338684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with batch normalization\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.1002 - acc: 0.7901\n",
      "Validation accuracy: 79.01\n"
     ]
    }
   ],
   "source": [
    "print('NN with batch normalization'); print('--'*40)\n",
    "results5 = model5.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlIflBua6Hie"
   },
   "source": [
    "<a id='o7'></a>\n",
    "##### Observation 7 - Batch Normalization\n",
    "* Batch normalization didn't result in improvement of score.\n",
    "* Relu activations, changing number of activators, Adam optimizers achieved the best score.\n",
    "* Next, let's try batch normalization with dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04uo_AzWTq5X"
   },
   "source": [
    "<a id='Dropout'></a>\n",
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smSh8h6RZthm"
   },
   "source": [
    "#### NN model, relu activations, SGD optimizers with weight initializers,  batch normalization and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xq5p41MtNvui",
    "outputId": "8c1d1307-fc95-40ba-cfdd-e4733b432c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with dropout - sgd optimizer\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model6 = Sequential()\n",
    "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
    "model6.add(Dense(512, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization()) \n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model6.add(Dense(256, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 2 - adding second hidden layer\n",
    "model6.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 3 - adding third hidden layer\n",
    "model6.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "#Hidden Layer 4 - adding fourth hidden layer\n",
    "model6.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
    "# Adding batch normalization\n",
    "model6.add(BatchNormalization())\n",
    "# Adding activation function\n",
    "model6.add(Activation('relu'))\n",
    "# Adding dropout layer\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model6.add(Dense(10, kernel_initializer = 'he_normal',bias_initializer = 'he_uniform'))\n",
    "# Adding activation function\n",
    "model6.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "colab_type": "code",
    "id": "IV_EP0rf6n6A",
    "outputId": "3fdbd698-c572-4d46-ee16-acf3ae768c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 703,658\n",
      "Trainable params: 701,674\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "E9TPI3fIZGTI",
    "outputId": "2d73d0c6-232d-405f-cbe8-11ea5d49550d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 3s 71us/sample - loss: 2.6054 - acc: 0.1100 - val_loss: 2.3237 - val_acc: 0.1209\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 2.4142 - acc: 0.1332 - val_loss: 2.2139 - val_acc: 0.2095\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 2.3151 - acc: 0.1548 - val_loss: 2.1209 - val_acc: 0.2677\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 2.2315 - acc: 0.1876 - val_loss: 2.0331 - val_acc: 0.3157\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 2.1531 - acc: 0.2181 - val_loss: 1.9579 - val_acc: 0.3408\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 2.0815 - acc: 0.2495 - val_loss: 1.8626 - val_acc: 0.3965\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 2.0079 - acc: 0.2789 - val_loss: 1.7922 - val_acc: 0.4313\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.9351 - acc: 0.3115 - val_loss: 1.7376 - val_acc: 0.4426\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.8709 - acc: 0.3382 - val_loss: 1.6199 - val_acc: 0.4975\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.8116 - acc: 0.3619 - val_loss: 1.5512 - val_acc: 0.5238\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 1.7518 - acc: 0.3841 - val_loss: 1.4736 - val_acc: 0.5408\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.7028 - acc: 0.4066 - val_loss: 1.3984 - val_acc: 0.5655\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 1.6439 - acc: 0.4270 - val_loss: 1.3576 - val_acc: 0.5828\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.5967 - acc: 0.4430 - val_loss: 1.3111 - val_acc: 0.5965\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.5617 - acc: 0.4607 - val_loss: 1.2542 - val_acc: 0.6111\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.5186 - acc: 0.4753 - val_loss: 1.2091 - val_acc: 0.6265\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 1.4842 - acc: 0.4868 - val_loss: 1.1793 - val_acc: 0.6374\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.4473 - acc: 0.5011 - val_loss: 1.1519 - val_acc: 0.6433\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 1.4101 - acc: 0.5162 - val_loss: 1.1057 - val_acc: 0.6606\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.3852 - acc: 0.5309 - val_loss: 1.0990 - val_acc: 0.6641\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.3534 - acc: 0.5418 - val_loss: 1.0614 - val_acc: 0.6762\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.3285 - acc: 0.5496 - val_loss: 1.0304 - val_acc: 0.6890\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 1.2992 - acc: 0.5609 - val_loss: 1.0085 - val_acc: 0.6887\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 1.2786 - acc: 0.5706 - val_loss: 0.9750 - val_acc: 0.7053\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 1.2606 - acc: 0.5766 - val_loss: 0.9636 - val_acc: 0.7069\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 1.2394 - acc: 0.5840 - val_loss: 0.9422 - val_acc: 0.7091\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.2132 - acc: 0.5979 - val_loss: 0.9353 - val_acc: 0.7169\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.1932 - acc: 0.6047 - val_loss: 0.9152 - val_acc: 0.7215\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 1.1793 - acc: 0.6109 - val_loss: 0.8742 - val_acc: 0.7368\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.1572 - acc: 0.6158 - val_loss: 0.9592 - val_acc: 0.7013\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.1357 - acc: 0.6254 - val_loss: 0.8361 - val_acc: 0.7450\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.1246 - acc: 0.6317 - val_loss: 0.8718 - val_acc: 0.7318\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.1089 - acc: 0.6334 - val_loss: 0.8291 - val_acc: 0.7463\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.0959 - acc: 0.6414 - val_loss: 0.8557 - val_acc: 0.7293\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.0835 - acc: 0.6456 - val_loss: 0.8170 - val_acc: 0.7472\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 1.0635 - acc: 0.6545 - val_loss: 0.8103 - val_acc: 0.7564\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 1.0554 - acc: 0.6572 - val_loss: 0.7692 - val_acc: 0.7686\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 1.0389 - acc: 0.6655 - val_loss: 0.7873 - val_acc: 0.7632\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 1.0189 - acc: 0.6723 - val_loss: 0.7743 - val_acc: 0.7618\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 1.0079 - acc: 0.6775 - val_loss: 0.7309 - val_acc: 0.7797\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 1.0044 - acc: 0.6761 - val_loss: 0.7411 - val_acc: 0.7771\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.9949 - acc: 0.6807 - val_loss: 0.7256 - val_acc: 0.7826\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.9799 - acc: 0.6888 - val_loss: 0.7309 - val_acc: 0.7801\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.9662 - acc: 0.6906 - val_loss: 0.7218 - val_acc: 0.7818\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.9578 - acc: 0.6942 - val_loss: 0.6908 - val_acc: 0.7904\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.9417 - acc: 0.7012 - val_loss: 0.7344 - val_acc: 0.7721\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.9413 - acc: 0.6993 - val_loss: 0.7060 - val_acc: 0.7821\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.9270 - acc: 0.7054 - val_loss: 0.7181 - val_acc: 0.7824\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.9109 - acc: 0.7130 - val_loss: 0.6612 - val_acc: 0.7988\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.9078 - acc: 0.7140 - val_loss: 0.6873 - val_acc: 0.7903\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.8992 - acc: 0.7155 - val_loss: 0.6665 - val_acc: 0.8009\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.8844 - acc: 0.7209 - val_loss: 0.6310 - val_acc: 0.8111\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.8843 - acc: 0.7214 - val_loss: 0.6561 - val_acc: 0.7992\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.8734 - acc: 0.7273 - val_loss: 0.6311 - val_acc: 0.8083\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.8673 - acc: 0.7281 - val_loss: 0.6193 - val_acc: 0.8108\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.8486 - acc: 0.7355 - val_loss: 0.6420 - val_acc: 0.8012\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.8503 - acc: 0.7352 - val_loss: 0.6994 - val_acc: 0.7861\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.8442 - acc: 0.7357 - val_loss: 0.5837 - val_acc: 0.8247\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.8391 - acc: 0.7389 - val_loss: 0.6363 - val_acc: 0.8028\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.8335 - acc: 0.7413 - val_loss: 0.5961 - val_acc: 0.8199\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.8294 - acc: 0.7413 - val_loss: 0.5941 - val_acc: 0.8199\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.8155 - acc: 0.7469 - val_loss: 0.6761 - val_acc: 0.7928\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.8109 - acc: 0.7496 - val_loss: 0.6227 - val_acc: 0.8094\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.8007 - acc: 0.7499 - val_loss: 0.6400 - val_acc: 0.8033\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.8021 - acc: 0.7507 - val_loss: 0.5850 - val_acc: 0.8219\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.7899 - acc: 0.7548 - val_loss: 0.5739 - val_acc: 0.8261\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.7775 - acc: 0.7590 - val_loss: 0.6175 - val_acc: 0.8099\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.7847 - acc: 0.7580 - val_loss: 0.5935 - val_acc: 0.8196\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.7709 - acc: 0.7607 - val_loss: 0.7921 - val_acc: 0.7597\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.7697 - acc: 0.7607 - val_loss: 0.5952 - val_acc: 0.8155\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.7648 - acc: 0.7630 - val_loss: 0.5414 - val_acc: 0.8394\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.7613 - acc: 0.7660 - val_loss: 0.5473 - val_acc: 0.8335\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.7515 - acc: 0.7674 - val_loss: 0.5540 - val_acc: 0.8316\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.7506 - acc: 0.7693 - val_loss: 0.5334 - val_acc: 0.8379\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.7394 - acc: 0.7717 - val_loss: 0.5623 - val_acc: 0.8301\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.7449 - acc: 0.7692 - val_loss: 0.5112 - val_acc: 0.8475\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.7367 - acc: 0.7758 - val_loss: 0.5499 - val_acc: 0.8302\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.7347 - acc: 0.7757 - val_loss: 0.5693 - val_acc: 0.8232\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.7263 - acc: 0.7772 - val_loss: 0.6090 - val_acc: 0.8162\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.7270 - acc: 0.7768 - val_loss: 0.6102 - val_acc: 0.8118\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.7203 - acc: 0.7798 - val_loss: 0.5505 - val_acc: 0.8335\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.7166 - acc: 0.7812 - val_loss: 0.5441 - val_acc: 0.8350\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.7112 - acc: 0.7829 - val_loss: 0.5885 - val_acc: 0.8156\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.7027 - acc: 0.7845 - val_loss: 0.5343 - val_acc: 0.8360\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.6959 - acc: 0.7876 - val_loss: 0.5293 - val_acc: 0.8361\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.6922 - acc: 0.7890 - val_loss: 0.5124 - val_acc: 0.8444\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.6874 - acc: 0.7903 - val_loss: 0.5271 - val_acc: 0.8380\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 2s 53us/sample - loss: 0.6906 - acc: 0.7891 - val_loss: 0.5241 - val_acc: 0.8409\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.6743 - acc: 0.7943 - val_loss: 0.5262 - val_acc: 0.8401\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.6803 - acc: 0.7909 - val_loss: 0.5525 - val_acc: 0.8313\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.6779 - acc: 0.7928 - val_loss: 0.5164 - val_acc: 0.8436\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.6767 - acc: 0.7927 - val_loss: 0.4986 - val_acc: 0.8492\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.6632 - acc: 0.7985 - val_loss: 0.5105 - val_acc: 0.8431\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.6640 - acc: 0.7991 - val_loss: 0.4897 - val_acc: 0.8518\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.6698 - acc: 0.7959 - val_loss: 0.5073 - val_acc: 0.8418\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 2s 54us/sample - loss: 0.6591 - acc: 0.8000 - val_loss: 0.6676 - val_acc: 0.7981\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.6608 - acc: 0.7998 - val_loss: 0.6813 - val_acc: 0.7984\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.6470 - acc: 0.8018 - val_loss: 0.4924 - val_acc: 0.8505\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.6573 - acc: 0.8001 - val_loss: 0.5130 - val_acc: 0.8419\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.6478 - acc: 0.8029 - val_loss: 0.4758 - val_acc: 0.8560\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, sgd optimizer\n",
    "sgd = optimizers.SGD(lr = 0.01)\n",
    "model6.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "JpuSGV5WUUJX",
    "outputId": "880b9b5c-f1da-48a1-c91a-471fd15c30e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with dropout - sgd optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4758 - acc: 0.8560\n",
      "Validation accuracy: 85.6\n"
     ]
    }
   ],
   "source": [
    "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
    "results6 = model6.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "658feR9fZ353",
    "outputId": "fd0788fc-79ef-4743-f378-54e217eb1095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 3s 75us/sample - loss: 1.0483 - acc: 0.6722 - val_loss: 1.5585 - val_acc: 0.4775\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.9086 - acc: 0.7194 - val_loss: 1.3776 - val_acc: 0.5337\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.8480 - acc: 0.7403 - val_loss: 1.0837 - val_acc: 0.6561\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.7984 - acc: 0.7575 - val_loss: 1.1266 - val_acc: 0.6141\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.7714 - acc: 0.7648 - val_loss: 1.2182 - val_acc: 0.5853\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.7336 - acc: 0.7772 - val_loss: 1.0197 - val_acc: 0.6763\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.7149 - acc: 0.7826 - val_loss: 1.1113 - val_acc: 0.6252\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.6941 - acc: 0.7898 - val_loss: 0.9268 - val_acc: 0.7488\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.6799 - acc: 0.7955 - val_loss: 1.3377 - val_acc: 0.5425\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.6518 - acc: 0.8032 - val_loss: 0.8574 - val_acc: 0.7210\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.6379 - acc: 0.8080 - val_loss: 1.2970 - val_acc: 0.5655\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.6319 - acc: 0.8079 - val_loss: 0.9649 - val_acc: 0.7167\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.6119 - acc: 0.8154 - val_loss: 1.2310 - val_acc: 0.5864\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.6004 - acc: 0.8187 - val_loss: 0.8527 - val_acc: 0.7557\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.5864 - acc: 0.8228 - val_loss: 1.2250 - val_acc: 0.5955\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5891 - acc: 0.8222 - val_loss: 0.9921 - val_acc: 0.6742\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5695 - acc: 0.8276 - val_loss: 1.0117 - val_acc: 0.6686\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.5593 - acc: 0.8319 - val_loss: 0.8131 - val_acc: 0.7855\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.5542 - acc: 0.8344 - val_loss: 1.1605 - val_acc: 0.6218\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.5464 - acc: 0.8354 - val_loss: 0.9370 - val_acc: 0.6977\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.5382 - acc: 0.8398 - val_loss: 0.7988 - val_acc: 0.7521\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5289 - acc: 0.8412 - val_loss: 0.9359 - val_acc: 0.6881\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.5178 - acc: 0.8447 - val_loss: 0.8229 - val_acc: 0.7292\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.5133 - acc: 0.8455 - val_loss: 1.0570 - val_acc: 0.6738\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5046 - acc: 0.8481 - val_loss: 0.7736 - val_acc: 0.7540\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.5038 - acc: 0.8490 - val_loss: 0.8040 - val_acc: 0.7357\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4964 - acc: 0.8505 - val_loss: 1.0628 - val_acc: 0.6397\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 3s 64us/sample - loss: 0.4956 - acc: 0.8516 - val_loss: 1.0051 - val_acc: 0.6658\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.4762 - acc: 0.8577 - val_loss: 0.9671 - val_acc: 0.6847\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4803 - acc: 0.8545 - val_loss: 0.9152 - val_acc: 0.7100\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4699 - acc: 0.8582 - val_loss: 0.7210 - val_acc: 0.7599\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4675 - acc: 0.8602 - val_loss: 0.7587 - val_acc: 0.7574\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4615 - acc: 0.8604 - val_loss: 0.7750 - val_acc: 0.7526\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4583 - acc: 0.8603 - val_loss: 0.8713 - val_acc: 0.7247\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.4579 - acc: 0.8632 - val_loss: 0.7976 - val_acc: 0.7362\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4483 - acc: 0.8645 - val_loss: 0.7683 - val_acc: 0.7563\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4392 - acc: 0.8675 - val_loss: 0.7715 - val_acc: 0.7536\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4412 - acc: 0.8669 - val_loss: 0.8139 - val_acc: 0.7285\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4326 - acc: 0.8701 - val_loss: 0.6890 - val_acc: 0.7885\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4206 - acc: 0.8730 - val_loss: 0.8502 - val_acc: 0.7258\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4262 - acc: 0.8727 - val_loss: 0.6450 - val_acc: 0.7916\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4188 - acc: 0.8747 - val_loss: 0.5895 - val_acc: 0.8100\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4113 - acc: 0.8745 - val_loss: 0.7252 - val_acc: 0.7735\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4156 - acc: 0.8732 - val_loss: 1.1334 - val_acc: 0.6520\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4083 - acc: 0.8765 - val_loss: 0.6479 - val_acc: 0.7949\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4091 - acc: 0.8773 - val_loss: 0.7718 - val_acc: 0.7485\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.4042 - acc: 0.8782 - val_loss: 0.7290 - val_acc: 0.7611\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3953 - acc: 0.8811 - val_loss: 0.6882 - val_acc: 0.7742\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3882 - acc: 0.8829 - val_loss: 0.8469 - val_acc: 0.7211\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3840 - acc: 0.8865 - val_loss: 0.6448 - val_acc: 0.7923\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3824 - acc: 0.8846 - val_loss: 0.7429 - val_acc: 0.7559\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3770 - acc: 0.8862 - val_loss: 0.6541 - val_acc: 0.7947\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3806 - acc: 0.8847 - val_loss: 0.7222 - val_acc: 0.7739\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3762 - acc: 0.8865 - val_loss: 0.6802 - val_acc: 0.7789\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3678 - acc: 0.8901 - val_loss: 0.5836 - val_acc: 0.8153\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3635 - acc: 0.8897 - val_loss: 0.7012 - val_acc: 0.7723\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3637 - acc: 0.8902 - val_loss: 0.5000 - val_acc: 0.8412\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3613 - acc: 0.8913 - val_loss: 0.7912 - val_acc: 0.7452\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3617 - acc: 0.8900 - val_loss: 0.7234 - val_acc: 0.7732\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3663 - acc: 0.8914 - val_loss: 0.7428 - val_acc: 0.7548\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3541 - acc: 0.8931 - val_loss: 0.4835 - val_acc: 0.8514\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3497 - acc: 0.8946 - val_loss: 0.9309 - val_acc: 0.7019\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3509 - acc: 0.8935 - val_loss: 0.6272 - val_acc: 0.8097\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3499 - acc: 0.8937 - val_loss: 0.6166 - val_acc: 0.8006\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3408 - acc: 0.8973 - val_loss: 0.5850 - val_acc: 0.8120\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3443 - acc: 0.8963 - val_loss: 0.5576 - val_acc: 0.8225\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3357 - acc: 0.8967 - val_loss: 0.5877 - val_acc: 0.8114\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3436 - acc: 0.8964 - val_loss: 0.8347 - val_acc: 0.7368\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3296 - acc: 0.8996 - val_loss: 0.5776 - val_acc: 0.8166\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3298 - acc: 0.9002 - val_loss: 0.7237 - val_acc: 0.7641\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3314 - acc: 0.8998 - val_loss: 0.7168 - val_acc: 0.7644\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3229 - acc: 0.9024 - val_loss: 0.6814 - val_acc: 0.7736\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3281 - acc: 0.8997 - val_loss: 0.5008 - val_acc: 0.8452\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3208 - acc: 0.9037 - val_loss: 0.5365 - val_acc: 0.8301\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3230 - acc: 0.9022 - val_loss: 0.6199 - val_acc: 0.8008\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3145 - acc: 0.9044 - val_loss: 0.5949 - val_acc: 0.8097\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3178 - acc: 0.9046 - val_loss: 0.6488 - val_acc: 0.7916\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3136 - acc: 0.9048 - val_loss: 0.5016 - val_acc: 0.8428\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.3124 - acc: 0.9065 - val_loss: 0.6061 - val_acc: 0.8069\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3083 - acc: 0.9069 - val_loss: 0.5600 - val_acc: 0.8191\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3097 - acc: 0.9063 - val_loss: 0.5729 - val_acc: 0.8153\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3036 - acc: 0.9097 - val_loss: 0.4584 - val_acc: 0.8562\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3066 - acc: 0.9073 - val_loss: 0.5082 - val_acc: 0.8383\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2970 - acc: 0.9109 - val_loss: 0.7692 - val_acc: 0.7568\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3044 - acc: 0.9073 - val_loss: 0.6092 - val_acc: 0.8047\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2958 - acc: 0.9103 - val_loss: 0.4906 - val_acc: 0.8454\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2953 - acc: 0.9108 - val_loss: 0.4721 - val_acc: 0.8436\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2943 - acc: 0.9105 - val_loss: 0.6549 - val_acc: 0.7836\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2943 - acc: 0.9100 - val_loss: 0.6813 - val_acc: 0.7893\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2928 - acc: 0.9110 - val_loss: 0.7207 - val_acc: 0.7628\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2925 - acc: 0.9115 - val_loss: 0.8072 - val_acc: 0.7591\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2900 - acc: 0.9117 - val_loss: 0.8668 - val_acc: 0.7243\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2853 - acc: 0.9132 - val_loss: 0.5118 - val_acc: 0.8381\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2833 - acc: 0.9132 - val_loss: 0.6805 - val_acc: 0.7838\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2863 - acc: 0.9134 - val_loss: 0.5222 - val_acc: 0.8342\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2776 - acc: 0.9172 - val_loss: 0.6173 - val_acc: 0.7970\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2856 - acc: 0.9139 - val_loss: 0.4914 - val_acc: 0.8441\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2740 - acc: 0.9163 - val_loss: 0.4306 - val_acc: 0.8653\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2711 - acc: 0.9185 - val_loss: 0.6014 - val_acc: 0.8122\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2757 - acc: 0.9154 - val_loss: 0.5253 - val_acc: 0.8297\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model6.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "f-n63MmSN9DQ",
    "outputId": "70ff10de-e9d7-4482-b8c5-905e6d835e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with dropout - adam optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.5253 - acc: 0.8297\n",
      "Validation accuracy: 82.97\n"
     ]
    }
   ],
   "source": [
    "print('NN model with dropout - adam optimizer'); print('--'*40)\n",
    "results6 = model6.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YEbd6Ynm6wmp"
   },
   "source": [
    "<a id='o8'></a>\n",
    "##### Observation 8 - Batch Normalization and Dropout\n",
    "* Didn't result in any improvement of score.\n",
    "* NN model, relu activations, SGD optimizers with weight initializers and batch normalization is still the best model.\n",
    "* Next, let's try batch normalization and dropout with adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-x-REsa00x7"
   },
   "source": [
    "<a id='Prediction'></a>\n",
    "### Prediction on test dataset using Model 3 - relu activations, Adam optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "E3V_MkU-018D",
    "outputId": "3ccf507a-70e3-4d2c-d4b9-d55d887ef826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN model with relu activations and changing number of activators\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
    "# Initialize the neural network classifier\n",
    "model3 = Sequential()\n",
    "\n",
    "# Input Layer - adding input layer and activation functions relu\n",
    "model3.add(Dense(256, input_shape = (1024, )))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 1 - adding first hidden layer\n",
    "model3.add(Dense(128))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "#Hidden Layer 2 - Adding second hidden layer\n",
    "model3.add(Dense(64))\n",
    "# Adding activation function\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
    "model3.add(Dense(10))\n",
    "# Adding activation function - softmax for multiclass classification\n",
    "model3.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dFUqzgvDL0Gf",
    "outputId": "67f86db0-42f7-4cef-c552-fcc3b20b3287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 2s 39us/sample - loss: 2.2450 - acc: 0.1494 - val_loss: 1.9481 - val_acc: 0.3398\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 1.5502 - acc: 0.4844 - val_loss: 1.3632 - val_acc: 0.5493\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.2826 - acc: 0.5840 - val_loss: 1.1971 - val_acc: 0.6190\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.1456 - acc: 0.6382 - val_loss: 1.0864 - val_acc: 0.6639\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.0588 - acc: 0.6704 - val_loss: 1.0618 - val_acc: 0.6653\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 1.0035 - acc: 0.6882 - val_loss: 0.9490 - val_acc: 0.7054\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.9524 - acc: 0.7060 - val_loss: 0.9711 - val_acc: 0.7005\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.9115 - acc: 0.7183 - val_loss: 0.8688 - val_acc: 0.7336\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.8822 - acc: 0.7289 - val_loss: 0.8723 - val_acc: 0.7294\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.8557 - acc: 0.7358 - val_loss: 0.8526 - val_acc: 0.7390\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.8294 - acc: 0.7449 - val_loss: 0.8516 - val_acc: 0.7340\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.8123 - acc: 0.7475 - val_loss: 0.7854 - val_acc: 0.7562\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.7956 - acc: 0.7556 - val_loss: 0.8098 - val_acc: 0.7527\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7827 - acc: 0.7574 - val_loss: 0.7563 - val_acc: 0.7686\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7541 - acc: 0.7695 - val_loss: 0.7475 - val_acc: 0.7719\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7410 - acc: 0.7709 - val_loss: 0.7352 - val_acc: 0.7752\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.7287 - acc: 0.7764 - val_loss: 0.7410 - val_acc: 0.7713\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7150 - acc: 0.7795 - val_loss: 0.7157 - val_acc: 0.7821\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.7025 - acc: 0.7840 - val_loss: 0.7085 - val_acc: 0.7825\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6890 - acc: 0.7884 - val_loss: 0.7198 - val_acc: 0.7801\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6722 - acc: 0.7940 - val_loss: 0.7024 - val_acc: 0.7853\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.6632 - acc: 0.7958 - val_loss: 0.7599 - val_acc: 0.7661\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6577 - acc: 0.7969 - val_loss: 0.6656 - val_acc: 0.7979\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6323 - acc: 0.8049 - val_loss: 0.6612 - val_acc: 0.7972\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6246 - acc: 0.8080 - val_loss: 0.6253 - val_acc: 0.8116\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.6241 - acc: 0.8078 - val_loss: 0.6045 - val_acc: 0.8176\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.6085 - acc: 0.8138 - val_loss: 0.6033 - val_acc: 0.8179\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5945 - acc: 0.8184 - val_loss: 0.5880 - val_acc: 0.8226\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.5854 - acc: 0.8206 - val_loss: 0.5891 - val_acc: 0.8232\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5819 - acc: 0.8211 - val_loss: 0.5972 - val_acc: 0.8191\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5704 - acc: 0.8244 - val_loss: 0.5885 - val_acc: 0.8224\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.5728 - acc: 0.8236 - val_loss: 0.5906 - val_acc: 0.8203\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5547 - acc: 0.8282 - val_loss: 0.5743 - val_acc: 0.8260\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5482 - acc: 0.8301 - val_loss: 0.5979 - val_acc: 0.8173\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5397 - acc: 0.8349 - val_loss: 0.5557 - val_acc: 0.8319\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5328 - acc: 0.8364 - val_loss: 0.5550 - val_acc: 0.8320\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5315 - acc: 0.8366 - val_loss: 0.5342 - val_acc: 0.8388\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5190 - acc: 0.8395 - val_loss: 0.5484 - val_acc: 0.8354\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5133 - acc: 0.8424 - val_loss: 0.5546 - val_acc: 0.8307\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5245 - acc: 0.8354 - val_loss: 0.5637 - val_acc: 0.8249\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.5054 - acc: 0.8429 - val_loss: 0.5232 - val_acc: 0.8415\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.5053 - acc: 0.8436 - val_loss: 0.5410 - val_acc: 0.8354\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.5047 - acc: 0.8423 - val_loss: 0.5490 - val_acc: 0.8328\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4911 - acc: 0.8472 - val_loss: 0.5196 - val_acc: 0.8423\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4943 - acc: 0.8452 - val_loss: 0.5047 - val_acc: 0.8465\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4728 - acc: 0.8534 - val_loss: 0.5193 - val_acc: 0.8446\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4728 - acc: 0.8541 - val_loss: 0.5411 - val_acc: 0.8332\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4783 - acc: 0.8500 - val_loss: 0.5254 - val_acc: 0.8409\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4661 - acc: 0.8535 - val_loss: 0.4862 - val_acc: 0.8540\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4548 - acc: 0.8591 - val_loss: 0.4810 - val_acc: 0.8564\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4544 - acc: 0.8582 - val_loss: 0.5049 - val_acc: 0.8487\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4551 - acc: 0.8577 - val_loss: 0.4743 - val_acc: 0.8570\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4540 - acc: 0.8572 - val_loss: 0.4703 - val_acc: 0.8590\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4449 - acc: 0.8603 - val_loss: 0.5001 - val_acc: 0.8488\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4390 - acc: 0.8627 - val_loss: 0.4796 - val_acc: 0.8548\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4353 - acc: 0.8651 - val_loss: 0.4902 - val_acc: 0.8526\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4404 - acc: 0.8614 - val_loss: 0.5234 - val_acc: 0.8398\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 1s 33us/sample - loss: 0.4391 - acc: 0.8615 - val_loss: 0.4935 - val_acc: 0.8507\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.4318 - acc: 0.8632 - val_loss: 0.4953 - val_acc: 0.8497\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4265 - acc: 0.8662 - val_loss: 0.5158 - val_acc: 0.8422\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4205 - acc: 0.8655 - val_loss: 0.5047 - val_acc: 0.8468\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.4207 - acc: 0.8683 - val_loss: 0.4942 - val_acc: 0.8505\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4136 - acc: 0.8702 - val_loss: 0.4649 - val_acc: 0.8625\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4127 - acc: 0.8694 - val_loss: 0.4480 - val_acc: 0.8663\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4169 - acc: 0.8680 - val_loss: 0.4424 - val_acc: 0.8684\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4004 - acc: 0.8735 - val_loss: 0.4501 - val_acc: 0.8655\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4046 - acc: 0.8707 - val_loss: 0.4404 - val_acc: 0.8692\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4078 - acc: 0.8700 - val_loss: 0.4328 - val_acc: 0.8717\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.4011 - acc: 0.8722 - val_loss: 0.4715 - val_acc: 0.8596\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3974 - acc: 0.8742 - val_loss: 0.4439 - val_acc: 0.8680\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3904 - acc: 0.8752 - val_loss: 0.4707 - val_acc: 0.8602\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3903 - acc: 0.8760 - val_loss: 0.4233 - val_acc: 0.8751\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3848 - acc: 0.8788 - val_loss: 0.4433 - val_acc: 0.8659\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3820 - acc: 0.8785 - val_loss: 0.4359 - val_acc: 0.8703\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3744 - acc: 0.8806 - val_loss: 0.4497 - val_acc: 0.8642\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3889 - acc: 0.8754 - val_loss: 0.4522 - val_acc: 0.8640\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.3780 - acc: 0.8807 - val_loss: 0.5019 - val_acc: 0.8474\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3741 - acc: 0.8813 - val_loss: 0.4329 - val_acc: 0.8713\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3686 - acc: 0.8828 - val_loss: 0.4347 - val_acc: 0.8714\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.3624 - acc: 0.8852 - val_loss: 0.4339 - val_acc: 0.8716\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3730 - acc: 0.8813 - val_loss: 0.4617 - val_acc: 0.8615\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3621 - acc: 0.8837 - val_loss: 0.4089 - val_acc: 0.8799\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3633 - acc: 0.8843 - val_loss: 0.4048 - val_acc: 0.8813\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3598 - acc: 0.8852 - val_loss: 0.4693 - val_acc: 0.8583\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3580 - acc: 0.8858 - val_loss: 0.4362 - val_acc: 0.8702\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3572 - acc: 0.8849 - val_loss: 0.4603 - val_acc: 0.8621\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3524 - acc: 0.8885 - val_loss: 0.4158 - val_acc: 0.8780\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3495 - acc: 0.8895 - val_loss: 0.4046 - val_acc: 0.8816\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3430 - acc: 0.8909 - val_loss: 0.4195 - val_acc: 0.8769\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3433 - acc: 0.8904 - val_loss: 0.4739 - val_acc: 0.8565\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 1s 32us/sample - loss: 0.3410 - acc: 0.8909 - val_loss: 0.4222 - val_acc: 0.8758\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3433 - acc: 0.8912 - val_loss: 0.4033 - val_acc: 0.8818\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3392 - acc: 0.8909 - val_loss: 0.4610 - val_acc: 0.8605\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 1s 28us/sample - loss: 0.3302 - acc: 0.8946 - val_loss: 0.3992 - val_acc: 0.8845\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3377 - acc: 0.8926 - val_loss: 0.4248 - val_acc: 0.8755\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3321 - acc: 0.8932 - val_loss: 0.4212 - val_acc: 0.8764\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3279 - acc: 0.8958 - val_loss: 0.4042 - val_acc: 0.8802\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3348 - acc: 0.8917 - val_loss: 0.3966 - val_acc: 0.8849\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.3188 - acc: 0.8980 - val_loss: 0.4341 - val_acc: 0.8735\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.3311 - acc: 0.8932 - val_loss: 0.3852 - val_acc: 0.8892\n"
     ]
    }
   ],
   "source": [
    "# compiling the neural network classifier, adam optimizer\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the neural network for training\n",
    "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "2ao6j_Dl1I9j",
    "outputId": "2197768b-98ca-41a3-f096-158282390297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN with batch normalization\n",
      "--------------------------------------------------------------------------------\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.3852 - acc: 0.8892\n",
      "Validation accuracy: 88.92\n"
     ]
    }
   ],
   "source": [
    "print('NN with batch normalization'); print('--'*40)\n",
    "results3 = model3.evaluate(X_val, y_val)\n",
    "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "2VfnxZqX1Mnm",
    "outputId": "f0a94d2e-950b-45ee-d164-5e7d93fc45a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model on test dataset\n",
      "18000/18000 [==============================] - 1s 52us/sample - loss: 0.6316 - acc: 0.8357\n",
      "Test loss : 0.6316036958562004\n",
      "Test accuracy : 0.8357222\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model on test dataset')\n",
    "predictions = model3.predict_classes(X_test)\n",
    "score = model3.evaluate(X_test, y_test)\n",
    "print('Test loss :', score[0])\n",
    "print('Test accuracy :', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "Zg8aVLVu1rkI",
    "outputId": "04585158-57c9-4abb-d75f-3e1a0bcbd051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      1814\n",
      "           1       0.85      0.86      0.86      1828\n",
      "           2       0.86      0.85      0.86      1803\n",
      "           3       0.79      0.79      0.79      1719\n",
      "           4       0.86      0.88      0.87      1812\n",
      "           5       0.76      0.83      0.80      1768\n",
      "           6       0.83      0.81      0.82      1832\n",
      "           7       0.91      0.85      0.88      1808\n",
      "           8       0.80      0.79      0.80      1812\n",
      "           9       0.84      0.82      0.83      1804\n",
      "\n",
      "    accuracy                           0.84     18000\n",
      "   macro avg       0.84      0.84      0.84     18000\n",
      "weighted avg       0.84      0.84      0.84     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report'); print('--'*40)\n",
    "print(classification_report(y_test_o, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "rTLDrm8u3KR0",
    "outputId": "feec8cdb-8b61-4280-f286-c5cfd29f4b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing the confusion matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7fa60fd0b8>"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAGjCAYAAABdU+ZeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3gUVcPG4d8kG0IPJWCQ+IIgoPAJ\nKr0rYOhFKTakiUgVRBDpRQHpgooE0Jei0kQBkV6kKCSBhCKQYOgEQpUSkIQk8/2RmDdAAlESZnZ5\n7uvay+XsZPfZ47Qz58xZwzRNRERERERExD7crA4gIiIiIiIit1JDTURERERExGbUUBMREREREbEZ\nNdRERERERERsRg01ERERERERm1FDTURERERExGYcGfGmN88f1pz/aZDdt6bVEZxKfHy81RGchpub\nrsGkldartNOOPe083DPk8OqSYuNirY7gVNzd3K2O4DQcqqs0i7p+xLA6w7+V3u0OD+8itqkLnc2J\niIiIiIjYjC75iYiIiIiIc4qPszpBhlFDTUREREREnJPpurcwaOijiIiIiIiIzahHTUREREREnJML\nTwqmHjURERERERGbUY+aiIiIiIg4JdOF71FTQ01ERERERJyThj6KiIiIiIjIg6IeNRERERERcU4a\n+igiIiIiImIzLvyD1xr6KCIiIiIiYjPqURMREREREeekoY8iIiIiIiI2o1kfH7xBoyZSo+GrNGvd\nOdVlAoP30LxtN5q+8Q7tuvW978+MiYnh/cGjqd+qA6+93YuI02cA2Ls/jOZtu9G8bTdebtuVdZt+\nve/Psgtf3wKsXr2AXSHrCQleR/duHQAYPWoge3ZvZEfQGhYumIGXV06Lk1pvxvQJRJzcTUjI+jte\n69XrHW7GRJA3b24LktlPausVQNcu7dizeyMhwesYNXKAhSntI6V1a/Dg3hw9soMdQWvYEbSGevVq\nWZjQnooXL5pUPzuC1nDxfCjv9uhodSzb8PT0ZMuWpQQErGTnzrUMGvQeANOnj+fAga1s376C7dtX\nULp0SYuTWi+lbXDYsL4E71zLjqA1rPj5OwoUeMTChPaRsF4tIzBwFcHB6xg8uDcAs2ZNZs+ejezc\nuRZ//3E4HOoLANh3YAsBgSv5bfvPbN66FICXXmpA0I7VXIk6xLPPPW1xQrEzwzTNdH/Tm+cP3/eb\n7ti1l6xZsjDgo/Es+WbaHa9fuRpF68698Z/wMQV88nPhz0vkzZ0rTe8dcfoMA0dOYNbnY28pn//D\ncsLCjzD0gx6sWPcL6zdtY8JH/fnrxg08HB44HO6cO3+R5m27smHptzgc7vf1HbP71ryvv08PPj75\n8fHJz65dv5M9eza2b1tBi5Yd8fUtwMaNvxIXF8fIj/sDMHDQaEuzxlt8xaRatYpci7rG1/+dzLPP\n1k4q9/V9FP9p4yhR4gkqVqrHhQt/WpgygZubtddgUluvHnnEmw/79aBps3bExMSQL19ezp27YGlW\nq9crSHndGjy4N1FR15g0yd/idP+T/keL9OPm5sbxozupUq0Rx49HWB0HD3d7nKRmy5aVa9eu43A4\n2LDhe/r0GU7Hjm+wcuUGfvxxhdXxAIiNi7U6QorbYI4c2bl6NQqA7t068NRTxenW/UMrYwLg7nZ/\n5x7p4db1ajF9+gwjd+5crF69EYA5cz5jy5YAZsz4xtKcDhvU1b4DW6hRrckt5wYlShQlPt5kymcj\nGTBgFCHBey1MmCDq+hHD6gz/VvSh7el6ePIsWsk2dXHPsznDMJ40DKOfYRhTEh/9DMN4KqODlXvm\nabxy5kj19RVrf6FOzaoU8MkPcEsj7afVG3i1Y0+at+3G8LFTiItL22wwG7Zso2mDOgD4PV+dgJ27\nME2TLJkzJzXKomNiwLDN/7/7Fhl5ll27fgcgKuoaoaHhFCzow7p1m5PqLSAwhIK+BayMaQtbtwZw\n8c9Ld5SPHz+M/gNGkhEXPZxVautVp7ffZNz4qcTExABY3kizi9TWLUm72rWqcfjwMVs00uzk2rXr\nAHh4OHA4PLSfSkVK2+DfjTSArNmyqu6SSb5eeXg4ME0zqZEGEBS0C1+dN6QqLOwQf/xx2OoYriM+\nPn0fNnLXhpphGP2A+YABBCY+DGCeYRiWXlY6evwkV65G0a77B7Tq0IOlK9cBcOjocVat38TcaRNY\nPPsL3NzcWL5m4z3eLcHZcxfwye8NgMPhTvZsWbl0+QoAe/aF0vSNd3ipTReG9O1+371pdlSokC9l\nnilFYGDILeXt2ra6ZQcs/9O4sR+nIk6zZ89+q6PYVvL1qlixIlStWoEtm5exdu0iypYtY3U8W+va\npT3BO9cyY/oEcuXysjqOrbVq1ZT5C5ZYHcN23Nzc2L59BcePB7NhwxaCgnYBMGxYHwIDVzF27GAy\nZcpkcUr7GjGiH4cPBfHaay8xbPg4q+PYhpubGwEBKzlxIoT167cmrVcADoeD119/mTVrNlmY0D5M\n02TpT3PY8usy2nd4zeo44mTuNTbjLaCUaZo3kxcahjER2Ad8klHB7iUuLp79oX8wc8onREdH88Y7\nvSlT6kkCduxif2g4r77VE4Do6GjyJPa2vdt/BBGnznAz9ianz5yjedtuALRu1ZSXGvrd9fNKl3qS\npd/6c+jocQZ+PIHqlcrj6ek6B7ds2bIyf54/ffoMu+UqYr9+PYiNjWPevB8tTGdPWbJk5sN+Pajf\n4HWro9jW7euVw+EgT+5cVK/RhHLlnuG7b6dS4smqVse0JX//OYwc+SmmaTJ8+AeMGzuEtzu9b3Us\nW/Lw8KBxIz/Lh2fbUXx8PJUqNcDLKycLFkynZMniDBkylsjIs2TKlIkvvhjN++93ZvToKVZHtaUh\nQ8YwZMgYPvigO127tmfEiAlWR7KF+Ph4Klasj5dXThYuTFiv9u8/CMCUKSPZujWQX38NtDilPbxY\npyWnT50hX768LPtpLgfDDqlu0ttDPOtjPPAocOy28gKJr1nmkfzeeHnlIGuWzGTNkpmyz/wfYeFH\nME2TJvXr8F6X9nf8zZTRQ4DU71HLny8vkWfP45M/H7GxcURdu06u2ybRKFr4P2TNkoU/Dh/l/54q\nnnFf8AFyOBwsmD+d+fOXsHTpqqTyN99sSYP6talX/1UL09lX0aKFKVz4P+zcsRZImEAjMGA1Vao2\n5MyZcxans15K61VExGmWLF0JwI4du4iPN/H2zsP58xetjGpLZ8+eT3r+1VffsmTJbAvT2Fu9ei8Q\nErL3ljqTW12+fIVNm37Dz+95Pv10OpAwgdacOYvo1auTxensb968H1i2bK4aardJWK+24ef3PPv3\nH2TgwF54e+ehWzfr7+Wzi9OnEiamO3fuAj/9tJqy5cqooZbeHuIfvO4FrDcMY6VhGNMTH6uA9UDP\njI+XuheqVyJkzz5iY+P468YN9u4Lo0jhx6hU7hnW/rKVC4ljzS9fucqpyDNpe89qlVi6ImEI5Zpf\ntlCxbBkMw+DkqUhiYxNWglORZzhy7AQFXWj2J3//cYSG/sHkKTOSyvxefJ73e3emeYsO/PXXDQvT\n2dfvv4dS0LcMxYpXoljxSpw8eZoKFeuqkZYopfVq2bLV1KxZBYBiTzyORyYPNdJS4ZN4/y1As6b1\n2bcvzMI09vbqK8007DEF3t55kmbszZzZk9q1qxMWFn7LutWkiR/792vdSskTTzye9LxJ47qEhR2y\nMI19pLxeHaJ9+1epU6cGbdp01/18ibJmzUL27NmSnteqXV3bm/wjd+1RM01zlWEYxYEKQMHE4ggg\nyDTNDG2+9h36CUEhe7h06Qq1m7Wm61tvEhubMDPUKy81pGjh/1C1YjlebtsFN8ON5o3rUqxIYQB6\nvN2GTr0GEm/G4+FwMLB3Vx71uXfD6uVGden/0Tjqt+qAV84cjBuecEUoeM8+vpq7EIfDgZubwaA+\n3cjtIveLVKlSntZvtGDv3gMEBiT0egwZMoaJE0eQyTMTK37+DoDAwGC693i4p1KfO/cLataojLd3\nHo4c3sGIEeP576z5VseypdTWq1mzFzB9+niCd64jJiaGjh3fszipPaS0btWsWYUyZUpimiZHj52k\na9d+Vse0paxZs1Cndg26qH7u4OOTnxkzJuLu7oabmxuLFy9n5coNrFw5D2/vPBiGwZ49++nxkO/b\nIeVtsF79WhQvXhQzPp5jxyPUS5TIxyc/M2dOxN3dPdl6tZ6oqMMcPx7Bpk0JF02WLl3FqFGTLU5r\nrfz5vZk3P2HmXofDnYULl7Fu7WYaN/Fj/IRheHvnYfHir9mzZz/Nmra1OK0Tc+Ghj7adnv9hYIfp\n+Z2JHaZRdxZWT8/vTLRepZ127Glnl+n5nYEdpud3JnaYnt9Z2GF6fmfh1NPz71ufvtPzl6ptm7rQ\n2ZyIiIiIiIjN6JKfiIiIiIg4Jxce+qiGmoiIiIiIOCcXvoVBQx9FRERERERsRj1qIiIiIiLilDJ4\nInpLqaEmIiIiIiLOyYXvUdPQRxEREREREZtRj5qIiIiIiDgnTSYiIiIiIiIiD4p61ERERERExDm5\n8D1qaqiJiIiIiIhzinfdWR819FFERERERMRm1KMmIiIiIiLOSUMfRUREREREbEazPoqIiIiIiMiD\nkiE9ajl8n8+It3U5Vw+vsjqCU8lRpJ7VEcQFZfHwtDqC07gRG2N1BKcR78JDcdKbu5u71RHERUVr\nn/VwcOH9rYY+ioiIiIiIc9LQRxEREREREXlQ1KMmIiIiIiLOyYV71NRQExERERERp2Sa+sFrERER\nEREReUDUUBMREREREecUH5++jxQYhvG1YRhnDcP4PYXX3jcMwzQMwzvx34ZhGFMMwwg3DGOPYRjP\nJVu2rWEYfyQ+2t7rq6mhJiIiIiIizsmMT99HymYBd/xOlGEYjwF+wPFkxfWBYomPTsCXicvmAYYC\nFYEKwFDDMHLf7aupoSYiIiIiIpIK0zQ3AxdTeGkS8AFgJitrCswxE2wHchmGUQCoC6w1TfOiaZp/\nAmtJofGXnCYTERERERER52TRrI+GYTQFIkzT3G0YRvKXCgInkv37ZGJZauWpUkNNREREREScU+rD\nFTOMYRhZgQEkDHvMMBr6KCIiIiIiknZFgceB3YZhHAV8gWDDMHyACOCxZMv6JpalVp4qNdRERERE\nRMQ5PYBZH29nmuZe0zTzm6ZZ2DTNwiQMY3zONM1IYBnQJnH2x0rAZdM0TwOrAT/DMHInTiLil1iW\nKg19FBERERER5/QAhj4ahjEPeB7wNgzjJDDUNM2vUll8BdAACAeuA+0BTNO8aBjGR0BQ4nIjTNNM\naYKSJGqoiYiIiIiIpMI0zdfu8XrhZM9NoFsqy30NfJ3Wz3XJoY+enp5s2bKMwMBVBAevY/Dg3re8\nPmHCcM6fP2BRuvQ3eMzn1HypHS+175nqMkG7fqdFx940a9eTdj0H3fdnxsTcpM/w8TR4oyuvd+lH\nRORZAPYe+IMWHXvTomNvmr/1Huu3bL/vz7ILX98CrF69gF0h6wkJXkf3bh2SXuvapR17dm8kJHgd\no0YOsDClPaRWV4MGvcfhQ0EEBqwiMGAV9eq+YHFSe9i7fzPbAleyddtyftmyFID+A3oS+sdvbN22\nnK3bluNX93lrQ9rEdP/xnDyxi5DgdUll334zlaDA1QQFruZg2DaCAu86kuShkdp2OHRoH3YErSEw\nYBU/L/+WAgUesTip9VI7b+jcuS379m3mxo3j5M171587emiorv6ZGdMnEHFyNyEh65PKBg/uzdEj\nO9gRtIYdQWuoV6+WhQldgAVDHx8UI6HRl74yZ/5P+r/pP5QtW1auXbuOw+Fgw4bF9OkzjMDAEJ57\nrjTdu3egSZO6eHs/ZWnGK4dXpsv77Ni9j6xZMjNw9BR+/O/kOz8n6hpvdu/PtDGDKfBIPi78eYm8\nuXOl6b0jIs8y6JPP+O+nH91SPn/JSg4ePsaQ3p1ZuWEr67dsZ/zQPvx1IxoPDwcOd3fOXbhIi469\nWf/9Vzjc3e/7e+YoctefmshwPj758fHJz65dv5M9eza2b1tBi5YdeeQRbz7s14OmzdoRExNDvnx5\nOXfugqVZrZZaXbVo0YhrUdeZ9Km/1RGTeLp7WB2Bvfs3U7N6Uy5e+DOprP+AnkRdu8Znk2damOxW\nN2JjrI5AtWoViYq6xn+//pRnn6tzx+tjxgzmyuWrjBz1qQXp/ue2qZotkdp2GBFxmqtXowDo1rU9\nTz1VjO49rLvAZGB9XUHK5w3R0TFcunSZNWsWUKVKIy4k20YfZs5SV3HxcVZHoFq1ilyLusbX/53M\ns8/WBhIaalFR15g0yT7HwpsxEfbYEP+Fv37+NF3bHVka9rJNXbjs0Mdr164D4OHhwMPDgWmauLm5\nMXr0ANq2fZcmTepanDD9lCtTKqlHKyUr1m2mdvVKFHgkH8AtjbSf1m7iux9+5ubNWJ5+qhiDenXC\nPQ2Nqo2/BtGl3SsAvFizMqMmz8A0TbJk9kxaJjrmJtjgZCW9REaeJTKxnqOirhEaGk7Bgj50aP8a\n48ZPJSYm4ST2YW+kQep1JXK/tm4NoFAh31Rfb9G8MXXrvfIAE9lXatthaOgfSctkzZaVDLhe65RS\nOm/YvXufxansSXWVdvfaZ4nczb8e+mgYRvv0DJLe3NzcCAhYyYkTIaxfv5WgoF106dKO5cvXJh24\nHhbHTp7iytUo2vcaTKtOfVi2eiMAh4+dZPXGX5nz2Si+nzkRdzc3fl63OU3vefb8BXzy5wXA4e5O\n9uxZuXTlKgB79h+kWbuevNzhPYa890669KbZTaFCvpR5phSBgSEUK1aEqlUrsGXzMtauXUTZsmWs\njmcryesKoHOXtuwIWoO//3hy5fKyOJ09mKbJkmWz2bR1Ke3av5pU3umdNvwWsIIvvhxDrlw5LUzo\nHKpVq8jZs+cIDz9idRTbuX07HD78A8LDA3jt1ZcYPmK8xensIaXzBkmZ6ur+de3SnuCda5kxfYKO\nhffLjE/fh43czz1qw9MtRQaIj4+nYsX6FC1akfLly1CtWgWaN2/I1KmzrI72wMXGxXPg4CG+GD0Q\n/3FD8J/7PUdPnGJ78B72HzzEa50/oEXH3gSE7OXk6TMA9Bz8CS069qbrhx+zL+xQ0n1nP65cf49P\ng9Ili7Nk1mTmTxvLzO9+IDrG+uFS6SlbtqzMn+dPnz7DuHo1CofDQZ7cuaheown9+4/ku2+nWh3R\nNm6vq+nT5/LUU9UoX6EukZFnGTNmsNURbaFunVbUqNqE5i914O133qRK1fLMnPktZf7veapWakhk\n5FlGjh5odUzbe+WVpixYuNTqGLZz+3YIMHToWJ54oiLz5v9Ily7trA1oE7efN5QsWdzqSLaluro/\n/v5zKPFkFcqW8+N05FnGjR1idSSxqbsOfTQMY09qLwFOcffx5ctX2LRpGzVrVqFIkULs35/QY5Q1\naxb27dtMqVI1LE6Y8R7Jl5dcOXOQNUtmsmbJTNnSJQk7dBTTNGlS9wV6vd36jr+Z/NGHQOr3qOX3\nzkvk2Qv45PMmNi6OqKjr5MqZ45ZlihTyJWuWzIQfOU6pEk9k3Bd8gBwOBwvmT2f+/CUsXboKgIiI\n0yxZmnC/4Y4du4iPN/H2zsP583edcdXlpVRXZ8+eT3r966+/48cfZlmUzl5OJ14gOX/uAsuXraFs\nuTL89mtQ0uuz/zufhYvtc6+aHbm7u9OsaX0qVW5gdRRbSWk7TG7+/B9ZumQOH3000YJ09vT3eYOf\n3/Ps33/Q6ji2prr6d5IfC7/66luWLJltYRoXYLMJQNLTvXrUHgHaAI1TeNj2Rhxv7zx4eSUME8qc\n2ZPatasTHLyXwoXLUaJEVUqUqMr16389FI00gFpVKxCy9wCxcXH8dSOavQcOUqRQQSo9V5q1m7Zx\n4c9LAFy+cpVTaRwW+nyV8klDKNdu2kaFZ5/GMAxOnj5DbFzCzbunIs9y5HgEj/rkz5gvZgF//3GE\nhv7B5CkzksqWLVtNzZpVACj2xON4ZPJ46BtpkHJd+SRbF5o2qce+fWFWRLOVrFmzkD17tqTntWpX\n48D+gzziky9pmcZN6nJgn06C7qZ27eqEhR0iIuK01VFsJaXt8ImihZOeN27kR1hYuAXJ7CWl84aw\nsEMWp7In1dX9S34sbNa0vo6F98uFhz7eazKR5UB20zTvGHxsGMYvGZIoHfj45GfmzIm4u7vj5ubG\n4sXLWZmGIXvO6oOPJhK063cuXb5K7ZYd6dbu1aTGUqsmdSlSyJeqFZ6l+Vvv4WYYvNywDsUeLwRA\njw6v8U7fEcSbJg53dwb2ejtNDauXG9am/6jJNHijK145szM2cXrekL0H+Oq7H3E43HFzMxjYqxO5\nvVzj3poqVcrT+o0W7N17gMCAhCvTQ4aMYdbsBUyfPp7gneuIiYmhY8f3LE5qvdTqqtUrTSlTuhSm\naXLs2Em6df/Q4qTWy5/fm2/nTwMS7vdctHAZ69ZuZvrMCTxduiSmaXL82El6vquhjwBz53xOjRqV\n8fbOw+FDQYz4aAKzZs2nVcsmLFi4xOp4tpLadtiu3asUL16U+Ph4jh8/aemMj3aR2nlD167t6d27\nMz4++QgKWsPq1Rvo0qWf1XEtpbr6Z+bO/YKaifusI4d3MGLEeGrWrEKZMgn796PHTtK1q+pJUuay\n0/M7g/Sanv9hYfX0/OKa7DA9v7Oww/T8zsIO0/M7C7tMzy+uxw7T8zsLp56e/8dP0nd6/pc+tE1d\nuOz0/CIiIiIi4uJsNlwxPd3PrI8iIiIiIiKSAdSjJiIiIiIizsmFZ31UQ01ERERERJyTCzfUNPRR\nRERERETEZtSjJiIiIiIizikDZrC3CzXURERERETEOWnoo4iIiIiIiDwo6lETERERERHn5MI9amqo\niYiIiIiIc9IPXouIiIiIiMiDoh41ERERERFxThr6KCIiIiIiYjMuPD2/hj6KiIiIiIjYjHrURERE\nRETEOWno4z8TFx+XEW/rcnIWqW91BKdyZe8CqyM4jbxl3rA6gtOIjrtpdQRxQfEufOKQ3jI5PKyO\n4FTitG6lmacjk9UR5EFw4W1CQx9FRERERERsRkMfRURERETEObnw76ipoSYiIiIiIk7JjNesjyIi\nIiIiIvKAqEdNRERERESckwtPJqKGmoiIiIiIOCcXvkdNQx9FRERERERsRj1qIiIiIiLinDSZiIiI\niIiIiDwo6lETERERERHnpMlEREREREREbMaFG2oa+igiIiIiImIz6lETERERERHnZLruZCJqqImI\niIiIiHPS0EfnMmP6BCJO7iYkZH1S2eDBvTl6ZAc7gtawI2gN9erVsjChfXh6erJlyzICA1cRHLyO\nwYN7A9C5c1v27dvMjRvHyZs3t8Up08+QT7+i5hvv8lLXQSm+HrQnlCqtutKyxxBa9hjCtHlL7/sz\nY27epO+YqTR8ux+v9/6IiDPnAdgbdjjpc1p0H8L633be92fZjZubG79uW86ixTMBqFmzMlt/+4nA\noFX4Tx+Pu7u7xQmt5+tbgNWrF7ArZD0hwevo3q0DAN/MnUpgwCoCA1YRFvYbgQGrLE5qD9P9x3Py\nxC5CgtcllZUpXZItm5cRFLiabb/9TLlyz1iY0D5SOhYOG9aX4J1r2RG0hhU/f0eBAo9YmNB+3Nzc\n+G3bz3y/+CsApn45hu3bVxIQsJJvvp1KtmxZLU5ovdTOG6ZNG0tg4CqCglbz3XfTVFfJ3H4sXLN2\nIb9t/5nftv/MH4e2M2+Bv8UJxa5csqE2e85CGjV6447yyVNmUK68H+XK+7Fq1QYLktlPdHQ09eq9\nSoUK9ahQoR4vvliTChWeZdu2HTRo8DrHjp2wOmK6alKnGl8O733XZZ4rVZxFn41g0Wcj6Pxa0zS/\nd8SZ83T48JM7yn9Ys4Wc2bLx84wxvNnUj09nLQTgiUIFmffpUBZ9NoIvR/RmxBeziY2L+2dfyOa6\ndmtPWGg4AIZh4D9jPO3avEuF8vU4cSKCN1o3tzih9WJj4+jX7yOeebY21Ws0pXPntjz5ZDFav9mV\nChXrUaFiPZb8uJIlS1daHdUW5sxdRKPGrW8pGzV6IB+PnET5CnUZPmICo0cNtCidvaR0LJww4Uue\nK/si5cr7sWLFOgYNfM+idPbULdk+C6DfBx9RqVJ9Klasz8kTp+jcua2F6ewhtfOGvn1HUKFCPcqX\nr8uJExF06dLO6qi20fW29crvxVZUqdSQKpUaEhgQzLKlqy1M5wLizfR9pMAwjK8NwzhrGMbvycrG\nGYYRahjGHsMwfjQMI1ey1/obhhFuGEaYYRh1k5XXSywLNwzjw3t9tXs21AzDeNIwjNqGYWS/rbze\nvf7WKlu3BnDxz0tWx3Aa165dB8DDw4GHhwPTNNm9ex/Hjp20OFn6K/d/JfDKkf3eC6Zg+cbfeP29\nEbTsMYQRn88iLi5tXe2/bA+mSe2qALxYrRwBuw9gmiZZMnviSOxRio65iWEY/yqXXT1a0Id69V5g\n9qwFAOTNm5uYmJuEhx8BYMP6rTRtZtvdyAMTGXmWXbsS9vtRUdcIDQ2nYEGfW5Zp3qIRCxfcf++u\nK9i6NYA/b9u/m6ZJzsTt2itnDk6fPmNFNNtJ6Vh49WpU0vOs2bJiuvC9Hf9Uwj6rFrNmzU8qS15f\nmbNkVn0lSum8IXldZVFdJbn9WJhcjhzZqVGzCst/WmNBMhdixqfvI2WzgNtPWtYC/2eaZmngINAf\nwDCMksCrQKnEv5lqGIa7YRjuwBdAfaAk8Frisqm6a0PNMIx3gaVAD+B3wzCSdy+Mutvf2lHXLu0J\n3rmWGdMnkCuXl9VxbMPNzY2AgJWcOBHC+vVbCQraZXUkS+0ODadF9yF0GTqR8GMRABw+cYpVmwOZ\nPW4Aiz4bgZubGz//si1N73fmwiUeyZcHAIe7O9mzZuHSlYQD2p6wQ7zUdSDNuw9mcNc2SQ03VzB2\n7BAGDfqE+MSx4+fPX8ThcPDsc08D0Oyl+vgWLGBlRNspVMiXMs+UIjAwJKmsWrWKnD1znvBDR60L\nZnN9+gxj9OhBHAoP5JNPBjNo8GirI9naiBH9OHwoiNdee4lhw8dZHcc2xo4dwsBBo4m/7Yr6NP9x\nHDkSRPHiRfnyy1nWhLOZ1F58ofIAACAASURBVM4bpk8fz7FjOylRoihTp/7X4pT2cPuxMLlGjf3Y\n9MtvtzRyxZ5M09wMXLytbI1pmrGJ/9wO+CY+bwrMN00z2jTNI0A4UCHxEW6a5mHTNGOA+YnLpupe\nPWpvA2VN02wGPA8MNgyjZ+JrTnX5399/DiWerELZcn6cjjzLuLFDrI5kG/Hx8VSsWJ+iRStSvnwZ\nSpYsbnUkyzz1RCFWfz2e7z8fweuNatPr4ykABOzaz4FDx5J61AJ2H+Bk5DkAen38GS17DKHbsEns\nCz+adN/ZkrVb7vl5pUsU5cepI5k3aQhfLfqZ6JibGfr9HpR69Wtx7tx5doX8fkt5uzY9GDNmML9s\nXkJU1DXiXPgG4H8qW7aszJ/nT58+w245aL/SqikLF6o37W46dWpD377DKfpEBfr2HYa//3irI9na\nkCFjKFK0PPPm/UjXru2tjmMLCfusC3fsswA6v9OXokUrEhYWTosWjS1IZz+pnTd06tSHxx8vT2ho\nOC1bqq5SOxb+rWWrxixauOwBp3JBD2DoYxp0AP6+R6EgkPzeoZOJZamVp+pesz66maYZBWCa5lHD\nMJ4HvjcMoxBO1lA7e/Z80vOvvvqWJUtmW5jGni5fvsKmTdvw83ue/fsPWh3HEtmzZkl6Xr18GUZ+\nOZc/L1/FBJrUqkLPdi3v+JtPB/UAEu5RGzxpJl9/cuuQ40fy5uLMuYv4eOchNi6OqOt/kSvnrcMv\nizz2KFmyeBJ+7CSlij2e/l/sAatUqSwNGtbBr+4LZM7sSY4c2Zn51SQ6vvUefi+2AqBW7eo88YTz\nf9f04HA4WDB/OvPnL2Hp0v9NGuLu7k7TpvWoXKWBhens783WLejdO+Hi2/eLlzNtmnqJ0mLevB9Y\ntmwuI0ZMsDqK5SpXKkfDhnWom2yf9dVXk3jrrYR7+OLj4/l+0U+81/sd5s5dZHFa+0jpvCE+Pp5F\ni5bRu3cX5sx5uOvqbsfCvHlzU7ZsGV575R2rYzo90+KLvoZhDARigW/T+73v1aN2xjCMpOmzEhtt\njQBv4On0DpORfHzyJz1v1rQ++/aFWZjGPry98+DllROAzJk9qV27OmFhhyxOZZ3zf15OGle/N+ww\n8aZJrpzZqVjmKdb+uoMLl64AcPlqFKeSNf7v5vmKz7Js/a8ArN26gwqln8IwDE5GnkuaPOTU2fMc\nPRnJo/m9M+BbPXjDho6jRLEqlHqqOu3a9GDTpt/o+NZ75MuXF4BMmTLRu/c7fDUz3fdpTsnffxyh\noX8wecqMW8pr16pO2MFDREREWpTMOZw+fYYaNSoD8MILVZPug5Q7Jb840qRx3Yd6f5/c0KFjKV6s\nMiWfqkbbxH3WW2+9R5EihZKWadiwDgdVXymeNxw8ePi2unqRsLDw1N7ioZHasRAShv+vWrmB6OgY\ni1PK/TAMox0JbaM3zP/dmBkBPJZsMd/EstTKU3WvHrU2JLQQkySOxWxjGIZt5xKdO/cLataojLd3\nHo4c3sGIEeOpWbMKZcqUxDRNjh47Sdeu/ayOaQs+PvmZOXMi7u7uuLm5sXjxclauXE/Xru3p3bsz\nPj75CApaw+rVG+jSxfnr7IOx09ixN5RLV6Ko07Y3Xd9oRmxsQmOpVYMXWLs1iIUrN+Lu5o6npwdj\nP+iMYRgU/U9Bur/5Mp0HjyfeNHG4uzOgy5tpali95FeDAROm0/Dtfnhlz8bYfp0BCNn/B19//zMO\nd3cMN4OBXd4kt1eODP3+VuvZqxP169fCcHNj5oxv2LQpbff5ubIqVcrT+o0W7N17IGkK/iFDxrBq\n9UZatmqiSURuM3fO59RI3L8fPhTEiI8m0LnLB0ycMByHw8GNG9F00f4dSPlYWK9+LYoXL4oZH8+x\n4xF063bPScceWoZhMH3GBHLmyI5hGOzde4CePVP+aZeHSWrnDRs2LCZHUl3tp0cPzb56Ny1aNGbC\nhC+tjuEa/v1wxfuSOLHiB0BN0zSvJ3tpGfCdYRgTgUeBYkAgCaMRixmG8TgJDbRXgdfv+hkZMSuP\nR6aCmuonDdzdXGfiiAfh8t75915IAMhb5s6fp5CU3YyPvfdCAqBZ3P4B1VXaZXJ4WB3Bqeje3rRz\n6DwrzaKuH3GqW5qSu/Zx63Td4WYb9M0ddWEYxjwS5uvwBs4AQ0mY5dETuJC42HbTNDsnLj+QhPvW\nYoFepmmuTCxvAHwKuANfm6Y58m5Z7tWjJiIiIiIi8tAyTfO1FIq/usvyI4E7GmGmaa4AVqT1c9VQ\nExERERER52TR0McHQQ01ERERERFxTi48HPhesz6KiIiIiIjIA6YeNRERERERcU4a+igiIiIiImIz\npoY+ioiIiIiIyAOiHjUREREREXFOGvooIiIiIiJiL6ZmfRQREREREZEHRT1qIiIiIiLinFx46KN6\n1ERERERERGxGPWoiIiIiIuKcXLhHTQ01ERERERFxTvodNREREREREXlQMqRHzTCMjHhbl2Piul21\nGcHr6VetjuA0/vy+l9URnEauFpOsjuA0CuV8xOoITuPYlTNWR3AaN+NirY7gVNzd3K2O4DRuxmvd\neiho6KOIiIiIiIi9mC7cUNPQRxEREREREZtRj5qIiIiIiDgnF+5RU0NNREREREScU7xmfRQRERER\nEZEHRD1qIiIiIiLinDT0UURERERExGZcuKGmoY8iIiIiIiI2ox41ERERERFxSqbpuj1qaqiJiIiI\niIhz0tBHEREREREReVDUoyYiIiIiIs7JhXvU1FATERERERGnZLpwQ01DH0VERERERGzGJRtqvr4F\nWLN6Ibt3bWBXyHq6d38LgNy5c7FixXfs27eFFSu+I1cuL4uTWs/XtwCrVy9gV8h6QoLX0b1bBwBG\njxrInt0b2RG0hoULZuDlldPipNbz9PRky5ZlBAauIjh4HYMH9wZg1qzJ7NmzkZ071+LvPw6HwzU6\nqofO/4UXhs6m+biFd13u9+NnKdt3Omt3H77vz7x8/QbvTFtO49HzeGfacq5cjwZg4+9HaTl+Ea0m\nfM/rkxYTcvj0fX+WXaS2Df6tV89ORN84Qd68uS1KmP5GTx7C9v1r+XnzghRfr1ClLMGHNrFs43cs\n2/gd3d9/+74/M1MmDz6dMZp1gUv4ftVsCj5WAICqNSvy47pvWL5pAT+u+4ZK1crf92fZRWrHwuYv\nN2RXyHpu/HWc554rbXFKe1BdpV3CsXApAQEr2blzLYMGvZf02rBhfdmzZyMhIevp2rWddSFtIrX9\n+zdzpxIYsIrAgFWEhf1GYMAqi5M6uXgzfR82YmTElJaZPH0t/ZY+Pvnx8cnPrl2/kz17NgK2r6RF\ni7do06YVFy9eYtz4L+jbpxu5c3sxYOAoy3IahmHZZ//t9rravm0FLVp2xNe3ABs3/kpcXBwjP+4P\nwMBBoy3NamB9fWXLlpVr167jcDjYsGExffoMI3fuXKxevRGAOXM+Y8uWAGbM+MbSnH9+3+u+32Pn\noVNk9fRg0LyNLO7bKsVl4uLj6ez/M5kc7jSr8CQvlimSpvcOCj/FsqAwPnrthVvKJ/20Ha+snnSo\n/Sxfrw/hyl/R9GpUievRN8mSyYFhGBw8dYEP5qxjyYev3Pd3BMjVYlK6vM+/ldo2GBr6B76+BZj2\n5TiKlyhK5coNuHDhT0uz/idH/nR5n/KVn+Xatb8Y9/lwGta48/9jhSpl6djtTTq98c/X44KPFWDM\nZ8No3eydW8pfb9+SJ0s+wZC+o2nYzI8XG75Ar7f7U/LpEpw/e4GzZ85T7MmifL3wc6qXrv+vv9vf\njl05c9/vcb9SOxaamMTHx/PF52Po9+FHBAfvsTqq5Zyprtzd3K2OcNux8Hv69BlOiRJPULNmZd5+\n+31M0yRfvrycO3fB0pzxZryln3+3/fvfxnwymMtXrjBq1GQLk0L0jRPWn2T9S5ffrJ2u7Q6vuett\nUxf37FEzDKOCYRjlE5+XNAyjt2EYDTI+2r8XGXmWXbt+ByAq6hqhoX/waEEfGjf2Y+43iwCY+80i\nmjSpa2VMW7izrsIpWNCHdes2ExcXB0BAYAgFfQtYGdM2rl27DoCHhwMPDwemaSY10gCCgnbh6yJ1\nVbboo+TMmvmuy8zb+ju1n36cPNmz3FI+a+MuXv/0B1qOX8TUVUFp/sxf9h2lcfniADQuX5yNvx8F\nIKunR9KFjb9ibmKDaxzpJrVtEGDc2KH0HzDS5X4jJmhbCJf/vPyv/rZJi/p8v3o2yzZ+x0fjB+Dm\nlraBIXXq1+SHBcsBWPXTeipXrwDA/r1hnD1zHoA/Qg+RObMnmTJ5/KtsdpPasTA0NJyDB++/B9yV\nqK7+meTHQofDA9M06dSpNaNGTU7aX1ndSLODu+3f/9a8RSMWLlhqRTxxAnc9whmGMRSYAnxpGMZo\n4HMgG/ChYRgDH0C++1aokC9lyvwfgYEh5M/vTWTkWSBh48mf39vidPZSqJAvZZ4pRWBgyC3l7dq2\nuqUx8jBzc3MjIGAlJ06EsH79VoKCdiW95nA4eP31l1mzZpOFCR+cM5evsXHvUVpVKXVL+W9hJzh+\n/jLf9nyJBb1bcODkeXYeOpWm97xw9S/y5cwGgHeOrFy4+lfSaxv2HqHZJwvoMXMVw16pmX5fxEaS\nb4ONG/lx6lQke/cesDqWJZ4p9zTLNs5j5vwpPFEioae2aLHCNGzmx6sN36LJC68TFxdPkxZp6/16\nxCcfkREJvVxxcXFEXYkid55ctyxTr3Ft9u0JJSbmZvp+GRtIfiyUu1Nd3Zubmxvbt6/g+PFgNmzY\nQlDQLh5/vBAtWjRm69afWLJkNkWLFrY6pq2kdI5VrVpFzp45T/iho9YFcwFmvJmuDzu51800LYBn\nAE8gEvA1TfOKYRjjgQBgZAbnuy/ZsmVlwfzp9OkzjKtXo+543dWuUt+PbNmyMn+e/x111a9fD2Jj\n45g370cL09lHfHw8FSvWx8srJwsXTqdkyeLs338QgClTRrJ1ayC//hpoccoHY9yS3+jZqCJubrd2\nb20PO8m2sJO8MnExAH9F3+T4+SuULfoorSf/SExsHH9F3+Ty9WhaTfgegF4NK1LlycdueR/DMG7p\nOav19OPUevpxdh46xdRVO/Dv3Chjv+ADlnwbjI2N5YMPutOw0RtWx7LE/j2hPP9cI65f+4uadary\n5ZwJvFjxJSrXqECpMk/xw9o5AHhm9uTC+YsAfDFrPI8VehQPDw8K+PqwbON3AMyePo/F836652c+\nUaIIfQe/S/tW3TLui1nkXsdC+R/VVdrEx8dTqVIDvLxysmBBwrHQ0zMT0dHRVKvWmKZN6+HvP446\ndVpaHdUWUjvHeqVVUxYuVG/afbNZ4yo93auhFmuaZhxw3TCMQ6ZpXgEwTfMvwzCsHfh7Dw6HgwUL\npjNv/o8sWboSgLNnz+Pjk5/IyLP4+ORXt3wih8PBgvnTmT9/CUuX/u+G1jffbEmD+rWpV/9VC9PZ\n0+XLV9i0aRt+fs+zf/9BBg7shbd3Hrp1+9DqaA/M/pPn6Dd3HQCXrt1ga+hx3N0MTOCt2s/SonLJ\nO/7mm54vAanfo5Y3RxbOXblGvpzZOHfl2h1DKiFhSObJ+b/wZ9Rf5E7hdWd0+zZYqtSTFC78GEFB\nqwHwLViA7dtXUq1aY86cOWdx2owXFXUt6fmmdb8ybMyH5M6TC8Mw+HHBciZ8/Pkdf9OtXR8g9XvU\nzkSew6fgI0SePou7uzvZc2bnz4uXAPApkJ+ps8fTt/sQjh89mYHf7MFL6VgoKVNd/XMJx8Lf8PN7\nnoiI0yxZknAOsXTpKvz9x1mczh5SO8dyd3enadN6VK5i67uJxGL3GtwfYxhG1sTnZf8uNAzDC7B1\nQ226/3hCQ8OZPHlGUtlPy9fyZuuEqztvtm7JTz+tsSqerfj7jyM09A8mT/lfXfm9+Dzv9+5M8xYd\n+OuvGxamsw9v7zxJs19mzuxJ7drVCQs7RPv2r1KnTg3atOn+UPXSrhj4OisHvcHKQW9Qp3QRBrxc\nnVpPP07lEr4sCQzjenTC8LEzl69xMdkQxrupWaoQPwUl9FD+FHSQ50sVBuD4+ctJdXvg5DliYuPI\nle3u9885k9u3wX37QnnsP89SokQVSpSowsmI01SqVP+haKQBeOfPm/S89LOlcHNz48+Ll9i2OZB6\njWuTxzthBkyvXDl51Ncntbe5xfpVm3j5lYRe2HqNa7N9a8K9kzlyZmf6d5MZ/9FnBAfuTudvYr2U\njoWSMtVV2qR8LAznp5/WULNmZQCqV69EePgRK2PaRkrnWAC1a1Un7OAhIiIiLUrmQuLT+WEj9+pR\nq2GaZjSAad4ydY4H0DbDUt2nKlXK07p1C/buPUBQYMIV6cFDxjBu3Od899002rV/lePHT/L6610s\nTmq9KlXK0/qNhLr6e3rYIUPGMHHiCDJ5ZmLFzwnDhwIDg+neY4CVUS3n45OfmTMn4u7ujpubG4sX\nL2flyvVERR3m+PEINm1aAiRcSbR69qb08OHcdew4dJpL127gN+IbutQtR2xcwm6gZZU7e8v+VqXE\nYxw5c4k2UxLqI6ung5Gv1yJPjnv3fnWo9SwfzFnLj4GhPJo7B2Pb1AFg/Z4j/LTjIA53NzJ7uDP2\nzTq2mDU1PaS2Da5y4ftCJ/mPpELVcuTOk4stu1cweaw/Hok/azFv9mLqNa7N6+1aEBsbR/SNaHp1\nSph5NvzgESaNnsqsRV9gGG7ExsYyvN8nnDp57xOdRd8uZfzUj1gXuIRLf17mvU4J+7M3O75Coccf\no3uft+neJ+FnANq17MbF89bOsJkeUjsWembKxKRJH5EvXx6WLpnN7j37aNSotcVpraW6Sjsfn/zM\nmDERd3e3ZMfCDfz22w7++9/J9OjxFteuXadLl35WR7Xc3fbvLVs10SQick8uOT2/s3CVE80HxQ7T\n8zuL9Jie/2Fh9fT8ziS9pud/GNhhen5xTXaYnt9ZWD09vzNx5un5/2z5fLq2O3Iv+sU2deEav8wr\nIiIiIiIPHxduj6ftB2hERERERETkgVFDTUREREREnNKD+B01wzC+NgzjrGEYvycry2MYxlrDMP5I\n/G/uxHLDMIwphmGEG4axxzCM55L9TdvE5f8wDOOe832ooSYiIiIiIs7pwcz6OAuod1vZh8B60zSL\nAesT/w1QHyiW+OgEfAkJDTtgKFARqAAM/btxlxo11ERERERERFJhmuZm4OJtxU2B2YnPZwPNkpXP\nMRNsB3IZhlEAqAusNU3zommafwJrubPxdwtNJiIiIiIiIk7Jwsk9HzFN83Ti80jgkcTnBYETyZY7\nmViWWnmq1FATERERERHnZINZH03TNA3DSPefJ9PQRxERERERkX/mTOKQRhL/ezaxPAJ4LNlyvoll\nqZWnSg01ERERERFxSmZ8+j7+gWXA3zM3tgWWJitvkzj7YyXgcuIQydWAn2EYuRMnEfFLLEuVhj6K\niIiIiIhzegBDHw3DmAc8D3gbhnGShNkbPwEWGobxFnAMaJW4+AqgARAOXAfaA5imedEwjI+AoMTl\nRpimefsEJbdQQ01ERERERCQVpmm+lspLtVNY1gS6pfI+XwNfp/Vz1VATERERERGnZOGsjxlODTUR\nEREREXFKrtxQ02QiIiIiIiIiNqMeNRERERERcUqu3KOmhpqFHG7uVkdwKrHxcVZHcBq5WkyyOoLT\niDq2zuoITiPrY7WsjuA03LV/T7M47dv/kYR5CiQtDAyrI8iDYLru/2cNfRQREREREbEZ9aiJiIiI\niIhT0tBHERERERERmzHjNfRRREREREREHhD1qImIiIiIiFPS0EcRERERERGbMTXro4iIiIiIiDwo\n6lETERERERGn5MpDH9WjJiIiIiIiYjPqURMREREREafkytPzq6EmIiIiIiJOyTStTpBxNPRRRERE\nRETEZtSjJiIiIiIiTklDH0VERERERGzGlRtqLjn0cbr/eE6e2EVI8LqkstJPP8XmTUsJ3rmOH3/4\nLzlyZLcwof24ubnx27af+X7xVwD4+49n3/4tbNu+gm3bV1C6dEmLE1rP17cAq1cvYFfIekKC19G9\nW4ek17p2acee3RsJCV7HqJEDLExpD3erK4BePTsRfeMEefPmtihh+hv0yWRqNHmTZm27p7pMYMhe\nmnfoSdM23WjXo/99f2ZMzE3eHzqW+q914rV3+hBx+gwAe/cfpHmHnjTv0JOX27/Lus3b7vuz7GLG\n9AlEnNxNSMj6O17r1esdbsZEuNR6dT8StsP5hISsJzh4Hd0St8Onn36KX375kR071rB48dc6HpLy\netW8eSN27dpA9I0TlH2utIXp7CW19ap06ZJs2rSEgICV/PrrcsqVK2NxUut5enqyZcsyAgNXERy8\njsGDewNQuPBjbN68lH37NjN37hd4eHhYnFTsyiUbanPmLqJR49a3lE2bNo6Bg0bzXNk6LFm6ivd7\nd7YonT1169aesNDwW8oGDhhF5UoNqFypAXv27LcomX3ExsbRr99HPPNsbarXaErnzm158sli1KxZ\nmcaN/ShXvi7PPleHSZ/6Wx3VcqnVFSQc5OvUqcGx4yctTpm+mtWrzbRxw1J9/crVKD6eOI3PRw9i\n6ZwvmDCiX5rfO+L0Gdq9e+cFgB9+XkvOHNlZOW86b7ZqwsRpswF4okghFkyfyOKvJ+M/bhgjxk8l\nNjbuH38nO5o9ZyGNGr1xR7mv76O8WKcGx4651np1PxK2w4959tna1KjRlM6d2/Dkk8X48suxDB78\nCeXK+bFs2Sp6937H6qiWS2m92rcvlFat3mbLlu0WpbKn1NarUaMGMHLkp1SsWJ8RIyYwapQuWkZH\nR1Ov3qtUqFCPChXq8eKLNalQ4Vk+/rg/n302k1KlanDp0mXatXvF6qhOzTTT92EnLtlQ27o1gD//\nvHRLWbFiRZJ2tuvXb+allxpYEc2WHi3oQ716tZg1a77VUWwtMvIsu3b9DkBU1DVCQ8MpWNCHTm+/\nybjxU4mJiQHg3LkLVsa0hdTqCmDc2KH0HzAS0257w/tU7pn/wytn6j0TK9Ztpk6NyhR4JB8AeXPn\nSnrtpzUbebXT+zTv0JPh474gLi5tjaoNWwNoWq8WAH41qxIQvBvTNMmS2ROHwx2A6JgYcKFRIVu3\nBnDxtv07wPjxw1xyvbofqW2HxYo9zpYtAQCsX7+FZs10PExpvQoNDefgwUMWJbKv1NYr0zTJmTMH\nAF5eOTid2MP/sLt27ToAHh4OPDwcmKbJ889X4YcfVgDwzTff06RJXSsjOj0z3kjXh53844aaYRhz\nMiJIRtu//2DShtC8eSN8fR+1OJF9jB07hIGDRhMff+sJztBhfQgIWMmYMYPJlCmTRensqVAhX8o8\nU4rAwBCKFStC1aoV2LJ5GWvXLqJsWQ33SC55XTVu5MepU5Hs3XvA6lgP3NETEVy5GkW7dwfQquN7\nLF21AYBDR0+wasNW5k4dw+KvJ+Pm7sbytZvS9J5nz1/AJ783AA6HO9mzZePS5asA7NkfRtM23Xip\n/bsMeb9rUsPNFTVu7MepiNPq+b+LQoV8eSZxO9y//yCNG/sB8PLLDfH1LWBxOnFWyderPn2GM3r0\nAMLDtzN69CAGDx5jdTxbcHNzIyBgJSdOhLB+/VYOHz7G5ctXki7IRUSc5tFHfSxOKXZ118lEDMNY\ndnsR8IJhGLkATNNsklHB0lund95n4sQRDOjfk+XL1xITc9PqSLZQr34tzp27wK6Q36levVJS+dCh\nY4iMPEemTJn4/PPR9H6/M5+MnmJhUvvIli0r8+f506fPMK5ejcLhcJAndy6q12hCuXLP8N23Uynx\nZFWrY9pC8rqKjY3lgw+60zCFYWsPg7i4OPYfDGfmpI+Jjo7hjS59KVOqBAE7d7M/7BCvdnofgOjo\nGPLk8gLg3YGjiDh9hps3Yzl99hzNO/QEoHWLxrzUoM5dP690yRIsnfMFh46eYOCoT6lesSyenq53\nwSVLlsx82K8H9Ru8bnUU28qWLSvz5vnTp89wrl6N4p13+jJx4nD69+/Jzz/reCj/zu3rVadOb9K3\n7wiWLFlJ8+aNmDZtHA20XRIfH0/FivXx8srJwoXTKVHiCasjuRzTtFcvWHq616yPvsB+YCZgktBQ\nKwdMyOBc6S4s7BANGyacIBYr9jj169e2OJE9VK5UjoYN61C37gtkzuxJjhzZ+eqrSbz11nsAxMTE\nMHfuInr2etvipPbgcDhYMH868+cvYenSVUDC1bAlS1cCsGPHLuLjTby983D+/EUro1ru9roqVepJ\nChd+jKCg1QD4FizA9u0rqVatMWfOnLM4bcZ7JJ83Xl45yZolM1mzZKZsmVKEhR/BBJrUe4H33ml7\nx99MSZyYJuL0GQaOnsysKaNueT2/d14iz57HJ783sbFxRF27Ri6vHLcsU7TwY2TNkpk/jhzj/xLv\nE3QlRYsWpnDh/7Bzx1og4R7IwIDVVKna8KFYr+7F4XAwf74/8+f/mLTPOnjwEI0aJdzH/cQTj1Mv\ncfisSFqltF61bt2c998fCsDixcv58kv1qCV3+fIVNm3aRsWKz+HllRN3d3fi4uIoWLAAp05FWh3P\nqZnxVifIOPca+lgO2AkMBC6bpvkL8JdpmptM00zb2BybyJcvLwCGYdD/w55MnzHX4kT2MHToWIoX\nq0zJp6rRtk0PNm36jbfeeg8fn3xJyzRu7Mf+fQctTGkf/v7jCA39g8lTZiSVLVu2mpo1qwBQ7InH\n8cjk8dA30uDOutq3L5TH/vMsJUpUoUSJKpyMOE2lSvUfmpPpF6pVJGTPfmJj4/jrRjR7DxykSKHH\nqFS2NGt/+Y0LiffHXL5ylVORZ9P2nlUrJA2hXLPpVyo+VxrDMDh5KjJp8pBTkWc5cjyCgj6PZMwX\ns9jvv4dS0LcMxYpXoljxSpw8eZoKFes+NOvVvSRsh+FMmTIzqeyW42H/d5k58xur4omTSmm9On36\nDDVqJIzMeeGFqoSHLBjg9AAAIABJREFUH7UonX14e+fByysnAJkze1K7dnVCQ8PZtGkbL7+ccG9o\n69Yt+OmnNVbGFBu7a4+aaZrxwCTDMBYl/vfMvf7GDubO+ZwaNSrj7Z2Hw4eCGPHRBLJnz0aXzglX\nrJcsWcns2QssTmlvX389GW/vPBiGwZ49+3n33YFWR7JclSrlaf1GC/buPUBgQMIVxCFDxjBr9gKm\nTx9P8M51xMTE0LHjexYntV5qdbVq9UaLk2WcvsPHERTyO5cuX6F28/Z0bf8asYn3ILzStD5FCz9G\n1YrP8XL7d3FzM2je8EWKFSkEQI+Oren0/lDi4+PxcDgY+N47POqT/56f+XLDF+k/ciL1X+uEV44c\njBvWF4DgvQf46tuPcDgcuBkGg3p3JneunBn35R+guXO/oGbi/v3I4R2MGDGe/2oipBRV+X/27js8\niupt4/j3JBsCSUgQaUr8IVIUC1hAkC69KIgUG4oK0lUQRAQpooI0KSJKbyKhSpWO0kkoCb2DQEIL\nRaoQksz7R5a8oCAtYWaX++O1F5uZ2Z17x9ndOXPOPFu0EG+9VZONG7cSHp7U69+xYw9y585J48bv\nADB16hxGjZpgZ0xHuNZ+deLkX/Tt8zWZM2dk2rTRrF+/+Z4dun2l6+1XTZu2pVevzrhcvly4cJFm\nzdranNR+2bJlYejQ7/D19cXHx4fJk2cye/ZCtm3byejRA+jc+VOiojYzcqSOSe9EohcPfTS3UiHL\nGFMVKGZZ1n/WXE3jH6qyWzfBz9fxbV5HiU/0jvLi4ixn9y248UICQMBDGiJ3s3x9vLd4S0pL0Gf7\nLdG+JanhwoX9Htva2f5Y5RRtdzy6bbZjtsUttRQsy5oFzEqlLCIiIiIiIoIHDGMUERERERG5Fqf9\n9llKUkNNREREREQ80i1cxeVxbvkHr0VERERERCR1qUdNREREREQ8koY+ioiIiIiIOIw3l+fX0EcR\nERERERGHUY+aiIiIiIh4JMuLe9TUUBMREREREY+kqo8iIiIiIiJy16hHTUREREREPJKKiYiIiIiI\niMhdo4aaiIiIiIh4JMsyKXq7FmNMS2PMZmPMJmPMOGNMWmNMTmNMuDFmlzFmvDEmjXtZf/ffu9zz\nH77d16aGmoiIiIiIeCTLStnbPxljsgMfAQUty3oS8AVeB7oDfSzLyg2cBOq7H1IfOOme3se93G1R\nQ01EREREROT6XEA6Y4wLCAAOAWWASe75o4BX3Peru//GPb+sMea2LqRLlWIit5nlnhMXf8nuCB7F\ni6uvprhg/wC7I3iMgIfK2B3BY5yJGGR3BI/xQPEWdkfwGGfi/rY7gkdx+fjaHcFjxCcm2B1B7oLU\nLiZiWVaMMaYXsB/4G5gHrAX+siwr3r1YNJDdfT87cMD92HhjzCngfuDYra5bPWoiIiIiIuKRUvsa\nNWPMfST1kuUEHgQCgUp347WpoSYiIiIiInJt5YC9lmXFWpZ1CZgCFAMyuIdCAoQCMe77McBDAO75\nIcDx21mxGmoiIiIiIuKREi2Tordr2A8UMcYEuK81KwtsAX4HarmXqQdMc9+f7v4b9/xFlnWtMiU3\nph+8FhERERERj5TaNQwsywo3xkwC1gHxQCQwGJgFhBljvnZPG+Z+yDBgjDFmF3CCpAqRt0UNNRER\nERERkeuwLKsT0Okfk/cAz19j2QtA7ZRYrxpqIiIiIiLikVK76qOd1FATERERERGPdK1Kjd5CxURE\nREREREQcRj1qIiIiIiLikRLtDpCK1FATERERERGPZKGhjyIiIiIiInKXqEdNREREREQ8UmJq/5Ca\njdRQExERERERj5SooY8iIiIiIiJyt3hlQy009AHmzh1PVORCItctoHmz9wH44ouW7Nm9mojwOUSE\nz6FSxRdtTmq/IYN7ExO9nsjIhcnTOnT4hD/3rmHN6nmsWT2PSpXK2JjQmfLmzZW8fdasnseJY9v4\n6MMGdsdyjNx5crJ4+fTk276YSBo3fZfPPv+QTduXJk8vV6GU3VEd4Vrvw8tatGjEpbgY7r//PhuS\npY6OP4ZR+oNOvNqq5zXnr968i2LvtqdOm97UadObnybNu+N1xl2K59O+o3npo6681b4fMUdPALBx\n1/7k9dT+tBcLIzbe8bqcIneenCxZMT35tu9gFI2bvkv1GpVZsXo2x0/v4OlnnrQ7piOFhAQzPmww\nmzYuZuOGPyhS+Dm7IznK5q1LCY+YzYpVs1iybBoANWpUYfWauZw+u5tnnn3K5oTOkHQ8GkZk5ELW\nrVtAM/fxaP78j7N48VTCw2ezfPlMChYsYHNSz2ZhUvTmJMayUn5gp3/ah2wdLZotWxayZctCVNQm\ngoICWbXyN2rVbkCtWi9x7ux5+vQdZGe8ZImJ9hcULV68MOfOnmP4iH4880xZIKmhdvbsOfr0ccZ2\nusypQ5B9fHzY/+daihZ/if37Y+yOA0Cwf4DdEZL5+Piweccyyr9Yi7fq1uTcufMM6D/M7ljJzlw8\nb3eEa74PAUJDH2TQTz159NHcFC5SiePHT9qYEs5EpMxnwtotuwlI60/7H8Yxpfen/5q/evMuRs38\ngwGf3frJj5ijJ+j4YxjDOjW9avr4ucvZsf8QHT6oxezlkSxavZGeLd7h74tx+Ll8cfn6EnvyNLXb\n9GbBTx1x+fre9usDeKB4izt6fErz8fFhy87llC9dk3QB6UhMTKRP/6/p0K4bUZGbbM12Ju5vW9d/\nLcOH9WXZsnCGjxiHn58fAQHpOHXqtN2xAEjrSmN3BDZvXUrJ4tWu+kx69NFcJCZa9P/+G9q160rk\nOvtPesQnJti6/n8ej65cOYvatT+gV69O9O8/lHnz/qBixRdp1aoxFSq8ZmvWCxf2O6uFcgvmZ30t\nRQ8Ryx8Z75ht4ZXXqB0+fJTDh48CcPbsObZt20X27NlsTuVMy5aFkyNHqN0xPFrZMsXZs2efYxpp\nTlOqdFH+3Luf6AMH7Y7iWNd7H/bq1ZnP233D5EnDbUiVep57PFdyj9atmrl0Lb/MXkp8fAJP5v4f\n7RvUxNfnxoNDfl+ziSa1KwJQvkh+vh0xBcuySOf//we9Fy9dwjjm6zlllSpdlD/37OeA3oc3FByc\nnhLFC/N+/aTG9qVLlzh16pLNqZxv+/bddkdwnOsdj1qWRXBwegBCQtJz6NARO2OKg93S0EdjTHFj\nzCfGmAqpFSil5cgRSoGnnyAiIhKAxk3qsWb1PAYN6kWGDCE2p3Oupk3eY93a+QwZ3Fvb6Qbq1KlO\n2PipdsdwrFdrVWXyxJnJfzdoWJelK2fw/cBuhGQItjGZs738cgUOxhxiw4YtdkexxYYd+6j9aS+a\ndhvCrgOHAdgTfYS5K6IY1eVDJvRoha+PD78tXXdTz3f0xGmy3Z8BAJevL0EB6fjrzLmkde3cR41W\nPajVuhdfNKh1x71pTvRqrapMnjTzxgsKOXP+j2PHjjNsaB9WR8xl0E89CQhIZ3csR7Esi2kzRrN0\n+XTee/8Nu+N4hBw5QnnafTzauvWXdOvWjl27VtGt2xd06NDd7ngezZuHPv5nQ80YE3HF/Q+AAUB6\noJMxpm0qZ7tjgYEBhI0bROvWnTlz5iyDB48hX77iFHq+IocPH6V79w52R3SkQYNG8+hjRXmuYAUO\nHT5Kzx4d7Y7kWH5+frz8UgUmTdYB0LX4+flRqUoZpv06G4DhQ3/h2fxlKVm0GocPH+Xrrp/bnNCZ\n0qVLS9vPPqTzl73sjmKLfDlDmfPDF0zs2Zo3KhWnZa8RAIRv2snWvdG81a4vddr0JnzTTqKPHgeg\nRa8R1GnTm+bfDmXz7gPJ151N/T3iv1YFQP48Ofi1dxt+6dqCYVMXcjHOu3pP/Pz8qFy1LFN//c3u\nKB7B5evLM888xaBBoyn0fEXOnTvPZ22a2x3LUcqXq03xoi/z6ivv0bDh2xQr9rzdkRwtMDCAceMG\n0br1l5w5c5aGDd/m00+7kDt3Edq06cJPP137el25OYkpfHOSGw199LvifkOgvGVZscaYXsAq4NtU\nS3aHXC4X48MGExY2lWnT5gBw9Oix5PnDh//Cr1NG2pTO2a7cTsOGjWXq1FE2pnG2SpVeJDJy41Xb\nTP5fuQol2RC1hdjYpIPpy/8CjB45gbCJg+2K5mi5cj3Mww//j7Vr5gNJF6RHhM+laLGqHDkSa3O6\n1BcUkDb5foln8tF12GROnj6LZVm8XLIgH79Z9V+P6dv6PeD616hlyRjM4eN/kfX+DMQnJHD2/N9k\nSB941TKPhGYlIK0/uw4c5olcD6XCK7NHuQqlWB+1hdijx2+8sBAdc4jo6ENErE4aiTNlyizafKqG\n2pUOHUwaqhcbe5wZM+byXMECLF9+45Mi9yKXy0VY2CDCwn5NPh6tW7cmrVp1AmDy5Jn8+KN61OTa\nbjT00ccYc58x5n6SCo/EAliWdQ6IT/V0d2DQoJ5s27aTfv2HJE/Lli1L8v3q1SqxefN2O6I53pXb\n6ZXqlbWd/sPrr72iYY//oWatl64abpU1a+bk+y+9XJ6tW3bYEcvxNm3aRvbQAuTJW4Q8eYsQHX2I\n5wtXvCcaaQDH/jrN5UJXG3ftJzHRIkP6QAo/lYcF4Rs4fuoMAKfOnudg7M1d61a64BNMX7wGgPmr\nNvD8E3kwxhB99DjxCUkFBw7GnuDPg0d5MLP3VNgEqFX7JSZPnGF3DI9x5Egs0dEHyZs3FwBlyhRn\n61Z9Vl0WEJCOoKDA5PtlypZgyxYdJ1xP0vHoLvr3H5o87dChI5QsWQSAF18sxq5df9qUzjvcyz1q\nIcBawACWMeYBy7IOGWOC3NMcqWjRQtR9qxYbN24lIjzp7EXHjt2p81p1CuR/Asuy2LcvmmbNHT96\nM9WNGfMDpUq+QKZMGdm7Zw1duvSiVKmiFCjwOJZl8ee+aJo2/czumI4UEJCOcmVL0kTb55oCAtJR\nukwxWn78/0OMO3/Vhqfy58OyLPbvj+GTjzT8GK79PhwxMszuWKnms35jWLNlN3+dOUf5Jl1oUrti\ncmOpTvmizF+1gQnzV+Dy8cE/jR/dP66LMYZcodlo9lolmnwzmETLwuXrS7v3X+XBzBlvuM4aLxam\n/YBfeOmjrgQHBdDj47cBiNy2l+HTFuHn64sxhnb1X+W+4KBUff13U0BAOkq/WIyWH32RPK3qy+Xp\n3qsTmTJlZPzkoWzcsJVar7xnY0rn+bhlB0aP+p40afzYu3c/9Rt8Ynckx8iSJRPjwpIqwLpcvkyY\nMJ0F85fwcrUK9OrdmUyZMjJ58nA2bNjCK9Xr2ZzWXkWLFuKtt2qyceNWwsOTLgHo2LEHTZu2pVev\nzrhcvly4cJFmzXQ8Ktd2W+X5jTEBQFbLsvZea77d5fk9hRPK83sS7VQ3z0nl+Z3OCeX5PUVKlee/\nFzitPL+TObE8v5M5oTy/p7C7PL8n8eTy/LOyvpGih4hVj4xzzLa4rfL8lmWdB67ZSBMREREREbkb\nEh3TrEp5t1SeX0RERERERFKfV/7gtYiIiIiIeL9E55bNuGNqqImIiIiIiEfy5hoGGvooIiIiIiLi\nMOpRExERERERj+TNNdTVUBMREREREY+UaLz3GjUNfRQREREREXEY9aiJiIiIiIhH8uZiImqoiYiI\niIiIR/Lma9Q09FFERERERMRh1KMmIiIiIiIeKdF7a4mooSYiIiIiIp4pEe9tqWnoo4iIiIiIiMOo\nR01ERERERDySqj7eIj8ftf9uxsXEOLsjeBQfL/5Bw5R2/tJFuyN4DJevPq9uVo5Sre2O4DEOr/jB\n7ggeI7Dg+3ZH8Chp9Jl10+ISLtkdQe4Cb75GTUMfRUREREREHEanZURERERExCN58++oqaEmIiIi\nIiIeyZuvUdPQRxEREREREYdRj5qIiIiIiHgkby4mooaaiIiIiIh4JG++Rk1DH0VERERERK7DGJPB\nGDPJGLPNGLPVGPOCMSajMWa+MWan+9/73MsaY0x/Y8wuY8wGY8yzt7teNdRERERERMQjJabw7Tr6\nAXMsy3oMKABsBdoCCy3LygMsdP8NUBnI4741BH683demhpqIiIiIiHgky6Ts7Z+MMSFASWAYgGVZ\ncZZl/QVUB0a5FxsFvOK+Xx0YbSVZBWQwxjxwO69NDTUREREREZFrywnEAiOMMZHGmKHGmEAgq2VZ\nh9zLHAayuu9nBw5c8fho97RbpoaaiIiIiIh4pLsw9NEFPAv8aFnWM8A5/n+YIwCWZVmkwk+6qaEm\nIiIiIiJybdFAtGVZ4e6/J5HUcDtyeUij+9+j7vkxwENXPD7UPe2WqaEmIiIiIiIeKbV71CzLOgwc\nMMY86p5UFtgCTAfquafVA6a5708H3nFXfywCnLpiiOQt0e+oiYiIiIiIR0rx8YbX9iEw1hiTBtgD\nvEdSh9cEY0x9YB9Qx73sb0AVYBdw3r3sbVFDTURERERE5Dosy4oCCl5jVtlrLGsBzVJivV479HHz\n1qWER8xmxapZLFmW1BNZo0YVVq+Zy+mzu3nm2adsTugMQwb3JiZ6PZGRC/81r0WLRlyKi+H++++z\nIZnzDB7Ui+gDUUSuW5A8Lf9T+ViyeBrr1i7g1ykjSJ8+yMaEzuHv78/SpdOJiJjDunUL6NDhEwBG\njuzHhg2/s3btfAYN6onLpXNFSdtqGuHhs1m7dj5ffNEyeV7nzp+yYcPvREYupGnTd+0L6SAfNH6b\nxSuns3jVDBo2eeeqeY2bv8eRU9vImDGDTelSXseBYylVvx01Pul2zfmrN++k6DttqN26O7Vbd+en\nibPveJ1xly7x6XcjqNq8C29+3puYo8cB2LhzX/J6arX+loXh6+94XU7k7+/PyuUzWbtmPuujFtGp\nYyu7IzlK7jw5Wbx8evJtX0wkjd2fTx80eptVa+ewIuI3On/Vxt6gDnGtY4exPw9kdcRcVkfMZcf2\nlayOmGtjQs+XaFL25iRefZRUpfKbHD9+MvnvLVu28+YbTej//Tc2pnKWUaMnMHDgCIaP6HfV9NDQ\nBylfriT79kXblMx5Ro+ZyMAfRzJieN/kaT/91JPP2n7N0qWrqFfvNVp90pjOX/ayMaUzXLx4kUqV\nXufcufO4XC4WLZrM3Lm/M27cVN5992MARo/+nvfee50hQ362Oa29krbVG1dsq0nMm/cHjz6am9DQ\nByhQoAyWZZE58/12R7XdY/nyULdebSqVqUNc3CXCpgxh3tw/+HPPfh7Mno3SZYpxYP9tXa/tWNVK\nF+b1SiVpP+D675Nn8+ViwOeNbvm5Y44ep8MPYxn+5UdXTZ+yaBXBQQHMGtCR2cvX0vfn6fT85D1y\n/+8BxnVvjcvXl9iTp6jVujulCj6Jy9f3ltftZBcvXqRchTrJ78klf/zKnDm/Ex6xzu5ojrBr515K\nFasGgI+PD5t3LGPmjHkUL1GYylXLUvKFasTFxZEpU0abkzrDtY4d3qrbNPl+9+4dOH3qjB3RvMZ/\n/Ei1x/vPHjVjTGFjTLD7fjpjzJfGmBnGmO7uH3/zKNu372bnzj12x3CUZcvCOXHyr39N79WrM5+3\n+4ak3luBpG118h/bKk+eR1i6dBUACxcuoUaNKnZEc6Rz584D4Ofnws/PhWVZzJ37e/L81aujCA29\nrd9/9DpXbiuXyw/LsmjYsC5du/ZLfg/Gxh63M6Ij5Hn0Edat3cDff18gISGBFctWU/Xl8gB06fY5\nXTr2xNs+sgo+npuQoIDbeuzMJat5s20varfuTpdBYSQk3NzhzB+rN1Kt1PMAlC/yNOGbdmBZFun8\n0yQ3yi7GxWOMw049p6Cr3pN+fvouvI5SpYvy5979RB84yPsN3qTfd4OJi4sD4NixEzanc4ZrHTtc\nqVbNlxk/Ydp158u97UZDH4eTdBEcQD8gBOjunjYiFXPdMcuymDZjNEuXT+e999+wO45HefnlChyM\nOcSGDVvsjuJ4W7bsoFq1igDUrPkSoaEP2pzIOXx8fAgPn82BA5EsXLiM1aujkue5XC7efPNV5s1b\nbGNC5/Dx8WHVqt/Yv38dixYtZfXqKHLmzEGtWi+zbNkMpk4dRa5cD9sd03bbtuyk8AsFue++DKRL\nl5ZyFUqRPfsDVKpShsMHj7Bl03a7I9pi/Y691Gr9LU2++ZFdB5IKi+2JPsycFesY9XVLJvb6DB8f\nH2YtW3NTz3fkxCmyZkoaPury9SUoIC1/nTkHwIadf1KjZVdqtupGhw/qeF1v2mU+Pj6sWT2PQzEb\nWLhwCRGrI+2O5Eiv1qrK5IkzAciVOycvFC3I/EWTmDF7rC4xuQnFixfm6NFYdu3aa3cUj3YXfkfN\nNjca+uhjWVa8+35By7Kedd9fZoyJut6DnKB8udocOniEzJnvZ/qMMezYvpvlyyPsjuV46dKlpe1n\nH1K5ypt2R/EIDRu14rvvutDu84+ZOXM+cXGX7I7kGImJiRQuXJmQkGAmTBjM44/nZcuWHQD07/8N\ny5ZF6D3plpiYSJEiVQgJCWb8+KRt5e+fhosXL1K8+MtUr16JQYN6Uq5cbbuj2mrnjj0M6DuE8VOH\ncf7ceTZt3Eoa/zR83KoRdWrUtzueLfLlDGXuwC8JSOfP0nWbadFjKDO/70D4xh1s3XOAN9smDcW+\nEHeJjCFJ19C26DGUmKPHuRQfz6FjJ6ndujsAb1UtxSsvFvnP9eXP8zC/9mnHnujDfDHgZ4o/8zj+\nafxS90XaIDExkYKFKhASEszkicN44olH2bz53jwRcD1+fn5UqlKGLp2S9jGXy5cM94VQvkwtnn0u\nP8NH9eOZp8rYnNLZXnutunrTUoA393ffqKG2yRjznmVZI4D1xpiClmWtMcbkBRx9RHro4BEgabjQ\njBlzea5gAR0U3oRcuR7m4Yf/x9o18wEIDX2AiPC5FC1WlSNHYm1O5zzbt++matW3AMiTJyeVK/+r\n+M8979Sp0yxevJIKFUqzZcsO2rdvQaZMGWnWrK3d0RwnaVutoEKF0sTEHGLq1DkATJs2h0GDetqc\nzhl+GTOZX8ZMBqBdx5bEHj1G5aplWeQuGvVg9qzMXzKFSmXqEHv0mJ1R74qggHTJ90s8+wTfDJ3I\nydNnsbCoVup5Pn6r2r8e07dNA+D616hlzRjCkWN/ke3++4hPSODs+QtkSB941TKPhGYjXVp/dh04\nxBO5/pcKr8wZTp06zR+Ll1OxQmk11P6hXIWSbIjakjws+2DMYWZOnwfAurUbSEy0uD9TRo5rCOQ1\n+fr68kr1yhR5QZdMyPXdaOhjA6CUMWY38Diw0hizBxjinudIAQHpCAoKTL5fpmwJtmzRB+zN2LRp\nG9lDC5AnbxHy5C1CdPQhni9cUY2067hc4MEYw+dtP2bwkDE2J3KGTJkyEhISDEDatP6ULVuC7dt3\n8957r1OuXEneeae5rvlwu/a22sWMGfMoVeoFAEqUKKKhMW6XCxRkD32AKi+XZ/y4qTyRuxiF8pel\nUP6yHIw5QvmSr94TjTSAYydPJ7+XNu7cR2KiRYb0gRR+Mi/zV63nuLtIwakz5zgYe3MHzKULPsn0\nxUknNueviuL5J/NgjCH6yHHiExIAOBh7gj8PHuHBzN5XMOLq92RaypUtyfbtu21O5Tw1a73E5Ekz\nk/+eNXMBJUom9cjmyv0wadL4qZH2Hy5/L8bE3NbvIMsV7tmqj5ZlnQLedRcUyelePtqyrCN3I9zt\nypIlE+PCBgFJXfETJkxnwfwlvFytAr16dyZTpoxMnjycDRu28Er1ejd4Nu82ZswPlCr5ApkyZWTv\nnjV06dKLESPD7I7lSGNGD6Cke1vt2b2aLl/1JigokCaNk/ahqVNnM2rUeJtTOkO2bFkYOvQ7fH19\n8fHxYfLkmcyevZCzZ/ewf38MixdPBZJ6irp27XeDZ/Nu2bJlYciQ7/D19bliWy1ixYo1jBjRjw8/\nrM+5c+dp0uQzu6M6wrAx/bkvYwbiL8XzeesuXl8trU3fkazZvIu/zpylXKMONK1TJbmxVKdCceav\nimLCvGX4+vrgn8aPHi3rYYwh10MP0Pz1qjT+aiCJloXL14d2DWrfVMOqRpkXaPf9GKo270JIUAA9\nWr4LQOS23QyfugCXry/Gx9C+QR3uC/a+nyR54IGsDB/WN/k9OWnSDGb9tuDGD7yHBASko3SZYrT8\nuEPytLFjJvH9wG4sD59FXNwlmjZSeX649rHDyJFh1KldjfETptodzys47bqylGRS46x2UEBOnSq/\nCRfj4+yO4FG8ucJYSvMxXvsTiSlO+9XNC06T7sYLCQD7l97bJyBuRWDB9+2O4FGC/W+vCui96Gzc\n33ZH8BhxF6M99svw2xx1U7Td0Xbfz47ZFl79O2oiIiIiIuK9vLl3SA01ERERERHxSIle3FTT+CgR\nERERERGHUY+aiIiIiIh4JG8uJqKGmoiIiIiIeCTvHfiooY8iIiIiIiKOox41ERERERHxSBr6KCIi\nIiIi4jCJjvnVs5SnoY8iIiIiIiIOox41ERERERHxSN78O2pqqImIiIiIiEfy3maahj6KiIiIiIg4\njnrURERERETEI3lz1Uf1qImIiIiIiDhMqvSoXUqMT42n9TpBadLZHcGjePPFoiktPjHB7gge41KC\nPq9u1om/z9gdwWOkL1Tf7gge49zWyXZH8CiB+WraHcFj+Bgvrtsuybz5+FBDH0VERERExCN5bzNN\nQx9FREREREQcRz1qIiIiIiLikby5mIgaaiIiIiIi4pG8+Ro1DX0UERERERFxGPWoiYiIiIiIR/Le\n/jQ11ERERERExEN58zVqGvooIiIiIiLiMOpRExERERERj2R58eBHNdRERERERMQjaeijiIiIiIiI\n3DXqURMREREREY/kzb+jpoaaiIiIiIh4JO9tpmnoo4iIiIiIiOOoR01ERERERDySNw999MoetdDQ\nB5g7dzxRkQuJXLeA5s3eT57XtMm7bFj/O5HrFtD1m3Y2pnSO4JD0jPx5AOHr5rJq7RwKPf8MTz6V\nj3mLJrFkxXR4GyxOAAAgAElEQVQWLfmVZ5/Lb3dMRwgJSc/onwewet08ItbOpdDzz9C+Q0uWr5rF\n0hUz+HXaSLJly2J3TMfw8fFhxcpZTJo8DIBGjd9hw8Y/OHf+T+6//z6b0zlDaOgDzJs7gfVRi4iK\nXEjz5vUBqPlqVaIiF3Lh7/08+6zef5cNGdybmOj1REYuTJ72bbcv2LhxMevWzmfixKGEhATbmNA5\n7rV9q2OfoZR6ozk1mlz7u331hq0UrdWY2s07ULt5B376ZeodrzPu0iU+7fYDVet/ypstviTmSCwA\nG7fvTl5PrWZfsHDFmjtel1OFhAQzPmwwmzYuZuOGPyhS+Dm7IznK4EG9iD4QReS6BcnT8j+VjyWL\np7Fu7QJ+nTKC9OmDbEzo+RJT+OYkxrJSvhXqn/YhW5u22bJlIVu2LERFbSIoKJBVK3+jVu0GZM2a\nibaffUj1V94lLi6OzJnvJzb2uG05A1z+tq37SgMH9WDlijWMGTUBPz8/0gWkZcTo7/lxwHAWzF9C\n+Qql+KhlQ16u/JatOZ1wxuTHQT1ZuWI1o93bKiAgLYmJFmfOnAWgUZN6PPZYblp+3MHWnPGJCbau\n/7IPP6zPs8/mJ31wELVq1qdAgSc4efIUc+aGUaL4yxw/ftLuiFxKiLd1/f/8vApfNZtatepjYZGY\nmMgPA7rzWduvWLdug605AVLj++JWFS9emHNnzzF8RD+eeaYsAOXKleT335eTkJBA165JB+nt2nW1\nMybGGFvXD56zb53ZMilFnmfNxm0EpEtL+96D+fXHf///X71hK6Mmz2bAl5/c8nPHHImlw3dDGd79\n86umh81cyM69B+jw4bvMXryKRSvW0vPzZvx94SJ+fi5cvr7EnvgrqbH2cz9cvr63/fouC8xX846f\nIyUNH9aXZcvCGT5inPt7MR2nTp22OxYAPg54HxYvXpizZ88xYnhfnnm2HAArls/ks7Zfs3TpKurV\ne42cDz9E5y972Zoz7mK0/RvrNn3wcO0U/XIa8ufEa24LY4wvsAaIsSzrJWNMTiAMuB9YC7xtWVac\nMcYfGA08BxwHXrMs68/byfKfPWrGmI+MMQ/dzhPb6fDho0RFbQLg7NlzbNu2i+zZs9Hwg7fp2Wsg\ncXFxALY20pwiODiIosUKMWbUBAAuXbrE6VNnsCyL9MFJZ3iCQ9Jz+NARO2M6QnBwEMWKFWL0Fdvq\n1KkzyY00gMCAdI44mHWCB7Nno1KlMowcGZY8bf36zezfH21jKuf59+fVTh7Mno1t23axY8cem9M5\nz7Jl4Zw4+ddV0xYsWEJCQtLJifDwdYRmf8COaI5zr+1bBZ96jJD0gbf12JmLlvNmi87Ubt6BLt+P\nICHh5s6r/7FqHdXKFQegfPFChK/fgmVZpEvrn9wouxh3yREN99QQHJyeEsULM3zEOODy96IzGmlO\nsWxZOCf/8ZmVJ88jLF26CoCFC5dQo0YVO6J5DSuF//sPHwNbr/i7O9DHsqzcwEmgvnt6feCke3of\n93K35UZDH78Cwo0xS40xTY0xmW93RXbJkSOUAk8/QUREJHnyPEKxYs+zdMl05s+fyHPPFbA7nu3+\nl+Mhjh07wQ8/dWfx8un0G9CVgIB0tPvsa7p83ZZN25bS5Zu2dOlk75keJ8jh3lYDf+rB0uXT+d69\nrQA6dGrF5m3LqP1adb75uq/NSZ2hR4+OtP+iG4mJarjerBw5QilQ4EkiIiLtjuKx3n33debM/d3u\nGI6jfSvJ+m27qNXsC5p06MWufUknjfbsP8icJRGM6vUFEwd8hY+PD7P+WHFTz3fk+EmyZs4IgMvX\nl6CAdPx1Ounk3YZtu6nR+HNqNm1Ph+b1UqQ3zWly5vwfx44dZ9jQPqyOmMugn3omfy/K9W3ZsoNq\n1SoCULPmS4SGPmhzIs92N4Y+GmNCgarAUPffBigDXB4SMAp4xX2/uvtv3PPLmts8W3OjhtoeIJSk\nBttzwBZjzBxjTD1jTPrbWeHdFBgYQNi4QbRu3ZkzZ87icrnIeF8GSpSsxueff8MvYwfaHdF2Lpcv\nBZ5+guFDf6FUsWqcP3+eFq0a8X6DN2nX9huefKwE7dt2pf/AbnZHtZ3L5aLA008wbOhYShSrxrnz\nf9OyVWMAvvqyN088VpyJ46fRsNHbNie1X6XKZYiNPU5U5Ca7o3iMwMAAxocNTv68klvXtu1HxMfH\n88svU+yO4ijat5Lky/0wc0d+x6QfvubNauVp8VV/AMLXb2brrj95s8WX1G7egfCoLUQfSrrWrMVX\n/ajdvAPNOn7H5p17k687mzpvyQ3Xl/+xXPz6UzfG9e3MsAkzuegezeNNXL6+PPPMUwwaNJpCz1fk\n3LnzfNamud2xHK9ho1Y0avQOq1b+RvqgIOLiLtkdSW6sL9CG/2/L3Q/8ZVnW5esnooHs7vvZgQMA\n7vmn3MvfshtVfbQsy0oE5gHzjDF+QGXgDaAX4NgeNpfLxfiwwYSFTWXatDkAxMQcYuq02QCsWRNF\nYqJFpkwZOXbshJ1RbXUw5jAHYw6zds16AKZPnUOLTxpR5IWCtP30KwCmTvmNfgPsvd7DCWJiDhFz\nxbaaNnU2LT9pfNUyE8ZPY+KU4XT7pp8dER3jhSIFqVq1HBUrvkjatP6kTx/EsGF9qF+/pd3RHMnl\ncjF+/GDGhf2a/Bklt+adt+tQtUo5KlSsY3cUR9G+9f+CrujpKVGoAN/8MJqTp85gWVCtbDE+fu/f\n+07fDh8D179GLev993Ek9gTZMmUkPiGBs+f/JkPw1YUhHvnfg6RLm5Zdf8bwRN6cqfDK7BMdc4jo\n6ENErE7qqZ0yZRZtPlVD7Ua2b99N1apJ1/3nyZOTypXL2pzIs91guOIdM8a8BBy1LGutMaZ0qq7s\nH27Uo3ZVN51lWZcsy5puWdYbQI7Ui3XnBg3qybZtO+nXf0jytOnT51KqVFEA8uTOiV8av3u6kQZw\n9OgxYmIOkTtP0pdHydJF2b5tF4cOH6FYicLuaS+wZ/efNqZ0hn9uq1LubfVIroeTl6nyUnl27tht\nU0Ln6NSpB3nzvMDj+YpT750PWbx4hRpp/2HwoF5s27aLfv2G3Hhh+ZcKFUrTqnUTarz6Ln//fcHu\nOI6ifev/HTvxV/I1xBu37ybRSiRDcBCFn36c+cvXcPyvpGurTp05y8Ejx27qOUsXfobpC5YBMH/Z\nap7Pnw9jDNGHY4l3Xzd58Mgx/ow+xINZM6XCq7LXkSOxREcfJG/eXACUKVOcrVt32JzK+TJnTupc\nMcbweduPGTxkjM2JPNtdGPpYDKhmjPmTpOIhZYB+QAZjzOVOr1Agxn0/BngIwD0/hKSiIrfsRj1q\nr11vhmVZ529nhXdD0aKFqPtWLTZu3EpEeFJvWseO3Rk5ajyDB/di3doFxMXF0aCBDhwB2rTqwuBh\n35EmjR9/7j1Asyaf8dusBXTr0QGXy5cLFy7S4sP2dsd0hDatvmTosD74JW+rNnz/Qzdy53mExMRE\nDuyPsb3io5M1afIuLT9pRNasmQmPmMPcub/TrGlbu2PZqmjRQtStm/R5tTpiLgAdOnbHP00a+vT5\nisyZMzJt6ijWb9jMSy/VtTmt/caM+YFSJV8gU6aM7N2zhi5detGmTXP8/f2ZMzupcE14+DqaNb+3\n9yu49/atNt0HsmbDNv46fZZyb7egad0axMcnNZbqVC3D/OWrmTBrEb6+vvinSUOPz5pijCHX/7LT\n/O2aNP6iJ4mJibhcvrRr+s5NNaxqVCxJu16DqVr/U0LSB9Ljs6YARG7ewfCJM3G5XBhjaN/0He4L\ncfwVI7fl45YdGD3qe9Kk8WPv3v3Ub3DrVTW92ZjRAyjp/szas3s1Xb7qTVBQIE0a1wNg6tTZjBo1\n3uaU8l8sy/oc+BzA3aPW2rKst4wxE4FaJDXe6gHT3A+Z7v57pXv+Ius2K815ZXl+T+GU8vyewgnl\n+T2FU8rzewK7y/N7ElU0vXneWuUvNaRUef57hdPK8zuZE8rzewpPLs//do5XU/TLacy+KdfdFlc0\n1F4yxjxCUiMtIxAJ1LUs66IxJi0wBngGOAG8blnWbZXavVGPmoiIiIiIyD3Psqw/gD/c9/cAz19j\nmQtA7ZRYnxpqIiIiIiLikbx5rIcaaiIiIiIi4pG8+dKYG1V9FBERERERkbtMPWoiIiIiIuKRUvt3\n1OykhpqIiIiIiHik6/z2mVfQ0EcRERERERGHUY+aiIiIiIh4JG8uJqKGmoiIiIiIeCRvvkZNQx9F\nREREREQcRj1qIiIiIiLikby5mIgaaiIiIiIi4pEsS0MfRURERERE5C5Rj5qIiIiIiHgkVX0UERER\nERFxGF2jdqtP6uObGk/rdf6Oj7M7gkfx5vKrKU3vwZvnYzQC/GYF+ae1O4LHSEj05kOHlBWYr6bd\nETzK2YXf2h3BY2Sp1MnuCCJ3RD1qIiIiIiLikbz5RL4aaiIiIiIi4pG8+Ro1jfkRERERERFxGPWo\niYiIiIiIR/Lm31FTQ01ERERERDySN5du0tBHERERERERh1GPmoiIiIiIeCRVfRQREREREXEYVX0U\nERERERGRu0Y9aiIiIiIi4pG8ueqjetREREREREQcRj1qIiIiIiLikbz5GjU11ERERERExCN5c9VH\nDX0UERERERFxGK9tqPn4+LBi5SwmTR4GwMAfu7Nq1WzCw2fz89iBBAYG2JzQGfz9/Vm6dDoREXNY\nt24BHTp8AsDDDz/EkiXT2Lx5CWPG/ICfn5/NSe0XGvoAc+eOJypyIZHrFtC82fvJ85o2eZcN638n\nct0Cun7TzsaUzvLP9yFAp86tiVq/iLXrFtCkybv2hXOIpP0qjMjIhaxbt4Bm7v0qf/7HWbx4KuHh\ns1m+fCYFCxawOakzNG72LsvCZ7F01UwGD/8Of/80/C9HKHMXTSQiaj5DR/TV55Vb7jw5WbJievJt\n38EoGjd9lwz3hTBl+kjWRC1gyvSRhGQItjuq43zYvD5RkQtZH7WIjz5sYHecFNdxxAxKt/yOVzsO\n+s/lNu09yLMNv2H+mq13vM5TZ/+mUe+xvNzuBxr1Hsvpc38D8Hvkdmp1GkydL4fwxlfDWLdz/x2v\ny0k2blnCyojZLFs5kz+WTgPgiw4tWRH+G8tWzmTq9FFky5bF5pSeLdGyUvTmJCY1KqUEBjxs+6v8\n8MP6PPtsftIHB1GrZn3Spw/izJmzAHz77RfExh6nd+8fbc2YkJho6/ovCwwM4Ny587hcLhYtmkzr\n1p356KMPmDZtNhMnzuD777uyYcMWhgz52dacdndtZ8uWhWzZshAVtYmgoEBWrfyNWrUbkDVrJtp+\n9iHVX3mXuLg4Mme+n9jY47Zmdfn42rr+y/75Pnz77dqULFmEhg1bY1mWI7aV3e/Df+5XK1fOonbt\nD+jVqxP9+w9l3rw/qFjxRVq1akyFCq/ZmjUoTVpb15/tgazMmvsLxZ6vwoULFxk6si8L5i2mXIVS\nzJoxn18nz6JXny/ZvGkbI4aNszWr3fvVP/n4+LBl53LKl65Jg4Z1OXnyFH2/G0SLTxqRIUMwnTv2\ntC3bmbi/bVv3tTzxxKOM/XkgLxStSlzcJX6bOZamzduye/efdkcD4OzCb+/4Odbu2EeAfxraD5vO\nlC6NrrlMQmIijb4bi7/LxSvFn6Z8wXw39dyrt/3J9BUb+Or9aldN7zNxIcGBaalfpRjDflvO6fMX\naFmrLOcvxJHO3w9jDDsOHOHTQVOY9nWTO36NAFkqdUqR57kTG7csoVSJ6pw4fjJ52pXHpI2b1OPR\nx/LQ8uMv7IoIwOlze4ytAe5AiexlU/QAcWnMQsdsi//sUTPGpDHGvGOMKef++01jzABjTDNjjGNP\nWT6YPRuVKpVh5Miw5GmX3xAAadOl9epSnrfq3LnzAPj5ufDzc2FZFqVLF2XKlN8A+PnnSVSrVtHO\niI5w+PBRoqI2AXD27Dm2bdtF9uzZaPjB2/TsNZC4uDgA2xseTnGt92GDD96iW7f+ye8/bavr71eW\nZREcnB6AkJD0HDp0xM6YjuFyuUibLi2+vr4EBKTjyJFYSpR6gelT5wAQNu5XKr9UzuaUzlOqdFH+\n3LOfAwcOUrlqOcaNnQLAuLFTqPJSeZvTOctjj+UhIiKSv/++QEJCAkuWrqLGK5XtjpWinsubg+DA\ndP+5zLiFqyn3bD4yBgdeNX3knJW8+fUwanUazMBpi296nb9Hbada0fwAVCuan98jtwMQkDYNxiQd\nF/8ddwnHHCGnoiuPSQMCA3RMKtd1o6GPI4CqwMfGmDFAbSAcKAQMTeVst61Hj460/6IbiYlX7/g/\nDerJ3r2ryZs3Fz/+ONKecA7k4+NDePhsDhyIZOHCZezZs49Tp06TkJAAQEzMIR58MJvNKZ0lR45Q\nCjz9BBERkeTJ8wjFij3P0iXTmT9/Is89pyFqcO33Yc6cOahZ6yWWLpvOr1NHkivXw/YFdKAcOUJ5\n2r1ftW79Jd26tWPXrlV06/YFHTp0tzue7Q4fOsIP3w8javMfbN65nNOnz7A+cvNVn1cHYw7zwANZ\nbU7qPK/WqsrkSTMByJIlE0eOxAJw5EgsWbJksjOa42zevI3ixQuTMeN9pEuXlsqVyhAa+qDdse6q\nIydPsyhyO3VKP3fV9BWbd7P/6AnGtn+fCZ0+YMu+Q6zdse+mnvPE6XNkzpB08ilTSBAnTp9Lnrdw\n3Taqf/EjzfuF8eV7L6fcC3EAy7KYOn0Ui5dN4933Xk+e3qFTK7ZsX0ad16rxzdd9bEzo+RKxUvTm\nJDdqqD1lWdZrQA2gAlDLsqwxwHvAM6kd7nZUqlyG2NjjREVu+te8xo0+JVeuwmzfvotatbzrg+BO\nJCYmUrhwZXLlKkyhQgV49NHcdkdytMDAAMLGDaJ1686cOXMWl8tFxvsyUKJkNT7//Bt+GTvQ7oi2\nu9770N8/DRcvXKRE8WqMGDGOH3/qYVNC5wkMDGDcuEG0bv0lZ86cpWHDt/n00y7kzl2ENm268NNP\n9g1Nc4qQDMFUrlKW554qw5N5ixMQEEDZ8iXsjuV4fn5+VK5alqm//nbN+Tqbf7Vt23bRs+cPzP7t\nF36bOZao9ZtJSHDWUNbU1jNsPi1qlsHH5+r+rZWb97Jy8x5e6zKU178ayp+HjrPvyAkA3vpmOHW+\nHMKXo2bxR9QO6nw5hDpfDmH5pt3/en5jDJj/f+6yzz7GtK+b0Ld5bX6Y+keqvra7rWK5OpQsVo2a\nNd7ng0ZvU7RYIQC++rI3jz9anAnjp9Oo0Ts2p/Rs3txQu1F5fh9jTBogEAgAQoATgD/gyKGPLxQp\nSNWq5ahY8UXSpvUnffoghg3rQ/36LYGkRsmkiTNo+UkjxoyZaHNaZzl16jSLF6+kcOFnCQkJxtfX\nl4SEBLJnf4CDBw/bHc8RXC4X48MGExY2lWnTkoZaxcQcYuq02QCsWRNFYqJFpkwZOXbshJ1RbXW9\n92FMzOHk7TZ92lw1PtxcLhdhYYMIC/s1efvUrVuTVq2Srq+YPHkmP/6oHrVSpYuyb180x93Xesyc\nMY/n//F59WD2bBom+g/lKpRifdQWYo8mDTU+evQYWbNm5siRWLJmzawhyNcwYmQYI9zDtr/+qi3R\n0YdsTnR3bd53kM8G/wrAybPnWbpxF76+PliWxftVilK71HP/eszY9kmFkK53jVrG4EBi/zpD5gzp\nif3rDBnT/7uo23N5cxAdO4OTZ85z3zXme6LLn0fHYo8zc/o8nitYgBXLVyfPnxA2jUm/DqPrN33t\niigOdqMetWHANiAKaA9MNMYMAVYDYf/1QLt06tSDvHle4PF8xan3zocsXryC+vVb8sgjOZKXqVq1\nHDu2//sMz70oU6aMhIQkVfxKm9afsmVLsG3bLhYvXsmrr1YBoG7dWsyYMc/OmI4xaFBPtm3bSb/+\nQ5KnTZ8+l1KligKQJ3dO/NL43dONNLj++3DmjHmUKvUCACVKFGHXrr02J3WGpP1qF/37//+I8kOH\njlCyZBEAXnyxGLt2/WlTOueIjj5IwUJPky5dUlGTkqVeYPv23Sxbsopqr1QC4PU3ajB71kI7YzpO\nrdovMXnijOS/5/y2kDfeehWAN956ldmzFtgVzbEyZ74fgIceepBXXqnMuLBfbU50d83+9kNmd0+6\nlX8uH+3fqkyZZx6l6JOPMHXZes5fSLom+8jJ0xy/Ygjjfyn9dF6mr9gAwPQVG3jx6UcB2H/kRHKv\n7tZ9h4iLTyBD0H9fP+cpAgLSERQUmHy/TNnibN2y46ph/1VfKseO7XtsSugdLMtK0ZuT/GePmmVZ\nfYwx4933DxpjRgPlgCGWZUXcjYApwRjD4CG9CU4fhDGGjRu38rHN1XWcIlu2LAwd+h2+vr74+Pgw\nefJMZs9eyLZtOxk9egCdO39KVNRmRo4cb3dU2xUtWoi6b9Vi48atRIQn9Xp07NidkaPGM3hwL9at\nXUBcXBwNGrS0Oalz9e79I8NH9KV58/qcPXeeZk3b2h3JdkWLFuKtt2qyceNWwsOTemY7duxB06Zt\n6dWrMy6XLxcuXKRZM22rdWs2MGPaXBYtnUp8fDwbN2xl9Igw5s/9gyEj+vB5hxZsXL+FsaM1WuKy\ngIB0lH6xGC0/+v/vvD7fDWLE6P7Ufac2Bw7E8N47H9mY0Jkmjh9Cxvvv49KleD76qD2nTp22O1KK\n+mzwFNZs389fZ89T/tN+NKlWknj38M5/Xpd2paJP5GLvoeO83W0EAAH+aejaoDr3/6PgyLW8X7ko\nn/40hanLonjg/hB6NqoJwIJ125ixcgN+vr74+7no0ahGcnERT5clSybGhv0EgMvXl4kTprNg/hLG\njB1Inrw5SUy0OLA/hhYf6Zj0TjhtuGJK8try/J7AaeWbnc7u8vyexCnl+T2B3oc3z+7y/J5E+9XN\nc1p5fqdLifL89wonlOf3FJ5cnv/5B0ul6AFixMHFjtkWN7pGTURERERExJG8+US+GmoiIiIiIuKR\nnHZdWUq6UTERERERERGRe5Ix5iFjzO/GmC3GmM3GmI/d0zMaY+YbY3a6/73PPd0YY/obY3YZYzYY\nY5693XWroSYiIiIiIh7pLvyOWjzQyrKsx4EiQDNjzONAW2ChZVl5gIXuvwEqA3nct4bAj7f72tRQ\nExERERERj5Ta5fktyzpkWdY69/0zwFYgO1AdGOVebBTwivt+dWC0lWQVkMEY88DtvDY11ERERERE\nRG7AGPMw8AwQDmS1LOuQe9ZhIKv7fnbgwBUPi3ZPu2UqJiIiIiIiIh7pbv2OmjEmCJgMtLAs6/SV\nv/dnWZZljEnxIGqoiYiIiIiIR7ob5fmNMX4kNdLGWpY1xT35iDHmAcuyDrmHNh51T48BHrri4aHu\nabdMQx9FRERERESuwSR1nQ0DtlqW9d0Vs6YD9dz36wHTrpj+jrv6YxHg1BVDJG+JetRERERERMQj\nJab+76gVA94GNhpjotzT2gHfAhOMMfWBfUAd97zfgCrALuA88N7trlgNNRERERER8UipPfTRsqxl\ngLnO7LLXWN4CmqXEujX0UURERERExGHUoyYiIiIiIh7pLgx9tI161ERERERERBxGPWoiIiIiIuKR\n7kZ5frukSkPN5eObGk/rdeITE+yO4FHMda/jlH/y5mEAYp+4hHi7I3iMC/FxdkfwGD5Gn+23Ikul\nTnZH8BhHl/a1O4LcBd58zKOhjyIiIiIiIg6joY8iIiIiIuKRNPRRRERERETEYTT0UURERERERO4a\n9aiJiIiIiIhH0tBHERERERERh7GsRLsjpBoNfRQREREREXEY9aiJiIiIiIhHStTQRxEREREREWex\nVPVRRERERERE7hb1qImIiIiIiEfS0EcRERERERGH0dBHERERERERuWvUoyYiIiIiIh4p0Yt71Ly2\noRYSkp7vf+hGvsfzYlkWzZq0pWy5EtR79zWOHTsBQJfOvZk/7w97g9osNPQBhg3rS9YsmbAsi2HD\nfmHAD8Pp1Kk1L79UgcTERGJjj9Pgg084dOiI3XFtlbSt+pAlS+bkbfXDD8PJn/9xvv++K2nT+hMf\nn8DHH7dnzZr1dse1lb+/PwsWTCBNmjS4XC5+/fU3vv66DwCdO3/Kq69WISEhkSFDxjBw4Eh7w9pM\n+9Wt27hlCWfPniMhIYH4+ARKl6jOk089Rt9+XxMYFMj+fdE0eL8lZ86ctTuqrQYP6kWVKuWIjT3G\nM8+WA2DszwPJmzcXACEhwZw6dZpCz1e0M6YjXGtb5X8qHwMGfEtQUCD79h3gnXof3vP71GXXeg8C\nNGr8Dh80fJuEhATmzv2djl90tznpnev40wSWRG4hY3AQU3q2/tf81Vt206LXSLJnuQ+AMoWeonHN\n8ne0zrhL8bQfGMbWvdGEBAXQ4+O6ZM+ckY279vPV0EkAWBY0rlWesoWeuqN1eQPLi69RM6kxrjMk\nKJftW+zHQT1ZuWI1o0dNwM/Pj4CAtDRp9h7nzp7n+/5D7Y4HwIX4OLsjkC1bFrJly0JU1CaCggJZ\ntfI3atVuQEzMoeQvpGZN3yNfvjw0/7CdrVkNxtb1/3NbrVw5i9q1P6BXr0707z+UefP+oGLFF2nV\nqjEVKrxma1Zj7N1WAIGBAZw7dx6Xy8WiRZNo3fpLHn00N6VKvcAHH7TCsiwyZ76f2Njjtua0e2y7\nJ+1XaXydcW5v45YllCpRnRPHTyZP+2PJVNq368ryZRHUfac2D+cI5euv+tiW0Qmf78WLF+bs2XOM\nGN43ufFxpe7dO3D61Bm+6drXhnTOcq1ttWL5TD5r+zVLl66iXr3XyPnwQ3T+spfNSSGtK43dEa75\nHixRsgit2zSj9qv1iYuLI1Pm+zlm8+f70aV3vm+v3bqHgLRpaD8w7LoNtVEzFzOgzfu3/NwxsSfo\n+ON4hnVsctX08fNWsGP/ITo0qMnsFVEsWr2Jnh/X5e+Lcfi5fHH5+hJ78jS1237HgoEdcPn63vbr\nuyztswtTC4kAAA4cSURBVNXsP3C4Tdky5EvRL/LDf211zLa44TVqxphHjDGtjTH9jDHfGWMaG2OC\n70a42xUcHESxYoUYPWoCAJcuXeLUqTM2p3Kmw4ePEhW1CYCzZ8+xbdsusmfPdtVZw4DAALy4V/mm\nXW9bWZZFcHB6IKkn917vebzs3LnzAPj5uXC5/LAsi4YN69K1a7/kxpHdjTQn0H6VMnLlzsnyZREA\n/L5wGdWqV7I5kf2WLQvn5Mm/rju/Vs2XGT9h2l1M5FzX2lZ58jzC0qWrAFi4cAk1alSxI5rHqN/g\nLfr0/om4uKSTFHY30lLKc/keITgo4LYeO3PpWt78oj912n5Hl6GTSEhMvKnH/b52M9VKPgdA+cJP\nEbFpJ5Zlkc4/TXKj7OKleNtPYDuFZVkpenOS/2yoGWM+An4C0gKFAH/gIWCVMaZ0qqe7TTlyPMSx\nYycY+FMPli6fzvcDuhIQkO7/2rn7OK3nfI/jr8/MNZWZodIdytJpsg9lCUn3IiISYVsecU72HN1o\nCSWt+3A2kc7GLqKR2DQiTbEVebBsh0aKFU21V0mabiZ3UVZ38zl/XJe23XSzx9T3e03v5+NxPbrm\nah4z7/k9fr9r5vP7fr4fAK7qewX/O+eP/O7he6lVK+p6c7876qhGnNCiOe+88x4Aw4YNIZks4bJL\nezDsrvB3EWNy1FGNaJE+VoMHD2P48JtJJucwfPit3HZb5rd6VIasrCzmzJnOihXzee21PzN37vs0\nbnwUl1xyPrNnv0hx8XiaNDk6dMyo6LzaO+5O8bTxvDF7Kr2vvBSARaVLOK9bqt3owovOpWGjw0NG\njF779qdSXr6OZPLj0FGitXDhErp3T7WFXnxxNxo1OiJwonj80DVY0LQxbduewmt/eoHpMydy0knH\nB065/3zw10/4+U2juPresSQ/XQPAsrK1vDznL4y/cwCT7r2BbMti+uz5e/X1yr9Yz2F1agGQyM4m\nP7cGX32Tuvn5QXIFPQaP5JIhD3Drf11UKatpma4Cr9RHTPa0onYV0NXd7wHOBJq7+y3AOUC4npI9\nSCQSnNCiOYVjJ9ChXXc2fvs3rh/Uj8KxE2jxs9Np36Yba9eu457fhG3li0leXi5FE8cwePCd21fT\n7rjjPgoKTmVi0RT69+8dNmBE8vJymThxDIMHD+ObbzbQp88V3HjjXRQUtGbIkLt49NH7Q0eMQkVF\nBa1bn0tBQWtatmxBs2bHUL16NTZt2kT79uczbtxExozRsfqezqu9d/aZPenYrjsX9/glV/W9grbt\nTuHq/jdxVZ/LeWP2VA7Oz2PL5i2hY0btF7+4QKtpe9Cn7yD69v135rw9nYPz89msc2q7H7oGE4ls\nateuyRmdLuK2W4bz5NMPhY65Xxx7dENmPnQzz424gcvObsf1o8YDUPJhktJlZfS6dTQ9h46i5KMk\nK8tTMxKue+BJeg4dxa9GFPLRspX0HDqKnkNHUfynuXv8fscX/IQpIwfzzH9fS+HU19mk87JK25sN\nBwlgG6nVtHwAd19hZjn7MtiPUVa2mrKyNcxLb7yfWjyD62/ox7ryvy/Djx9XxLPPx7FXLbREIsGz\nRY9RVFTM1Kkzd/r/oqIpTC1+irvvHhUgXVwSiQRFRWNSxyR9rC6//GIGDboDgMmTX+KRR7TysaP1\n67/mjTfeokuXTpSVraa4OHXcpk6dqUItTefVv+b7NtDP1n3OS9Ne4eSWJ/DQ6LFc2P0/ACgoaMzZ\n55weMmLUsrOzufCCrrRuo1a+3Vm8eCnnndcLgKZNG9O1a+fAieLxQ9fgqrI1TJv2MgDz5n2AV1RQ\np+6hfJ4e4FZV5efW2P68w4nH8psnpvDl1xtxd87veDIDL9v5OvvtoN7Arveo1T+0Jms+/4oGdWqx\ndds2Nnz7HbUO/sf2y39r2IDc6tVIfrqG5k2OrPwfLIPE1q5Ymfa0ojYWmGtmjwNvA78HMLN6QLRX\nXnn5Z5SVraagaWMATuvUlsWLkjRoUG/753Q7vwulC5eEihiVMWPuZ9GivzL6wce3v1awQ0va+d26\nsHhxMkCy+KSOVZIHdxhIs3r1Wjp2bA3A6ae3I5lcHihdPOrWPZSaNVOtxTVqVKdz5w4sXpzkxRdf\n4bTT2gDQoUNrtV2l6bzae7m5B5Gfn7f9+Rmd21O6cAl169UBUoN0brxpAIWFz4SMGbXU9biUsrLV\noaNErd4O59Svhw7kscefDpwoDru6Bl96cdb296yCgsbkVMup8kUawGdffb29UFiQXEGFO7UOzuXU\n45ry6jsL+Hx9qktp/YZvWbXuy919qe06ndyMaW/OA2BWyQJaNS/AzFhZ/gVbt20DYNW6L1m+ah1H\n1Dt0H/xUmaXCvVIfMdntipq7jzazV4FjgQfcfVH69XVAx/2Q7/9tyKBhjC38H3Kq5bD8408Z0H8I\nI+6/nZ8d3wx3Z8UnK7nu2ltDxwyubdtTuLzXJSxYUMo7Jak7+bffPoLevS/lmGOaUFFRwYoVK4NP\nfIxB27an0KvXxSxYUEpJyQwAbr/9Pq6+eigjR95JIpHNd99tYsCAoYGThnfYYfV5/PFRZGdnkZWV\nxeTJLzFjxmu89da7jBs3mmuu+U82bvyW/v1vCh01OJ1X/5r69esyoehRILV347lJ03h11pv0v7o3\nV/W5AoBp017mD089FzJmFJ5+6nd07NiGunUPZdnSudx19wM8+WQRPX/enWcnFYeOF5UfOlb5+Xn0\n75dapS0unsH48c8GThmHXV2DOTk5PPzoCObMncHmzVvo1+fGwEkrx00PTuDd0qV89c1GzhpwD/0v\n6cLWraliqedZbZhVsoBJs94mkZ1F9Wo5jLi2F2ZGk0YNGNDzbPoPf4yKCieRyObmK3twRL3ae/ye\nPTq14paHi+h23b0ckp/LfdekVnbfW/wxT0x9nZxEFmZZ3PzLHtQ+JG+f/vwSVpUdz58JYhjfnEk0\n3WjvxTCeP1NU5ZaJyhbLeP5MoPd32VdiGM+fKSpjPP+BIpPH89fOL6jUX+RfbkhGcyz2OJ5fRERE\nRERE9i/dHhURERERkYwU20j9yqRCTUREREREMlJV3sKg1kcREREREZHIaEVNREREREQyUmwj9SuT\nCjUREREREclIXoX3qKn1UUREREREJDJaURMRERERkYyk1kcREREREZHIaOqjiIiIiIiI7DdaURMR\nERERkYxUlYeJqFATEREREZGMpNZHERERERER2W+0oiYiIiIiIhmpKq+oqVATEREREZGMVHXLNLCq\nXIWKiIiIiIhkIu1RExERERERiYwKNRERERERkcioUBMREREREYnMAVGomdk5ZrbYzJJmNjR0nliZ\n2RNmVm5mH4bOEjszO9LMXjezhWb2kZkNDJ0pVmZWw8zeMbO/pI/VsNCZYmdm2Wb2npm9FDpL7Mxs\nuZktMLP3zezd0HliZma1zOx5M1tkZqVm1iZ0phiZ2U/T59P3j6/N7LrQuWJlZten39s/NLOJZlYj\ndKZYmdnA9HH6SOeU7I0qP0zEzLKBJcBZwEpgLnCZuy8MGixCZtYR2AA85e7Hhc4TMzM7HDjc3eeb\n2cHAPOBCnVc7MzMD8tx9g5nlALOBge4+J3C0aJnZDUBL4BB37xY6T8zMbDnQ0t0/C50ldmY2Hviz\nu481s2pArrt/FTpXzNJ/Q5QBp7r7J6HzxMbMGpJ6T2/m7n8zs0nAdHd/Mmyy+JjZcUAR0ArYDMwE\n+rl7MmgwidqBsKLWCki6+zJ330zqIrkgcKYoufubwBehc2QCd1/t7vPTz78BSoGGYVPFyVM2pD/M\nST+q9h2iH8HMGgHnAWNDZ5Gqw8xqAh2BQgB336wiba90BpaqSNutBHCQmSWAXGBV4DyxOhYocfdv\n3X0r8AZwUeBMErkDoVBrCHy6w8cr0R/UUonM7GjgRKAkbJJ4pVv53gfKgVnurmO1a78FhgAVoYNk\nCAdeMbN5ZtYndJiINQbWAePSbbVjzSwvdKgMcCkwMXSIWLl7GTASWAGsBta7+ythU0XrQ6CDmdUx\ns1zgXODIwJkkcgdCoSayz5hZPjAZuM7dvw6dJ1buvs3dWwCNgFbpFhD5J2bWDSh393mhs2SQ9u5+\nEtAVGJBu4ZadJYCTgEfc/URgI6A927uRbg/tDjwXOkuszKw2qS6lxsARQJ6ZXR42VZzcvRQYAbxC\nqu3xfWBb0FASvQOhUCvjH+9YNEq/JvKjpPdbTQYmuPsLofNkgnSr1evAOaGzRKod0D2976oIOMPM\n/hA2UtzSd/Rx93JgCql2d9nZSmDlDqvZz5Mq3GTXugLz3X1t6CAROxP42N3XufsW4AWgbeBM0XL3\nQnc/2d07Al+SmqEgsksHQqE2F2hqZo3Td8cuBaYFziQZLj0goxAodfdRofPEzMzqmVmt9PODSA32\nWRQ2VZzc/dfu3sjdjyb1XvWau+vu9C6YWV56mA/pNr4upNqL5J+4+xrgUzP7afqlzoCGH+3eZajt\ncU9WAK3NLDf9e7EzqT3b8gPMrH7635+Q2p/2TNhEErtE6AD7mrtvNbNfAS8D2cAT7v5R4FhRMrOJ\nQCegrpmtBO5w98KwqaLVDrgCWJDeewVws7tPD5gpVocD49PT07KASe6usfNSGRoAU1J/H5IAnnH3\nmWEjRe0aYEL6puUy4MrAeaKVLvzPAvqGzhIzdy8xs+eB+cBW4D3gsbCpojbZzOoAW4ABGugje1Ll\nx/OLiIiIiIhkmgOh9VFERERERCSjqFATERERERGJjAo1ERERERGRyKhQExERERERiYwKNRERERER\nkcioUBMREREREYmMCjUREREREZHIqFATERERERGJzP8BZJNoALDwyfkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x518.4 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Visualizing the confusion matrix')\n",
    "plt.figure(figsize = (15, 7.2))\n",
    "sns.heatmap(confusion_matrix(y_test_o, predictions), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "91DnbyzFQCVg",
    "outputId": "82c28e5f-8009-4655-a542-d567a4c0d2d7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEXCAYAAAD82wBdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEXCAYAAAAjlXpCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns/4FUnlyVsAU2xwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model3 Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model3 Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rX4gZs4YB3Kc",
    "outputId": "43ddc14c-8b2e-4de5-e0e0-e44017d97147"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict_classes(X_test)[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "8YADP-ZLB7HA",
    "outputId": "72b128ca-23b2-47cd-ed8b-c96c1967fc36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7f9f3c72b0>"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Showing the image\n",
    "plt.imshow(X_test[20].reshape(32, 32), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZnPW2DeGCBf8",
    "outputId": "750a7e76-cbe8-4c4a-b03d-b64ae030cc8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict_classes(X_test)[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "SNYI3rhrCEVv",
    "outputId": "9a7b29ec-738a-4dcf-bc8a-8cab43f49509"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7f9f2fd198>"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJyRiqOCERAwVnJCI+X8SLdBus2/nIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[10].reshape(32, 32), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "liZmu-GVCPoV",
    "outputId": "795f01e0-a6a2-4cec-8a9f-b4e0d55b3f49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict_classes(X_test)[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-nx-k4eMKAl"
   },
   "source": [
    "<a id='Conclusion'></a>\n",
    "### Conclusion\n",
    "Evaluated the accuracy using two methods i.e. baby sitting the NN and NN through API. Followed all the required steps starting with loading the datasets to performing hyperparameter optimization and running a finer search by using a finer range. Explored different options in optimizers, number of activators, learning rate and activation methods in NN through API. Found that baby sitting process achieved the best accuracy of 21% using hyper parameter optimization. It might have been further improved but that's the trade off vs time taken to run the script. NN through API method achieved best accuracy of 90% on validation set. Also printed the classification report, visualized the confusion matrix and summarized history for accuracy and loss."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07_Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
